{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45da25f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300 entries, 0 to 299\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   Day     300 non-null    int64\n",
      " 1   Demand  300 non-null    int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 4.8 KB\n",
      "None\n",
      "Mean absolute error in train dataset using Linear Regression Model: 446.38\n",
      "Mean absolute error in test dataset using Linear Regression Model: 578.07\n",
      "\n",
      "R2 score in train dataset using Linear Regression Model: 0.70\n",
      "R2 score in test dataset using Linear Regression Model: -1.25\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dineshbisht/masterdegree/myproject_env/lib/python3.13/site-packages/keras/src/layers/core/dense.py:106: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 12552623.0000 - mae: 3392.9314 - val_loss: 21492440.0000 - val_mae: 4610.8398\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992us/step - loss: 12537593.0000 - mae: 3390.7615 - val_loss: 21467286.0000 - val_mae: 4608.1118\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 897us/step - loss: 12518510.0000 - mae: 3387.9614 - val_loss: 21429838.0000 - val_mae: 4604.0474\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 918us/step - loss: 12490324.0000 - mae: 3383.8113 - val_loss: 21378622.0000 - val_mae: 4598.4814\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 12446730.0000 - mae: 3377.4177 - val_loss: 21311856.0000 - val_mae: 4591.2163\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 817us/step - loss: 12392782.0000 - mae: 3369.4465 - val_loss: 21229640.0000 - val_mae: 4582.2539\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 868us/step - loss: 12326632.0000 - mae: 3359.5442 - val_loss: 21130626.0000 - val_mae: 4571.4370\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - loss: 12244035.0000 - mae: 3347.6694 - val_loss: 21016136.0000 - val_mae: 4558.8975\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825us/step - loss: 12154864.0000 - mae: 3334.1165 - val_loss: 20884510.0000 - val_mae: 4544.4385\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825us/step - loss: 12056209.0000 - mae: 3319.3132 - val_loss: 20739456.0000 - val_mae: 4528.4507\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 11944449.0000 - mae: 3302.6396 - val_loss: 20582000.0000 - val_mae: 4511.0322\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 11827216.0000 - mae: 3284.3071 - val_loss: 20409596.0000 - val_mae: 4491.8823\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 984us/step - loss: 11689571.0000 - mae: 3263.7083 - val_loss: 20225050.0000 - val_mae: 4471.2930\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 966us/step - loss: 11563339.0000 - mae: 3243.5193 - val_loss: 20027916.0000 - val_mae: 4449.1938\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 946us/step - loss: 11413109.0000 - mae: 3221.4688 - val_loss: 19813084.0000 - val_mae: 4424.9854\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 850us/step - loss: 11238238.0000 - mae: 3194.6191 - val_loss: 19586856.0000 - val_mae: 4399.3486\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step - loss: 11071640.0000 - mae: 3167.8884 - val_loss: 19344292.0000 - val_mae: 4371.6934\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 825us/step - loss: 10901306.0000 - mae: 3139.5928 - val_loss: 19093896.0000 - val_mae: 4342.9604\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 785us/step - loss: 10711216.0000 - mae: 3111.1338 - val_loss: 18829904.0000 - val_mae: 4312.4604\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - loss: 10517796.0000 - mae: 3078.8857 - val_loss: 18555508.0000 - val_mae: 4280.5273\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 795us/step - loss: 10362111.0000 - mae: 3052.5674 - val_loss: 18279458.0000 - val_mae: 4248.1602\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 796us/step - loss: 10126750.0000 - mae: 3014.5793 - val_loss: 17991128.0000 - val_mae: 4214.0874\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 9936199.0000 - mae: 2981.0273 - val_loss: 17695208.0000 - val_mae: 4178.8296\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 790us/step - loss: 9717692.0000 - mae: 2946.0803 - val_loss: 17390282.0000 - val_mae: 4142.1846\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 808us/step - loss: 9481597.0000 - mae: 2905.9590 - val_loss: 17079952.0000 - val_mae: 4104.5537\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - loss: 9273034.0000 - mae: 2866.7949 - val_loss: 16764033.0000 - val_mae: 4065.8875\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step - loss: 9041295.0000 - mae: 2825.4519 - val_loss: 16441700.0000 - val_mae: 4026.0537\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step - loss: 8815954.0000 - mae: 2786.5547 - val_loss: 16119002.0000 - val_mae: 3985.7764\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - loss: 8550388.0000 - mae: 2743.0225 - val_loss: 15782358.0000 - val_mae: 3943.3193\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 793us/step - loss: 8325382.5000 - mae: 2698.5562 - val_loss: 15443615.0000 - val_mae: 3900.1313\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 849us/step - loss: 8140837.0000 - mae: 2660.1006 - val_loss: 15109006.0000 - val_mae: 3856.9956\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 799us/step - loss: 7902307.5000 - mae: 2617.2874 - val_loss: 14772654.0000 - val_mae: 3813.1436\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 991us/step - loss: 7665338.5000 - mae: 2573.6177 - val_loss: 14427743.0000 - val_mae: 3767.6455\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 891us/step - loss: 7462334.0000 - mae: 2534.2451 - val_loss: 14091436.0000 - val_mae: 3722.7468\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 912us/step - loss: 7190497.5000 - mae: 2481.3411 - val_loss: 13744709.0000 - val_mae: 3675.8838\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 975us/step - loss: 7038182.5000 - mae: 2449.3025 - val_loss: 13401766.0000 - val_mae: 3628.9363\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883us/step - loss: 6691121.0000 - mae: 2379.1558 - val_loss: 13059904.0000 - val_mae: 3581.5237\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 816us/step - loss: 6613900.5000 - mae: 2360.0166 - val_loss: 12718116.0000 - val_mae: 3533.4863\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - loss: 6307479.0000 - mae: 2296.3977 - val_loss: 12380979.0000 - val_mae: 3485.4539\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 806us/step - loss: 6092007.5000 - mae: 2247.2783 - val_loss: 12042924.0000 - val_mae: 3436.6165\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 837us/step - loss: 5854243.0000 - mae: 2197.7227 - val_loss: 11704248.0000 - val_mae: 3386.9834\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 831us/step - loss: 5728122.5000 - mae: 2168.0500 - val_loss: 11369733.0000 - val_mae: 3337.2356\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769us/step - loss: 5417959.5000 - mae: 2101.6846 - val_loss: 11044097.0000 - val_mae: 3288.0852\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776us/step - loss: 5238058.0000 - mae: 2063.3896 - val_loss: 10715805.0000 - val_mae: 3237.7791\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 782us/step - loss: 4934107.5000 - mae: 1996.4014 - val_loss: 10388841.0000 - val_mae: 3186.8870\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 834us/step - loss: 4808461.0000 - mae: 1967.0612 - val_loss: 10074981.0000 - val_mae: 3137.2585\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 4636828.0000 - mae: 1926.1533 - val_loss: 9751077.0000 - val_mae: 3085.2043\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 804us/step - loss: 4482291.0000 - mae: 1883.0303 - val_loss: 9453117.0000 - val_mae: 3036.5322\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - loss: 4343481.0000 - mae: 1852.7090 - val_loss: 9148051.0000 - val_mae: 2985.8770\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783us/step - loss: 4152016.5000 - mae: 1803.7690 - val_loss: 8844694.0000 - val_mae: 2934.6384\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Mean absolute error in train dataset using Linear Regression Model: 1766.78\n",
      "Mean absolute error in test dataset using Linear Regression Model: 2934.64\n",
      "\n",
      "R2 score in train dataset using Linear Regression Model: -2.83\n",
      "R2 score in test dataset using Linear Regression Model: -37.03\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load CSV data into Pandas DataFrame\n",
    "df = pd.read_csv(\"Demand.csv\", sep=';')\n",
    "\n",
    "# Get basic information about data\n",
    "print(df.info())\n",
    "\n",
    "# Split the data based on the 'Day' column\n",
    "train_data = df[df['Day'] <= 250] # Days 1 to 250 for training\n",
    "test_data = df[df['Day'] > 250] # Days 251 to 300 for testing\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X_train = train_data[['Day']]\n",
    "y_train = train_data['Demand']\n",
    "\n",
    "X_test = test_data[['Day']]\n",
    "y_test = test_data['Demand']\n",
    "\n",
    "# Train the linear regression model on the training dataset\n",
    "linreg_model = LinearRegression()\n",
    "linreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on train and test datasets\n",
    "linreg_model_prediction_train = linreg_model.predict(X_train)\n",
    "linreg_model_prediction_test = linreg_model.predict(X_test)\n",
    "\n",
    "# Evaluate and display model accuracy with Mean Absolute Error for both train and test datasets\n",
    "print(f\"Mean absolute error in train dataset using Linear Regression Model: {mean_absolute_error(y_train, linreg_model_prediction_train):.2f}\")\n",
    "print(f\"Mean absolute error in test dataset using Linear Regression Model: {mean_absolute_error(y_test, linreg_model_prediction_test):.2f}\")\n",
    "\n",
    "# Evaluate and display model accuracy with R2 Score for both train and test datasets\n",
    "print(f\"\\nR2 score in train dataset using Linear Regression Model: {r2_score(y_train, linreg_model_prediction_train):.2f}\")\n",
    "print(f\"R2 score in test dataset using Linear Regression Model: {r2_score(y_test, linreg_model_prediction_test):.2f}\")\n",
    "\n",
    "\n",
    "# Train the non-linear Neural Network model on the training dataset\n",
    "nn_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(30, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(30, activation='tanh',),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(30, activation='relu',),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "nn_model.compile(\n",
    "    tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "nn_model.fit(X_train, y_train, validation_data=(X_test, y_test) , epochs=50, batch_size=10)\n",
    "\n",
    "# Predict on train and test datasets\n",
    "nn_model_prediction_train = nn_model.predict(X_train)\n",
    "nn_model_prediction_test = nn_model.predict(X_test)\n",
    "\n",
    "# Evaluate and display model accuracy with Mean Absolute Error for both train and test datasets\n",
    "print(f\"Mean absolute error in train dataset using Linear Regression Model: {mean_absolute_error(y_train, nn_model_prediction_train):.2f}\")\n",
    "print(f\"Mean absolute error in test dataset using Linear Regression Model: {mean_absolute_error(y_test, nn_model_prediction_test):.2f}\")\n",
    "\n",
    "# Evaluate and display model accuracy with R2 Score for both train and test datasets\n",
    "print(f\"\\nR2 score in train dataset using Linear Regression Model: {r2_score(y_train, nn_model_prediction_train):.2f}\")\n",
    "print(f\"R2 score in test dataset using Linear Regression Model: {r2_score(y_test, nn_model_prediction_test):.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myproject_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
