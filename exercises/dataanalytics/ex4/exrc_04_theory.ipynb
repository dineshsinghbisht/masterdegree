{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c386233-9127-4783-9698-67ee04c66215",
   "metadata": {},
   "source": [
    "\n",
    "Last updated: Mon 27 Oct (helmitaulu stuff at the end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a9339-5e64-43ba-ac5d-1efef62628d8",
   "metadata": {},
   "source": [
    "### On problem 1\n",
    "\n",
    "The problem is difficult, in that there may not be a readily available recipe for it. But it's a real-life problem, and everyone should expolore it enough to see that it's not that easy! Feel free to help each other, and after the deadline (maybe already during the weekend) we'll check:\n",
    "\n",
    "- what you did\n",
    "- what the students did last year\n",
    "- what we did with Mika (before any student work).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6727f899-3510-4a99-beb1-c931a349391b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### On problem 2\n",
    "\n",
    "Real-life time series data! There's a story behind it. All the Jamk employees need to report their worktimes into a shitty system called reportronic. Before getting the lecturer position, I worked as a part-time teacher, and it was okay for me to robot-report the daily 4h48min of work and 3h12 min of break between 8-16 (every day the same & no explanations), no matter what the reality was.\n",
    "\n",
    "However, after I got the lecturer position (60% - 70%) in fall 2022, they started complaining: One has to explain each entry & it's not allowed to use a robot. So I refined my robot to report random entries according to predefined tasks (explanations) and sums of working hours.\n",
    "\n",
    "That worked fine for a while, but people already knew about it, so soon the supervisors started complaining: You cannot use a robot, but you have to be completely honest.\n",
    "\n",
    "So then (spring 2023, see [worktimes](https://student.labranet.jamk.fi/~varpha/worktimes)) I started to be honest... But my yearly working hours immediately went over the predefined limit, which resulted first in a lot of useless fiddling and a waste of time for many people, and finally in me reporting zero hours in the official system (which was okay for the supervisors, although it was not honest). I was not surprised at all!\n",
    "\n",
    "Nowadays I use a combination of my old robot and this data shared to you. So I don't report everything in the official system, but the supervisors (and whoever else may be interested) can find the honest worktimes from the website (not updated anymore in fall 2025 as I don't have the time for that).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ba20a0-636b-4db9-864c-2f58d01b1b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem 3 related cell\n",
    "\n",
    "import pandas as pd\n",
    "from seaborn import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = load_dataset('dowjones')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "# df = df.set_index('Date')\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "df['moving_avg'] = df['Price'].rolling(20, center=True).mean()\n",
    "# df['moving_avg'] = df['Price'].expanding().mean()\n",
    "# df['moving_avg'] = df['Price'].ewm(alpha=0.1, adjust=False).mean()\n",
    "\n",
    "# df.info()\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d9cdb9-161a-4480-a624-1682a4a20651",
   "metadata": {},
   "source": [
    "### A general \"ml model fitting\" skeleton\n",
    "\n",
    "The rest of the course (starting from the weekend) will be about fitting machine learning models to data.\n",
    "\n",
    "An `autofit` library doesn't exist in reality, so you need to replace that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99f345c-322a-4299-975e-2c4ab2f2d77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an ml model fitting skeleton\n",
    "# autofit doesn't exist in reality\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from autofit_library import autofit\n",
    "# replace this\n",
    "\n",
    "df = pd.DataFrame()\n",
    "# your data\n",
    "\n",
    "X = df['features'] # \"supervised\" refers to this\n",
    "y = df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "model = autofit(X_train, y_train)\n",
    "# fit model\n",
    "\n",
    "model.save('my_model')\n",
    "# save model\n",
    "\n",
    "#### then later ####\n",
    "\n",
    "model = load_model('my_model')\n",
    "# load the saved model\n",
    "\n",
    "unseen_data = pd.DataFrame()\n",
    "# get some new data\n",
    "\n",
    "X = unseen_data['features']\n",
    "my_target_prediction = model.predict(X)\n",
    "# make a prediction\n",
    "\n",
    "print(my_target_prediction)\n",
    "# satisfy your customer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1af10e0-7ee4-4c61-92cc-dde6aa844f75",
   "metadata": {},
   "source": [
    "The skeleton modified to a linear regression fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ecdfbe-d9a3-4855-8820-261e238148bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "\n",
    "df = pd.read_excel('/home/varpha/dan/public/exrc_04/data/weekend/shoesize_length_gender.xlsx')\n",
    "\n",
    "df.columns=['shoesize', 'length', 'gender', 'a', 'b']\n",
    "df = df.drop(columns=['a','b'])\n",
    "\n",
    "X = df[['length','gender']]\n",
    "y = df['shoesize']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, random_state=300)\n",
    "\n",
    "model.fit(X_train, y_train) # fit\n",
    "\n",
    "model.predict(X_test)\n",
    "print('test score:', model.score(X_test, y_test))\n",
    "# the thing above is called validation\n",
    "\n",
    "\n",
    "# model.save('my_model')\n",
    "# save model\n",
    "\n",
    "#### then later ####\n",
    "\n",
    "# model = load_model('my_model')\n",
    "# load the saved model\n",
    "\n",
    "unseen_data = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        'length': [213],\n",
    "        'gender':[1]\n",
    "    }\n",
    ")\n",
    "# lauri markkanen\n",
    "\n",
    "my_target_prediction = model.predict(unseen_data)\n",
    "# make a prediction\n",
    "\n",
    "print(my_target_prediction)\n",
    "# satisfy your customer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5309fed-aca2-4993-a5db-fb1985a5b33a",
   "metadata": {},
   "source": [
    "## Time series intro\n",
    "\n",
    "  - [This](https://medium.com/towards-data-science/12-things-you-should-know-about-time-series-975a185f4eb2) is an okay blogtext (more or less)\n",
    "  - Note: a multiplicative data can be transformed into an additive data by taking (any) logarithm, because $$ \\log(ab)=\\log(a)+\\log(b).$$ In the end one deals with everything in the additive fashion, and I guess the libraries nowadays can do that automatically, so one doesn't need to worry that much about the data being multiplicative. (I think.)\n",
    "  - From now on we will refer to the [jakevdp book](https://jakevdp.github.io/PythonDataScienceHandbook) instead of the wesmckinney book (although neither book will help us in what follows below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d8e60e-d1bc-4410-82a1-825888e2fa8c",
   "metadata": {},
   "source": [
    "## Champagne sales stuff\n",
    "\n",
    "Below follows a modification of the [champagne blog](https://machinelearningmastery.com/time-series-forecast-study-python-monthly-sales-french-champagne).\n",
    "\n",
    "See also:\n",
    "- [the full time series book by the same author](https://github.com/linh22/Data-Scientist-Books/blob/main/Introduction%20to%20Time%20Series%20Forecasting%20with%20Python%20How%20to%20Prepare%20Data%20and%20Develop%20Models%20to%20Predict%20the%20Future%20by%20Jason%20Brownlee%20(z-lib.org).pdf)\n",
    "- [acf pacf gentle introduction](https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/) (missing from the book)\n",
    "- [another explanation of the same champagne thing](https://github.com/saksham219/Time-series-forecasting---Champagne-sales/blob/master/champagne.ipynb)\n",
    "- [yet another](https://github.com/ManojKumarMaruthi/Time-Series-Forecasting/tree/master)\n",
    "- [and another](https://github.com/DataForScience/Timeseries)\n",
    "\n",
    "The last link above is more general than the champagne thing. It's hard to read but it has good structure. One can probably read it with the help of AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ed322d-4db1-4f22-abb5-e956ae7a4c5a",
   "metadata": {},
   "source": [
    "## Additions (weekend)\n",
    "\n",
    "ARIMA = AutoRegressive Integrated Moving Average\n",
    "\n",
    "- one could do moving average only (explained during the weekend)\n",
    "- autoregressive: the time series correlates with \"its own past\"\n",
    "    - typically that doesn't happen\n",
    "    - but it happens more often to the **differenced** time series (see library cell below)\n",
    "- integrated: refers to the differencing operation\n",
    "- one takes a weighted average of the autoregressive and moving average parts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fff160b-e30c-4746-a445-83f0d5562139",
   "metadata": {},
   "source": [
    "### Part 1/7\n",
    "- library cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99860aa9-7289-4bfa-83bd-449483762819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# champagne sales example stripped & modified - part 1/7\n",
    "\n",
    "# just a library cell\n",
    "# these imports and functions are repeatedly used in\n",
    "# the cells that follow\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# create a differenced dataset that (hopefully) has\n",
    "# better stationarity properties than the original\n",
    "# (default differencing interval size = 1)\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return diff\n",
    "\n",
    "\n",
    "# get original value back from differenced (y_hat), given history\n",
    "def inverse_difference(history, y_hat, interval=1):\n",
    "    return y_hat + history[-interval]\n",
    "\n",
    "# demo\n",
    "a = 2*np.array([1,2,3,4,5,6])\n",
    "\n",
    "print(difference(a))\n",
    "# this is pretty straighforward\n",
    "\n",
    "print(inverse_difference(a[:-1], difference(a)[-1]))\n",
    "# get current (here the last) value back from previous values and the current value of the differenced data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5163b2-15e5-46f1-ad61-6106da08b442",
   "metadata": {},
   "source": [
    "### Part 2/7\n",
    "- load data\n",
    "- describe data\n",
    "- split to train and test parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faedc39b-6b59-498b-9ff0-02dd3438bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# champagne sales example stripped & modified - part 2/7\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/home/varpha/dan/public/exrc_04/data/weekend/monthly_champagne_sales.csv', header=0, index_col=0, parse_dates=True).squeeze('columns')\n",
    "\n",
    "print(df.describe())\n",
    "\n",
    "split_point = len(df) - 12\n",
    "# manual splitting, the last year is for \"validation\"\n",
    "# quotes because the last year is saved for the very end\n",
    "# the \"usual\" validation is done before that (by splitting the train data)\n",
    "\n",
    "dataset, validation = df[0:split_point], df[split_point:]\n",
    "# train and test data\n",
    "\n",
    "print('Dataset %d, Validation %d' % (len(dataset), len(validation)))\n",
    "# print sizes\n",
    "\n",
    "dataset.to_csv('./data/dataset.csv', header=False)\n",
    "validation.to_csv('./data/validation.csv', header=False)\n",
    "# save the train and test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd6fbf3-ebba-4d03-bbe1-b69d9aec92d0",
   "metadata": {},
   "source": [
    "### Part 3/7\n",
    "- apply a differencing operation to the data\n",
    "- test that the differenced data is stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71db84e-12f3-4adc-a416-00cb53bf0ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# champagne sales example stripped & modified - part 3/7\n",
    "\n",
    "# produce a differenced series (the I letter in ARIMA)\n",
    "# where the differencing parameter d=12 (12 months)\n",
    "# the differenced series is stationary (as tested here)\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "df = pd.read_csv('./data/dataset.csv', header=None, index_col=0, parse_dates=True).squeeze('columns')\n",
    "X = df.values.astype('float32')\n",
    "# load training dataset and save only the values (not dataframe) to X\n",
    "\n",
    "months_in_year = 12\n",
    "differenced_data = pd.Series(difference(X, months_in_year))\n",
    "# use the differencing function from the library cell\n",
    "\n",
    "differenced_data.index = df.index[months_in_year:]\n",
    "\n",
    "adfuller_result = adfuller(differenced_data)\n",
    "# test for stationarity\n",
    "\n",
    "print('ADF Statistic: %f' % adfuller_result[0])\n",
    "print('p-value: %f' % adfuller_result[1])\n",
    "print('Critical Values:')\n",
    "for key, value in adfuller_result[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))\n",
    "\n",
    "# one does not need to save the stationary.csv as in the blog\n",
    "# (it never gets used)\n",
    "\n",
    "# okay we can plot this but the picture alone\n",
    "# doesn't reveal stationarity very obviously\n",
    "# (hard to detect stationarity from a picture only)\n",
    "differenced_data.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048bb8ff-1600-4af0-acb5-806b6318ce85",
   "metadata": {},
   "source": [
    "\n",
    "## Understanding P-Values\n",
    "(An excerpt from *Essential Math for Data Science*)\n",
    "\n",
    "When we say something is statistically significant, what do we mean by that? We hear it used loosely and frequently but what does it mean mathematically? Technically, it has to do with something called the p-value, which is a hard concept for many folks to grasp. But I think the concept of p-values makes more sense when you trace it back to its invention. While this is an imperfect example, it gets across some big ideas.\n",
    "\n",
    "In 1925, mathematician Ronald Fisher was at a party. One of his colleagues Muriel Bristol claimed she could detect when tea was poured before milk simply by tasting it.\n",
    "\n",
    "Intrigued by the claim, Ronald set up an experiment on the spot.\n",
    "\n",
    "He prepared eight cups of tea. Four had milk poured first; the other four had tea poured first. He then presented them to his connoisseur colleague and asked her to identify the pour order for each. Remarkably, she identified them all correctly, and the probability of this happening by chance is 1 in 70, or 0.01428571.\n",
    "\n",
    "This 1.4% probability is what we call the p-value, the probability of something occurring by chance rather than because of a hypothesized explanation. Without going down a rabbit hole of combinatorial math, the probability that Muriel completely guessed the cups correctly is 1.4%. What exactly does that tell you?\n",
    "\n",
    "When we frame an experiment, whether it is determining it organic donuts cause weight gain or living near power lines causes cancer, we always have to entertain the possibility that random luck played a role. Just like there is a 1.4% chance Muriel identified the cups of tea correctly simply by guessing, there's always a chance randomness just gave us a good hand like a slot machine. This helps us frame our null hypothesis (Ho), saying that the variable in question had no impact on the experiment and any positive results are just random luck. The alternative hypothesis (H) poses that a variable in question (called the controlled variable) is causing a positive result.\n",
    "\n",
    "Traditionally, the threshold for statistical significance is a p-value of 5% or less, or .05. Since .014 is less than .05, this would mean we can reject our null hypothesis that Muriel was randomly guessing. We can then promote the alternative hypothesis that Muriel had a special ability to detect whether tea or milk was poured first.\n",
    "\n",
    "\n",
    "### Part 4/7 (version a &mdash; not recommended)\n",
    "- find optimal ARIMA parameters by brute force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91cc8d1-58c2-4163-90b2-b13c472d12d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# champagne sales example stripped & modified - part 4/7 version a\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "import warnings\n",
    "\n",
    "# evaluate an ARIMA model for a given order (p,d,q) and return RMSE\n",
    "# the same thing is done faster in version b (next cell)\n",
    "\n",
    "def evaluate_arima_model(X, arima_order):\n",
    "\n",
    "    # (float32 not redundant because not the same X as in previous cell)\n",
    "    X = X.astype('float32')\n",
    "\n",
    "    # here is done another split\n",
    "    # this is the \"typical\" model fit split\n",
    "    # that is called validation\n",
    "    train_size = int(len(X) * 0.50)\n",
    "    train, test = X[0:train_size], X[train_size:]\n",
    "    history = [x for x in train]\n",
    "\n",
    "    # make predictions\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "        # difference data\n",
    "        months_in_year = 12\n",
    "        diff = np.array(difference(history, months_in_year))\n",
    "        model = ARIMA(diff, order=arima_order)\n",
    "        model_fit = model.fit()\n",
    "        y_hat = model_fit.forecast()[0]\n",
    "        y_hat = inverse_difference(history, y_hat, months_in_year)\n",
    "        predictions.append(y_hat)\n",
    "        history.append(test[t])\n",
    "    # calculate score (root mean square error)\n",
    "    # we get back to that in round 5\n",
    "    # (meanwhile one can think: score = correlation coefficient for linear fit)\n",
    "    rmse = sqrt(mean_squared_error(test, predictions))\n",
    "    return rmse\n",
    "\n",
    "# evaluate combinations of p, d and q values for an ARIMA model\n",
    "def evaluate_models(dataset, p_values, d_values, q_values):\n",
    "    dataset = dataset.astype('float32')\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for p in p_values:\n",
    "    \tfor d in d_values:\n",
    "    \t\tfor q in q_values:\n",
    "    \t\t\torder = (p,d,q)\n",
    "    \t\t\ttry:\n",
    "    \t\t\t\trmse = evaluate_arima_model(dataset, order)\n",
    "    \t\t\t\tif rmse < best_score:\n",
    "    \t\t\t\t\tbest_score, best_cfg = rmse, order\n",
    "    \t\t\t\tprint('ARIMA%s RMSE=%.3f' % (order,rmse))\n",
    "    \t\t\texcept:\n",
    "    \t\t\t\tcontinue\n",
    "    print('Best ARIMA%s RMSE=%.3f' % (best_cfg, best_score))\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_csv('./data/dataset.csv', header=None, index_col=0, parse_dates=True).squeeze('columns')\n",
    "\n",
    "# evaluate parameters\n",
    "p_values = range(0, 7)\n",
    "d_values = range(0, 3)\n",
    "q_values = range(0, 7)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "evaluate_models(df.values, p_values, d_values, q_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dd55cc-5c20-41ea-a5dd-72d85c4c81c3",
   "metadata": {},
   "source": [
    "### Part 4/7 (version b &mdash; recommended)\n",
    "- find optimal ARIMA parameters (= best fit) using [pmdarima](https://github.com/alkaline-ml/pmdarima)\n",
    "- see also [sktime](https://github.com/sktime/sktime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c646392e-e73a-4adb-9f1b-f997e5abca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# champagne sales example stripped & modified - part 4/7 version b\n",
    "\n",
    "import pmdarima as pm\n",
    "from pmdarima.arima import auto_arima\n",
    "\n",
    "from pandas import read_csv\n",
    "\n",
    "def evaluate_models(dataset):\n",
    "    \n",
    "    train, test = pm.model_selection.train_test_split(dataset, train_size=90)\n",
    "    # pmdarima has train_test_split too\n",
    "    # note: this split is used to find the best fit in\n",
    "    # the dataset (the split done at the beginning is different)\n",
    "    \n",
    "    # arima_model = auto_arima(train)\n",
    "    # one can try brainless first (line above)\n",
    "    # but the below works better\n",
    "    \n",
    "    arima_model = auto_arima(\n",
    "        train,\n",
    "        start_p=0,\n",
    "        d=1,\n",
    "        start_q=0, \n",
    "        max_p=5,\n",
    "        max_d=5,\n",
    "        max_q=5,\n",
    "        start_P=0,\n",
    "        D=1,\n",
    "        start_Q=0,\n",
    "        max_P=5,\n",
    "       max_D=5,\n",
    "       max_Q=5,\n",
    "       m=12, # found in the stationarity test before\n",
    "       seasonal=True,\n",
    "       error_action='warn',\n",
    "       trace = True,\n",
    "       supress_warnings=True,\n",
    "       stepwise = True,\n",
    "       random_state=20,\n",
    "       n_fits = 50\n",
    "    )\n",
    "\n",
    "    return arima_model\n",
    "\n",
    "\n",
    "##### main #####\n",
    "\n",
    "df = read_csv('./data/dataset.csv', header=None, index_col=0, parse_dates=True).squeeze('columns')\n",
    "model = evaluate_models(df.values)\n",
    "\n",
    "print(model.summary())\n",
    "# you should get [12] (because yearly seasonality)\n",
    "# please use the latter triple in the next stage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322b4099-d15f-47ff-b4f7-94c4ba8051d0",
   "metadata": {},
   "source": [
    "### Part 5/7\n",
    "- validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84be3ded-0810-4d4a-b459-d9fcfeef0410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# champagne sales example stripped & modified - part 5/7\n",
    "\n",
    "# in this cell one just validates\n",
    "# (similar to linear regression score)\n",
    "# (but now the validation process and score concept are more complicated)\n",
    "\n",
    "# check gaussianity of the residuals\n",
    "# a common thing to do\n",
    "# ideally gaussian with zero mean\n",
    "# if mean not zero then it has to be subtracted from the predictions\n",
    "# (the predictions are biased by the mean)\n",
    "\n",
    "# this cell doesn't save anything\n",
    "# it just checks the gaussianity\n",
    "# and produces a mean (= bias)\n",
    "\n",
    "# input your best parameters (latter triple, see below) from the previous step (4/7)\n",
    "# and save the mean as the bias in the next step (6/7)\n",
    "\n",
    "# Best model:  ARIMA(0,1,2)(2,1,0)[12]     \n",
    "# (copy-pasted from the output of the previous cell)\n",
    "# the latter triple is the one we need here\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "df = pd.read_csv('./data/dataset.csv', header=None, index_col=0, parse_dates=True).squeeze('columns')\n",
    "\n",
    "X = df.values.astype('float32')\n",
    "\n",
    "# validation split 50-50 (at first -- will be \"walked forward\")\n",
    "train_size = int(len(X) * 0.50)\n",
    "train, test = X[0:train_size], X[train_size:]\n",
    "\n",
    "# walk-forward validation\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "for i in range(len(test)):\n",
    "    months_in_year = 12\n",
    "    diff = list(difference(history, months_in_year))\n",
    "    model = ARIMA(diff, order=(2,1,0))\n",
    "    # (ARIMA is from https://www.statsmodels.org/devel/generated/statsmodels.tsa.arima.model.ARIMA.html)\n",
    "\n",
    "    y_hat_diff = model.fit().forecast()[0]\n",
    "    # predict the next timestep for the differenced data\n",
    "    \n",
    "    y_hat = inverse_difference(history, y_hat_diff, months_in_year)\n",
    "    predictions.append(y_hat)\n",
    "    # the corresponding prediction for the data (not differenced)\n",
    "\n",
    "    history.append(test[i])\n",
    "    # the corresponding actual data (not predicted)\n",
    "\n",
    "\n",
    "# errors\n",
    "residuals = [test[i]-predictions[i] for i in range(len(test))]\n",
    "residuals = pd.DataFrame(residuals)\n",
    "print(residuals.describe())\n",
    "\n",
    "# plot\n",
    "plt.figure()\n",
    "plt.subplot(211)\n",
    "residuals.hist(ax=plt.gca())\n",
    "plt.subplot(212)\n",
    "residuals.plot(kind='kde', ax=plt.gca())\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354dc242-adde-4ab5-9f1b-c3af3df611e4",
   "metadata": {},
   "source": [
    "### Part 6/7\n",
    "- save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598ffce3-dd4e-4c40-a441-967bdd0f8985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# champagne sales example stripped & modified - part 6/7\n",
    "\n",
    "# save model\n",
    "# copy-paste the mean from the output of the previous cell\n",
    "# this cell produces no output\n",
    "\n",
    "from pandas import read_csv\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# load data\n",
    "df = read_csv('./data/dataset.csv', header=None, index_col=0, parse_dates=True).squeeze('columns')\n",
    "\n",
    "# prepare data\n",
    "X = df.values.astype('float32')\n",
    "\n",
    "# difference data\n",
    "months_in_year = 12\n",
    "diff = difference(X, months_in_year)\n",
    "\n",
    "# fit model\n",
    "# replace the stars with your parameters from step 4\n",
    "model = ARIMA(diff, order=(2,1,0))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# this is the mean (bias) from the previous output\n",
    "mean = -16.041581\n",
    "\n",
    "# the previous output told us that the predictions were SYSTEMATICALLY\n",
    "# a tiny bit larger than the actual values\n",
    "# (larger because we subtracted the predictions from actual values and got a mean of -16)\n",
    "\n",
    "# so we can just save that systematic \"error\" (bias) separately and take it into account\n",
    "# in our future predictions\n",
    "\n",
    "# save model (and the bias)\n",
    "model_fit.save('./data/model.pkl')\n",
    "np.save('./data/model_bias.npy', [mean])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82cea01-1734-489c-b910-f5318ba0d813",
   "metadata": {},
   "source": [
    "### Part 7/7\n",
    "- load\n",
    "- predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3ec05a-a528-4aee-86d0-592ec9d6b2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# champagne sales example stripped & modified - part 7/7\n",
    "\n",
    "# load and evaluate the model on the validation dataset\n",
    "# note: the model is already produced and saved so\n",
    "# loading and using it should be fast\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.arima.model import ARIMAResults\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# load and prepare datasets\n",
    "dataset = pd.read_csv('./data/dataset.csv', header=None, index_col=0, parse_dates=True).squeeze('columns')\n",
    "X = dataset.values.astype('float32')\n",
    "\n",
    "history = [x for x in X]\n",
    "months_in_year = 12\n",
    "validation = read_csv('./data/validation.csv', header=None, index_col=0, parse_dates=True).squeeze('columns')\n",
    "y = validation.values.astype('float32')\n",
    "\n",
    "# load model\n",
    "model_fit = ARIMAResults.load('./data/model.pkl')\n",
    "bias = np.load('./data/model_bias.npy')\n",
    "\n",
    "# make the first prediction\n",
    "predictions = list()\n",
    "y_hat = float(list(model_fit.forecast())[0]) # this y_hat is a differenced series value\n",
    "y_hat = bias + inverse_difference(history, y_hat, months_in_year) # this y_hat is an original series value\n",
    "predictions.append(y_hat)\n",
    "history.append(y[0])\n",
    "print(f'Predicted={y_hat[0]:.3f}, Expected={y[0]:.3f}')\n",
    "\n",
    "# rolling forecasts\n",
    "for i in range(1, len(y)):\n",
    "    months_in_year = 12\n",
    "    diff = list(difference(history, months_in_year))\n",
    "    model = ARIMA(diff, order=(2,1,0))\n",
    "    model_fit = model.fit()\n",
    "    y_hat = float(list(model_fit.forecast())[0])\n",
    "    y_hat = bias + inverse_difference(history, y_hat, months_in_year)\n",
    "    predictions.append(y_hat)\n",
    "\n",
    "    # obs stands for observation\n",
    "    # (the actual data)\n",
    "    obs = y[i]\n",
    "    history.append(obs)\n",
    "    print(f'Predicted={y_hat[0]:.3f}, Expected={obs:.3f}')\n",
    "\n",
    "# report performance\n",
    "mse = mean_squared_error(y, predictions)\n",
    "print('MSE: %.3f' % mse)\n",
    "plt.plot(y)\n",
    "plt.plot(predictions, color='red')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63c98a2-1a07-478a-b772-68ee41701798",
   "metadata": {},
   "source": [
    "### P.S.\n",
    "- We have skipped several parts of the original text. They were not really needed.\n",
    "\n",
    "Below Harri's sktime ad hoc try from 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c8d216-d43a-481c-9fb2-7964e83fa7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "# we need to understand this better\n",
    "\n",
    "from sktime.split import temporal_train_test_split\n",
    "# this too\n",
    "\n",
    "from sktime.forecasting.arima import AutoARIMA\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "# one can try different models\n",
    "\n",
    "# forecaster = NaiveForecaster(strategy=\"last\")\n",
    "forecaster = AutoARIMA(\n",
    "    sp=12, d=0, max_p=2, max_q=2, suppress_warnings=True\n",
    "    # season parameter 12 hours is clearly not optimal for weather data\n",
    "    # probaly 24 hours makes at least some difference (daily fluctuations should be seen in weather data)\n",
    ") \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "df = pd.read_csv('/home/varpha/dan/public/exrc_04/data/prob4_weather.csv')\n",
    "df['time'] = pd.to_datetime(df['time'],unit='s')\n",
    "df = df.set_index('time')\n",
    "\n",
    "y = df['air_temperature_2m'].dropna().asfreq(freq='h')\n",
    "\n",
    "y_train, y_test = temporal_train_test_split(y)\n",
    "# the above needs to be done better\n",
    "# in a manual fashion, like in the champagne thing\n",
    "# (currently the returned y_train has a lot of NANs for some reason)\n",
    "\n",
    "fh = ForecastingHorizon(\n",
    "    y_test.index, is_relative=False\n",
    "    # replace with something better\n",
    "    # e.g. the next day after the test data\n",
    "    # (instead of all the test data)\n",
    ")\n",
    "# print(fh)\n",
    "\n",
    "\n",
    "forecaster.fit(y_train.dropna())\n",
    "# one should not need to do dropna here\n",
    "\n",
    "forecaster.predict(fh)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e70381-a176-4b2a-abf0-262ed6e641f5",
   "metadata": {},
   "source": [
    "## On the *helmitaulu* cluster\n",
    "\n",
    "Excerpt from `exrc_03/maths_notebooks/00_Python_Basics.ipynb` (helmitaulu uses the file-based approach):\n",
    "\n",
    ">The style of programming that we will be undertaking in this subject is a `notebook` framework. This contrasts with the original style of python, which is more of a `file-based` (or synonymously, `module-based`) framework. \n",
    "\n",
    ">The ways these differ is that the `notebook` style:\n",
    "\n",
    ">1. Uses cells and evaluates them one at a time.\n",
    ">2. Produces output below the cell that called for it.\n",
    ">3. Is (arguably) more common in scientific computing and data science.\n",
    "\n",
    ">In contrast, the `file-based/module-based` style:\n",
    "\n",
    ">1. Can keep code in multiple files that can then be accessed from one another.\n",
    ">2. Produces all outputs to one location.\n",
    ">3. Is very common in software engineering environments -- e.g. building apps and running websites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518fec45-5aaa-49f6-aa48-7c9c46715680",
   "metadata": {},
   "source": [
    "There's a cluster system available for you at `helmitaulu.labranet.jamk.fi`. Requires vpn. You can log in from our jupyterhub command line (= Terminal) with `ssh helmitaulu` (labranet password). Essentially similar to csc puhti etc. Actually our jupyterhub runs on one of the helmitaulu nodes, but there are many more available (and mostly idle).\n",
    "\n",
    "Some unix commands are available in our (jupyterhub) `public/exrc_04` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6534ec4-9f6a-4bb8-8ab5-410fb45be13c",
   "metadata": {},
   "source": [
    "### Steps required to run python stuff on helmitaulu\n",
    "\n",
    "- load python module: `module load Python/3.9.5-GCCcore-10.3.0` (copy-pasted from the output of `module avail`; chosen by Harri based on trial and error)\n",
    "- double check: `which python` or just try `python` and verify that it's 3.9 (Ctrl-D exits the python prompt)\n",
    "- create a directory (python virtual environment in the workdir storage) for python libraries: `python -m venv ${WORKDIR}/dan` (dan stands for data analytics)\n",
    "- activate the virtual environment: `source ${WORKDIR}/dan/bin/activate` (deactivating happens with just `deactivate`, and removing the virtual environment altogether is done by just removing the `${WORKDIR}/dan` directory with `rm -rf ${WORKDIR}/dan`)\n",
    "- update the python package installer: `python -m pip install --upgrade pip`\n",
    "- install the packages we need: `python -m pip install sktime pmdarima`\n",
    "- downgrade numpy (sorry): `python -m pip install --upgrade numpy==1.26.4`\n",
    "  - the above has to do with some missing C library (one could send a labranet helpdesk ticket about it)\n",
    "- copy your dataset from jupyterhub to helmitaulu: at the jupyterhub terminal, do `scp /home/varpha/dan/public/exrc_04/data/prob4_weather.csv helmitaulu:` (nothing after the colon copies it to your home directory, but one could specify a destination there as well, like `scp /home/varpha/dan/public/exrc_04/data/prob4_weather.csv helmitaulu:path/to/my/directory`\n",
    "- then edit (or `scp` from jupyterhub) a python script in a `.py` file, and run it with `srun python myscript.py` or (for more verbose) `srun -v python myscript.py`\n",
    "- the `srun` command doesn't stream the output (at least by default), so you get all the output at once when the script has finished (some discussion about this [here](https://stackoverflow.com/questions/43397789/slurm-echo-output-file-so-it-prints-on-screen))\n",
    "- in order to run your scripts with the `sbatch` command (and log out in between), you need an executable script such as this `myscript.sh`:\n",
    "```bash\n",
    "#!/usr/bin/sh\n",
    "module load Python/3.9.5-GCCcore-10.3.0\n",
    "source ${WORKDIR}/dan/bin/activate\n",
    "cd ~\n",
    "python test.py\n",
    "```\n",
    "- do e.g. `nano myscript.sh`, copy-paste the contents above, save & exit, do `chmod 700 myscript.sh` and finally `sbatch myscript.sh`; the output will come to a `slurm-XXX.out` file (where XXX is the job number)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (ipykernel)",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
