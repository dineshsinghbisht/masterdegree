{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebabbbab-c235-4789-a004-14e69d06ec9d",
   "metadata": {},
   "source": [
    "# Data Analytics Fall 2025 &mdash; Exercises 1\n",
    "\n",
    "### XXXXX XXXXX XXXXX XXXXX (last modified: Sun 31 Aug)\n",
    "\n",
    "### Deadline: Around Tue 16 Sep (to be specified)\n",
    "\n",
    "- Five problems\n",
    "- Minor variations between users\n",
    "- Theme: Python & Numpy (no Pandas allowed)\n",
    "- Theory: see <tt>public/exrc_01</tt>\n",
    "- Make a copy of the original notebook (e.g. <tt>File $\\rightarrow$ Save Notebook As</tt>)<br/>\n",
    "  and add your answers (new cells) there\n",
    "- Please make both your code and your notebook readable\n",
    "- Keep your folder structure up to date by running the config script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde89ee5-64f7-4289-8065-d36dfc25c58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuring...\n",
      "\n",
      "  created the ~/dan directory tree\n",
      "  changed all ~/dan subdir permissions to 700\n",
      "  removed any broken filelinks under ~/dan\n",
      "  copied filelinks from /home/varpha/dan to ~/dan\n",
      "  removed any python cache dirs\n",
      "  creating answers workbook (Darren's idea)\n",
      "  answers workbook /home/XXXXX/dan/private/exrc_01/exrc_01_answers.ipynb already exists, skipping copy\n",
      "\n",
      "  upgrading jupyterlab etc. (may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts jlpm, jupyter-lab, jupyter-labextension and jupyter-labhub are installed in '/home/XXXXX/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  done (you may need to restart your server in order for the upgrades to take effect)\n",
      "\n",
      "All Done!\n",
      "\n",
      "Please run this config script whenever you start working on the hub.\n",
      "\n",
      "If you encountered errors, please re-run the script. If the errors persist, please report to our Teams channel.\n",
      "\n",
      "Also, please do 'File -> Hub Control Panel -> Stop My Server'\n",
      "whenever you stop working on the hub.\n",
      "\n",
      "Thank you!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.system('/usr/bin/bash /home/varpha/dan/config.sh');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c82629f-43fd-4272-a4a2-97addd6614b6",
   "metadata": {},
   "source": [
    "## Problem 1. Documentation\n",
    "- Browse through the Python and Numpy documentation\n",
    "- Find a function that a) interests you, and b) has a messy documentation\n",
    "- Play with the function and find simple use cases\n",
    "- Explain the function to your anonymous peer reviewer.\n",
    "\n",
    "Please write a nice and clear explanation. Include some elementary examples.\n",
    "\n",
    "I used ChatGPT for help me in this exercise.\n",
    "\n",
    "The function I selected is numpy.linspace. It is the first time I am working with Python and the documentation for that function is difficult for me to understand, but that function creates vectors which can be useful.\n",
    "\n",
    "The function is called in this form. numpy.linspace(start, stop, num)\n",
    "-Start is the first number of the vector\n",
    "-Stop is the last number of the vector\n",
    "-Num is the number in the vector. Example: Num=4 the vector will have 4 number, first and last incluid.\n",
    "\n",
    "So the function will be called. V1=numpy.linspace(1, 4, 4) and return V1=[1. 2. 3. 4.]\n",
    "\n",
    "Examples of a moment of needed this function:\n",
    "- You need to divide a line in same parts.\n",
    "- You need calculate the velocity in diferent moments but with same seconds betwen for calculated de aceleration.\n",
    "- You need no draw a soft curve and request a lot of point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe4d8d-c0fc-43f7-b336-3fc95dd0dd76",
   "metadata": {},
   "source": [
    "## Problem 2. Map, Lambda, Groupby\n",
    "In this problem, only plain python may be used, no numpy.<br/>\n",
    "The following links may be helpful:\n",
    "- [sorting howto](https://docs.python.org/3/howto/sorting.html)\n",
    "- [lambda sorting](https://blogboard.io/blog/knowledge/python-sorted-lambda)\n",
    "- [itertools groupby](https://stackoverflow.com/questions/773/how-do-i-use-itertools-groupby).\n",
    "\n",
    "Using the code cell below, read a csv (real wind turbine data) into a list of dicts.<br/>\n",
    "Then do the following:\n",
    "- a) using map, convert the timestamps into the format <b>MM/dd/yyyy HH:mm:ss</b>, e.g. 11/04/2018 09:10:43\n",
    "- b) using sorted and lambda, sort the rows according to increasing rotorspeed\n",
    "- c) add a column called <b><i>WindSpeed_Group</i></b> that contains the letter A, B or C, where A = less than 5mps, B = 5-10mps, C = more than 10mps. Try to use [itertools.groupby](https://docs.python.org/3/library/itertools.html#itertools.groupby) (although it may not be very smart).\n",
    "\n",
    "In your handin, include the code that does a) - c) above. No need to save the modified data. Here is the code for reading the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c90bb9bd-c729-4d75-ae15-6aa7c1ae4a3d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TimeStamp': '2018-04-01 00:00:52.584', 'ReactivePower_kVAr': '907', 'Power_kW': '-1', 'WindSpeed_mps': '8.358698', 'ErrorCode': '416', 'GenState': '1', 'RunState': '3', 'RotorSpeed_rpm': '0.03000221', 'RotorSpeedAve': '0.001997522', 'RotorOverSpeed': '0', 'RotorUnderSpeed': '0'}\n",
      "2018-04-01 00:00:52.584\n",
      "2018-04-01 00:04:09.477\n",
      "2018-04-01 00:07:54.942\n",
      "2018-04-01 00:08:04.105\n",
      "2018-04-01 00:08:11.285\n"
     ]
    }
   ],
   "source": [
    "from getpass import getuser\n",
    "import csv\n",
    "user = getuser()\n",
    "csv_location = f'/home/varpha/dan/private/{user}' + \\\n",
    "                f'/exrc_01/data/prob2_{user}.csv'\n",
    "with open(csv_location) as handle:\n",
    "    mydata = list(csv.DictReader(handle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1564608-f968-48f2-8efa-a49889167fa9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TimeStamp': '04/01/2018 00:00:52', 'ReactivePower_kVAr': '907', 'Power_kW': '-1', 'WindSpeed_mps': '8.358698', 'ErrorCode': '416', 'GenState': '1', 'RunState': '3', 'RotorSpeed_rpm': '0.03000221', 'RotorSpeedAve': '0.001997522', 'RotorOverSpeed': '0', 'RotorUnderSpeed': '0'}\n",
      "04/01/2018 00:00:52\n",
      "04/01/2018 00:04:09\n",
      "04/01/2018 00:07:54\n",
      "04/01/2018 00:08:04\n",
      "04/01/2018 00:08:11\n"
     ]
    }
   ],
   "source": [
    "# a) I will use IA for help me with the code\n",
    "#First we will look for the dates. This code displays the data from the file.\n",
    "print(mydata[0])\n",
    "\n",
    "for row in mydata[:5]:\n",
    "    print(row[\"TimeStamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "072caea1-2323-47b6-be45-db693d4a33cd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data '04/01/2018 00:00:52' does not match format '%Y-%m-%d %H:%M:%S.%f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m row\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Do the same with all dates.\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m mydata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconvert_timestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmydata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Display the first fives dates to chek\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m mydata[:\u001b[38;5;241m5\u001b[39m]:\n",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m, in \u001b[0;36mconvert_timestamp\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_timestamp\u001b[39m(row):\n\u001b[1;32m      5\u001b[0m     ts \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimeStamp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m# Read the original dates\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     dt \u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mH:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mM:\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mS.\u001b[39;49m\u001b[38;5;132;43;01m%f\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Save the original dates\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimeStamp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dt\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)     \u001b[38;5;66;03m# Save the dates in the new format\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m row\n",
      "File \u001b[0;32m/usr/lib64/python3.9/_strptime.py:568\u001b[0m, in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_strptime_datetime\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data_string, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%a\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mb \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    566\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a class cls instance based on the input string and the\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;124;03m    format string.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 568\u001b[0m     tt, fraction, gmtoff_fraction \u001b[38;5;241m=\u001b[39m \u001b[43m_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m     tzname, gmtoff \u001b[38;5;241m=\u001b[39m tt[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    570\u001b[0m     args \u001b[38;5;241m=\u001b[39m tt[:\u001b[38;5;241m6\u001b[39m] \u001b[38;5;241m+\u001b[39m (fraction,)\n",
      "File \u001b[0;32m/usr/lib64/python3.9/_strptime.py:349\u001b[0m, in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    347\u001b[0m found \u001b[38;5;241m=\u001b[39m format_regex\u001b[38;5;241m.\u001b[39mmatch(data_string)\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m found:\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime data \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m does not match format \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    350\u001b[0m                      (data_string, \u001b[38;5;28mformat\u001b[39m))\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_string) \u001b[38;5;241m!=\u001b[39m found\u001b[38;5;241m.\u001b[39mend():\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munconverted data remains: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    353\u001b[0m                       data_string[found\u001b[38;5;241m.\u001b[39mend():])\n",
      "\u001b[0;31mValueError\u001b[0m: time data '04/01/2018 00:00:52' does not match format '%Y-%m-%d %H:%M:%S.%f'"
     ]
    }
   ],
   "source": [
    "#Now we know how are the dates, and can work with they\n",
    "from datetime import datetime\n",
    "\n",
    "def convert_timestamp(row):\n",
    "    ts = row[\"TimeStamp\"] # Read the original dates\n",
    "    dt = datetime.strptime(ts, \"%Y-%m-%d %H:%M:%S.%f\") # Save the original dates\n",
    "    row[\"TimeStamp\"] = dt.strftime(\"%m/%d/%Y %H:%M:%S\")     # Save the dates in the new format\n",
    "    return row\n",
    "\n",
    "# Do the same with all dates.\n",
    "mydata = list(map(convert_timestamp, mydata))\n",
    "\n",
    "# Display the first fives dates to chek\n",
    "for row in mydata[:5]:\n",
    "    print(row[\"TimeStamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4df5225f-436f-4f86-b68d-5982d67be33b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2550188 04/24/2018 07:07:10\n",
      "-0.2100155 04/26/2018 14:55:32\n",
      "-0.1950144 04/10/2018 13:01:48\n",
      "-0.1950144 04/23/2018 13:29:11\n",
      "-0.1950144 04/23/2018 21:11:47\n"
     ]
    }
   ],
   "source": [
    "# b)\n",
    "mydata_sorted = sorted(\n",
    "    mydata,\n",
    "    key=lambda row: float(row[\"RotorSpeed_rpm\"]) if row[\"RotorSpeed_rpm\"].strip() != \"\" else 0.0 #Empy cells is trated like 0\n",
    ")\n",
    "for row in mydata_sorted[:5]: #Displace some of the cells\n",
    "    print(row[\"RotorSpeed_rpm\"], row[\"TimeStamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43698d83-3112-456d-b6bd-7709320d4f98",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 A 04/10/2018 12:15:11\n",
      "0.0 A 04/10/2018 12:17:39\n",
      "0.0 A 04/10/2018 12:19:11\n",
      "0.0 A 04/10/2018 12:21:10\n",
      "0.0 A 04/10/2018 12:21:16\n",
      "0.0 A 04/10/2018 12:22:05\n",
      "0.0 A 04/10/2018 12:23:23\n",
      "0.0 A 04/10/2018 12:24:16\n",
      "0.0 A 04/10/2018 12:29:32\n",
      "0.0 A 04/10/2018 12:29:52\n"
     ]
    }
   ],
   "source": [
    "# c)\n",
    "from itertools import groupby\n",
    "\n",
    "for row in mydata_sorted: # Convert WindSpeed_mps to float if is string; If is empty, assig 0 to value\n",
    "    ws = row[\"WindSpeed_mps\"]\n",
    "    if isinstance(ws, str):\n",
    "        ws = ws.strip()\n",
    "        row[\"WindSpeed_mps\"] = float(ws) if ws != \"\" else 0.0\n",
    "    else:\n",
    "        row[\"WindSpeed_mps\"] = float(ws)  # Is is float, maintains the type\n",
    "\n",
    "mydata_sorted_ws = sorted(mydata_sorted, key=lambda row: row[\"WindSpeed_mps\"]) # Sort by WindSpeed_mps to use groupby\n",
    "\n",
    "def wind_group(speed): # Function to assign group\n",
    "    if speed < 5:\n",
    "        return \"A\"\n",
    "    elif speed <= 10:\n",
    "        return \"B\"\n",
    "    else:\n",
    "        return \"C\"\n",
    "\n",
    "for key, group in groupby(mydata_sorted_ws, key=lambda row: wind_group(row[\"WindSpeed_mps\"])): # Use groupby to assign WindSpeed_Group\n",
    "    for row in group:\n",
    "        row[\"WindSpeed_Group\"] = key\n",
    "\n",
    "for row in mydata_sorted_ws[:10]: # Show some rows\n",
    "    print(row[\"WindSpeed_mps\"], row[\"WindSpeed_Group\"], row[\"TimeStamp\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e231c0-75b4-42d5-8662-813a47cc5218",
   "metadata": {},
   "source": [
    "## Problem 3. Vectorization\n",
    "- Some [general info](https://www.askpython.com/python-modules/numpy/vectorization-numpy)\n",
    "- The code in <tt>dan/public/exrc_01/integrator.py</tt> contains rudimentary code,<br/>\n",
    "  written in plain python, that numerically integrates a (math) function<br/>\n",
    "  $f\\colon \\mathbb{R} \\to \\mathbb{R}$ over an interval $[a,b]$.\n",
    "- Rewrite the code using numpy and vectorization.\n",
    "- Introduce timings to measure the gain of vectorization.\n",
    "- Use the (math) function $f(x)=- 14 x^{12} - 9 x^{11} - 8 x^{5} + 3$ and interval $[a,b] = [-9, 18]$ to test the code.\n",
    "- Increase the number of subintervals in order to obtain a noticeable difference in the timings.\n",
    "\n",
    "In your handin, include the rewritten code along with the timing measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85496d73-7ca2-4cf6-9289-30ceaff5360f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of subintervals: 10000\n",
      "Pure Python: -2.329488e+16, time = 0.007872 s\n",
      "NumPy version.: -2.329488e+16, time = 0.002724 s\n",
      "\n",
      "Number of subintervals: 100000\n",
      "Pure Python: -2.329488e+16, time = 0.078898 s\n",
      "NumPy version.: -2.329488e+16, time = 0.019087 s\n",
      "\n",
      "Number of subintervals: 1000000\n",
      "Pure Python: -2.329488e+16, time = 0.774100 s\n",
      "NumPy version.: -2.329488e+16, time = 0.190410 s\n"
     ]
    }
   ],
   "source": [
    "# I will use IA to help me in this exercise\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "# Function to integrate\n",
    "def f(x):\n",
    "    return -14*x**12 - 9*x**11 - 8*x**5 + 3\n",
    "\n",
    "\n",
    "# Vectorized version\n",
    "def create_mesh_np(a, b, n):\n",
    "    return np.linspace(a, b, n, endpoint=False)\n",
    "\n",
    "def integrate_np(f, a, b, n):\n",
    "    mesh_width = (b - a) / n\n",
    "    left_endpoints = create_mesh_np(a, b, n)\n",
    "    midpoints = left_endpoints + mesh_width / 2\n",
    "    heights = f(midpoints)\n",
    "    return np.sum(heights * mesh_width)\n",
    "\n",
    "\n",
    "# Original version\n",
    "def create_mesh(a, b, n):\n",
    "    return [a+i*(b-a)/n for i in range(n)]\n",
    "\n",
    "def integrate(f, a, b, n):\n",
    "    sum_of_rectangles = 0\n",
    "    left_endpoints = create_mesh(a,b,n)\n",
    "    mesh_width = (b-a)/n\n",
    "    for left_endpoint in left_endpoints:\n",
    "        midpoint = left_endpoint + mesh_width/2\n",
    "        height = f(midpoint)\n",
    "        sum_of_rectangles += height * mesh_width\n",
    "    return sum_of_rectangles\n",
    "\n",
    "\n",
    "# Timings to measure\n",
    "if __name__ == \"__main__\":\n",
    "    a, b = -9, 18\n",
    "    \n",
    "    for n in [10_000, 100_000, 1_000_000]:\n",
    "        print(f\"\\nNumber of subintervals: {n}\")\n",
    "        \n",
    "        # Pure Python version\n",
    "        start = time.time()\n",
    "        result_py = integrate(f, a, b, n)\n",
    "        end = time.time()\n",
    "        print(f\"Pure Python: {result_py:.6e}, time = {end - start:.6f} s\")\n",
    "        \n",
    "        # Vectorized NumPy version\n",
    "        start = time.time()\n",
    "        result_np = integrate_np(f, a, b, n)\n",
    "        end = time.time()\n",
    "        print(f\"NumPy version.: {result_np:.6e}, time = {end - start:.6f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c99308-0e88-4316-a0c5-5359c07a6999",
   "metadata": {},
   "source": [
    "## Problem 4. Numpy arrays\n",
    "\n",
    "- The directory <tt>dan/private/exrc_01/data</tt><br/>\n",
    "  contains a csv file (<tt>prob4_XXXXX.csv</tt>) with some weather data.\n",
    "- a) Use [numpy.genfromtxt](https://numpy.org/doc/stable/reference/generated/numpy.genfromtxt.html) to read the file into a 2-dimensional numpy array.<br/>\n",
    "  Use dtype=str in order to not lose the headers.\n",
    "- b) Use Boolean masking to drop the rows that contain <tt>nan</tt> entries.\n",
    "- c) Convert the time entries (standard timestamp) into a human-readable format of your choice.\n",
    "- d) Add a new row that contains the averages of the columns, except <tt>nan</tt> for the time column.\n",
    "\n",
    "In your handin, include the code that does a) - d) above. Do not include any saved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a2aac047-bc4e-455a-a4c0-cf0bac708cc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers:\n",
      "['air_temperature_2m' 'relative_humidity_2m' 'snowfall_amount_acc' 'time'\n",
      " 'x_wind_10m' 'y_wind_10m']\n",
      "\n",
      "Clean data with legible times (includes row of averages at the end):\n",
      "[['268.424561' '0.868437' '0.000000' '2023-01-04 00:00:00' '0.300850'\n",
      "  '-2.382514']\n",
      " ['267.929047' '0.869720' '0.039103' '2023-01-04 01:00:00' '0.152142'\n",
      "  '-2.623582']\n",
      " ['267.227448' '0.872451' '0.069010' '2023-01-04 02:00:00' '-0.037141'\n",
      "  '-2.734714']\n",
      " ...\n",
      " ['270.487732' '0.877308' '0.000651' '2023-01-02 22:00:00' '-1.110839'\n",
      "  '-0.406309']\n",
      " ['270.797119' '0.862045' '0.013021' '2023-01-02 23:00:00' '-1.196100'\n",
      "  '-0.412851']\n",
      " [264.9092538055556 0.8358128222222222 0.1489041222222222 'nan'\n",
      "  0.37566888333333337 -0.25667310555555545]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from getpass import getuser\n",
    "\n",
    "user = getuser()\n",
    "file_path = f\"/home/varpha/dan/private/{user}/exrc_01/data/prob4_{user}.csv\"\n",
    "\n",
    "# a) Read the header and separate the body\n",
    "data = np.genfromtxt(file_path, delimiter=\",\", dtype=str)\n",
    "headers = data[0, :]\n",
    "rows = data[1:, :]\n",
    "\n",
    "# b) Eliminate the rows with nan entries\n",
    "lower_rows = np.char.lower(rows)\n",
    "is_nan = np.isin(lower_rows, ['nan'])\n",
    "mask = ~np.any(is_nan, axis=1)\n",
    "clean_rows = rows[mask]\n",
    "\n",
    "# c) Convert the time into a human-readable format\n",
    "timestamps = clean_rows[:, 3].astype(float).astype(int)\n",
    "readable_times = [datetime.utcfromtimestamp(int(ts)).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                  for ts in timestamps]\n",
    "clean_rows[:, 3] = readable_times\n",
    "\n",
    "# d) Calculate averages for numeric columns (except time)\n",
    "ncols = clean_rows.shape[1]\n",
    "numeric_cols = [i for i in range(ncols) if i != 3]\n",
    "numeric_data = clean_rows[:, numeric_cols].astype(float)\n",
    "col_means = np.mean(numeric_data, axis=0)\n",
    "avg_row = np.array(['nan'] * ncols, dtype=object)\n",
    "for idx, mean in zip(numeric_cols, col_means):\n",
    "    avg_row[idx] = mean\n",
    "clean_rows_obj = clean_rows.astype(object)\n",
    "final_data = np.vstack([clean_rows_obj, avg_row])\n",
    "\n",
    "# Show results\n",
    "print(\"Headers:\")\n",
    "print(headers)\n",
    "print(\"\\nClean data with legible times (includes row of averages at the end):\")\n",
    "print(final_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf511d-2bff-4d3b-8cd3-2231732df527",
   "metadata": {},
   "source": [
    "## Problem 5. Data download\n",
    "- Start by exploring / running the code in <tt>dan/public/exrc_01/statfi.py</tt>\n",
    "- Choose a topic that interests you. Then try to download a \"lot\" of data of data of that topic. Here a lot means something like 500kB - 2MB range. (It's not really a lot but enough that the downloaded data is hard to grasp manually.)\n",
    "- Save your data in one or several json files.\n",
    "\n",
    "In your handin, include the code that you used (no saved data).\n",
    "Also, tell a few words about your experiences. What problems, if any, did you encounter?\n",
    "\n",
    "This is so dificult, I never use Python before and this exercise have a lot of problem u needed help of IA no fix my code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a678c58f-942b-4dba-94df-fa8f7b7ce18d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matk Accommodation statistics\n",
      "adopt Adoptions\n",
      "tilma Air emission accounts\n",
      "ilma Air transport\n",
      "vtp Annual national accounts\n",
      "mata Balance of payments and international investment position\n",
      "kony Bankruptcies and business restructuring proceedings\n",
      "synt Births\n",
      "ras Building and dwelling production\n",
      "rki Building cost index\n",
      "raku Building stock and new production\n",
      "rakke Buildings and free-time residences\n",
      "bdem Business demography\n",
      "ksyyt Causes of death\n",
      "vkp Central government monthly salaries\n",
      "ssaaty Changes in marital status\n",
      "kans Citizenships granted\n",
      "kbar Consumer confidence\n",
      "khi Consumer price index\n",
      "kivih Consumption of hard coal\n",
      "maku Cost index of civil engineering works\n",
      "alvaa County elections\n",
      "klt Culture\n",
      "klts Culture satellite accounts\n",
      "cvts CVTS, continuing vocational training\n",
      "kuol Deaths\n",
      "uloa Debtors in enforcement\n",
      "kkesk Discontinuation of education\n",
      "kvliik Domestic waterborne traffic\n",
      "asas Dwellings and housing conditions\n",
      "vaka Early childhood education and care\n",
      "eaa Economic accounts for agriculture (EAA)\n",
      "kanma Economy-wide material flow accounts\n",
      "kotal Educational finances\n",
      "vkour Educational structure of population\n",
      "tyokay Employment\n",
      "tyonv Employment service statistics (KEHA Centre)\n",
      "entp Energy accounts\n",
      "\n",
      "\n",
      "asen Energy consumption in households\n",
      "ehi Energy prices\n",
      "ehk Energy supply and consumption\n",
      "tene Energy use in manufacturing\n",
      "aly Enterprise openings and closures\n",
      "khak Entrance to education\n",
      "ylt Environmental goods and services sector\n",
      "ympsm Environmental protection expenditure accounts\n",
      "ymtu Environmental subsidies\n",
      "yev Environmental taxes\n",
      "euvaa European Parliament elections\n",
      "perh Families\n",
      "asyta Finance of housing companies\n",
      "rtp Financial accounts\n",
      "stu Finnish affiliates abroad\n",
      "tiet Finnish road statistics\n",
      "smat Finnish travel\n",
      "merek First registrations of motor vehicles\n",
      "ulkoy Foreign affiliates in Finland\n",
      "ssij Foreign direct investments\n",
      "uvliik Foreign shipping traffic\n",
      "jyev General government debt by quarter\n",
      "jali General government deficit and debt\n",
      "jmete General government expenditure by function\n",
      "jtume General government revenue and expenditure by quarter\n",
      "gvc Global value chains\n",
      "kttav Goods transport by road\n",
      "tkker Government R&D funding in the state budget\n",
      "khki Greenhouse gases\n",
      "ttut Growth and productivity measures\n",
      "asku Household dwelling units and housing conditions\n",
      "vtutk Households' assets\n",
      "ktutk Households' consumption\n",
      "tjt Income distribution statistics\n",
      "velk Indebtedness\n",
      "mthi Index of producer prices of agricultural products\n",
      "ttohi Index of purchase prices of the means of agricultural production\n",
      "kyki Index of real estate maintenance costs\n",
      "tlv Index of turnover in industry\n",
      "rlv Index of turnover of construction\n",
      "ati Index of wage and salary earnings\n",
      "tti Industrial output\n",
      "inn Innovation\n",
      "kvhv International price comparison\n",
      "tpulk International trade in goods and services\n",
      "atp Job vacancy survey\n",
      "tvki Labour cost index\n",
      "tvtutk Labour cost survey\n",
      "tyti Labour force survey\n",
      "oaiop Liberal adult education\n",
      "ksp Local government sector wages and salaries\n",
      "jvie Mass media statistics\n",
      "klaiv Merchant fleet\n",
      "muutl Migration\n",
      "mkan Motor vehicle stock\n",
      "kvaa Municipal elections\n",
      "teul New orders in manufacturing\n",
      "ttap Occupational accident statistics\n",
      "evaa Parliamentary elections\n",
      "aku Participation in adult education\n",
      "vpa Participation in leisure activities\n",
      "vaenn Population projection\n",
      "vaerak Population structure\n",
      "vamuu Preliminary population statistics\n",
      "pvaa Presidential elections\n",
      "jmhi Price index of public expenditure\n",
      "ashi Prices of dwellings in housing companies\n",
      "ystp Private sector hourly wages\n",
      "yskp Private sector monthly salaries\n",
      "thi Producer price indices\n",
      "pthi Producer price indices for services\n",
      "salatuo Production of electricity and heat\n",
      "opku Progress of studies\n",
      "syyttr Prosecutions, sentences and punishments\n",
      "kjarj Providers of education and educational institutions\n",
      "tyoolot Quality of work life\n",
      "ntp Quarterly national accounts\n",
      "sekn Quarterly sector accounts\n",
      "rtie Railway statistics\n",
      "kihi Real estate prices\n",
      "altp Regional accounts\n",
      "alyr Regional statistics on entrepreneurial activity\n",
      "kora Renovation building\n",
      "asvu Rents of dwellings\n",
      "tkke Research and development\n",
      "velj Restructuring of debts\n",
      "yrtt Statistics on business subsidies\n",
      "tta Statistics on labour disputes\n",
      "eot Statistics on living conditions\n",
      "rpk Statistics on offences and coercive measures\n",
      "ton Statistics on road traffic accidents\n",
      "palhy Statistics on service industry commodities\n",
      "mmtal Statistics on the finances of agricultural and forestry enterprises\n",
      "yrti Structural business and financial statement statistics\n",
      "pra Structure of earnings\n",
      "opiskt Students and qualifications\n",
      "ava Subject choices of students\n",
      "pt Supply, use and input-output tables\n",
      "erop Support for learning\n",
      "akay Time use\n",
      "sijk Transition from school to further education and work\n",
      "ktkk Trend indicator of output\n",
      "kosu Trend indicator of renovation building\n",
      "plv Turnover of service industries\n",
      "klv Turnover of trade\n",
      "sutivi Use of information and communications technology by individuals\n",
      "icte Use of information technology in enterprises\n",
      "ktps Wage and salary indices\n",
      "kotsa Value of household production\n",
      "jate Waste statistics\n",
      "ttvi Volume index of industrial output\n"
     ]
    }
   ],
   "source": [
    "##### imports #####\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# this has to do with pass by value / reference\n",
    "from copy import deepcopy\n",
    "\n",
    "##### config #####\n",
    "\n",
    "english = True\n",
    "# english = False\n",
    "\n",
    "\n",
    "##### helpers #####\n",
    "\n",
    "\n",
    "# notebook replacement of sys.exit()\n",
    "# call with raise StopExecution\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "\n",
    "query_template = {\n",
    "    \"query\": [], # list of query items\n",
    "    \"response\": {\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "}\n",
    "\n",
    "query_item_template = {\n",
    "    \"code\": \"\", # variable\n",
    "    \"selection\": {\n",
    "        \"filter\": \"item\",\n",
    "        \"values\": [] # list of strings\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "##### main #####\n",
    "\n",
    "\n",
    "with requests.Session() as session:\n",
    "\n",
    "    '''\n",
    "    first, some browsing in order to get the correct database\n",
    "    you can do this with a browser too (but translation may become an issue)\n",
    "    '''\n",
    "\n",
    "    lang_id = 'en' if english else 'fi'\n",
    "    base_url = f'https://pxdata.stat.fi/PXWeb/api/v1/{lang_id}/StatFin'\n",
    "    response = session.get(base_url)\n",
    "\n",
    "    for item in response.json():\n",
    "        print(item['id'], item['text'])\n",
    "\n",
    "    # stop execution\n",
    "    raise StopExecution\n",
    "\n",
    "    '''\n",
    "    next, append the id of your thing of interest to the url\n",
    "    (EDIT the adopt below)\n",
    "    '''\n",
    "\n",
    "    catalogue_url = f'{base_url}/adopt'\n",
    "    response = session.get(catalogue_url)\n",
    "\n",
    "    '''\n",
    "    check what .px files are available in the \"catalogue\"\n",
    "    '''\n",
    "    for item in response.json():\n",
    "        print(item['id'], item['text'])\n",
    "\n",
    "    # stop execution\n",
    "    raise StopExecution\n",
    "\n",
    "    '''\n",
    "    once you decide what .px file interests you, \n",
    "    EDIT it below in order to fetch the available data headers\n",
    "\n",
    "    '''\n",
    "\n",
    "    headers_url = f'{base_url}/adopt/statfin_adopt_pxt_11lv.px'\n",
    "    response = session.get(headers_url)\n",
    "\n",
    "    myjson = response.json()\n",
    "    print()\n",
    "    print('variables:', len(myjson['variables']))\n",
    "    print()\n",
    "    for var in myjson['variables']:\n",
    "        print(var['text'])\n",
    "    print()\n",
    "\n",
    "    if english:\n",
    "        tmp_url = headers_url.replace('/en/','/fi/')\n",
    "        response = session.get(tmp_url)\n",
    "        myjson = response.json()\n",
    "        print()\n",
    "        print('the corresponding variables in finnish (may needed in the actual query):')\n",
    "        print()\n",
    "        for var in myjson['variables']:\n",
    "            print(var['text'])\n",
    "        print()\n",
    "\n",
    "    # stop execution\n",
    "    raise StopExecution\n",
    "\n",
    "    '''\n",
    "    okay, but then things get more serious as we build the actual query for the data\n",
    "\n",
    "    first, fetch the maximum values that one can download\n",
    "    (this is kind of hi-tech, got it from the documentation)\n",
    "    (which typically sucks in free & public apis like this)\n",
    "    '''\n",
    "    response = session.get(f'https://statfin.stat.fi/PXWeb/api/v1/{lang_id}/?config')\n",
    "    maxvalues = response.json()['maxValues']\n",
    "\n",
    "    '''\n",
    "    query building (we don't request anything yet)\n",
    "    please edit only the \"for myvar\" line\n",
    "    '''\n",
    "    query = deepcopy(query_template)\n",
    "    total_values = 1\n",
    "    for myvar in ['Vuosi', 'Syntymävaltio', 'Ikä', 'Sukupuoli']: # EDIT this line\n",
    "        myvalues = []\n",
    "        query_item = deepcopy(query_item_template)\n",
    "        for v in myjson['variables']:\n",
    "            if v['code'] == myvar:\n",
    "                myvalues = v['values']\n",
    "        total_values = total_values * len(myvalues)\n",
    "        query_item['code'] = myvar\n",
    "        query_item['selection']['values'] = myvalues\n",
    "        query['query'].append(query_item)\n",
    "    if total_values > maxvalues:\n",
    "        print('your query is too big, try again with fewer variables')\n",
    "        raise StopExecution\n",
    "\n",
    "\n",
    "    '''\n",
    "    obtain the actual data with a \"post\" request\n",
    "    that's like submitting a web form\n",
    "    and cannot be done by gui browsing anymore\n",
    "    '''\n",
    "    response = session.post(headers_url, json=query)\n",
    "\n",
    "    '''\n",
    "    finally, dump the data to a file\n",
    "    '''\n",
    "    myjson = response.json()\n",
    "    with open('test.json', 'w') as handle:\n",
    "        json.dump(myjson, handle, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b4fe27d7-c997-4ca7-9e84-4607261385e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Topics in 'Subject choices of students'\n",
      "statfin_ava_pxt_12a9.px - 12a9 -- Language choices of completers of grades 1 to 9 and of additional education of comprehensive education, school year 2020/2021 -- school year 2023/2024\n",
      "statfin_ava_pxt_12aa.px - 12aa -- Language choices of students who have completed the curriculum in basic education for adults, 2020-2023\n",
      "statfin_ava_pxt_12ad.px - 12ad -- Foreign languages selected by upper secondary level students, 2020-2023\n",
      "statfin_ava_pxt_139d.px - 139d -- Number of foreign languages selected by upper secondary level students, 2020-2023\n",
      "statfin_ava_pxt_139p.px - 139p -- Shares of number of foreign languages selected by upper secondary level students, 2020-2023\n",
      "statfin_ava_pxt_14cd.px - 14cd -- Number of languages completed in grades 1 to 9 and in additional education in comprehensive education, school year 2020/2021 -- school year 2023/2024\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "##### CONFIG #####\n",
    "english = True  \n",
    "base_url = f'https://pxdata.stat.fi/PXWeb/api/v1/{\"en\" if english else \"fi\"}/StatFin'\n",
    "\n",
    "##### TEMPLATES #####\n",
    "query_template = {\n",
    "    \"query\": [],\n",
    "    \"response\": {\"format\": \"json\"}\n",
    "}\n",
    "\n",
    "query_item_template = {\n",
    "    \"code\": \"\",\n",
    "    \"selection\": {\n",
    "        \"filter\": \"item\",\n",
    "        \"values\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "##### STEP 1: VIEW THE DATA SETS #####\n",
    "with requests.Session() as session:\n",
    "    catalogue_url = f\"{base_url}/ava\"\n",
    "    response = session.get(catalogue_url)\n",
    "    datasets = response.json()\n",
    "\n",
    "    print(\"=== Topics in 'Subject choices of students'\")\n",
    "    for d in datasets:\n",
    "        print(d[\"id\"], \"-\", d[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "303a8cda-f724-46c3-a378-a34daced6442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Variables available in statfin_ava_pxt_12ad.px ===\n",
      "Vuosi - Year (4 valores)\n",
      "Alue - Area (21 valores)\n",
      "Sukupuoli - Gender (4 valores)\n",
      "Koulutusaste - Level of education (3 valores)\n",
      "Tiedot - Information (25 valores)\n",
      "\n",
      "=== Variables in finish ===\n",
      "Vuosi - Vuosi (4 valores)\n",
      "Alue - Alue (21 valores)\n",
      "Sukupuoli - Sukupuoli (4 valores)\n",
      "Koulutusaste - Koulutusaste (3 valores)\n",
      "Tiedot - Tiedot (25 valores)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "base_url = f'https://pxdata.stat.fi/PXWeb/api/v1/{\"en\" if english else \"fi\"}/StatFin'\n",
    "dataset_id = \"statfin_ava_pxt_12ad.px\"\n",
    "\n",
    "with requests.Session() as session:\n",
    "    headers_url = f\"{base_url}/ava/{dataset_id}\"\n",
    "    response = session.get(headers_url)\n",
    "    myjson = response.json()\n",
    "\n",
    "    print(f\"=== Variables available in {dataset_id} ===\")\n",
    "    for var in myjson[\"variables\"]:\n",
    "        print(f\"{var['code']} - {var['text']} ({len(var['values'])} valores)\")\n",
    "\n",
    "    fi_headers_url = headers_url.replace(\"/en/\", \"/fi/\")\n",
    "    response = session.get(fi_headers_url)\n",
    "    myjson_fi = response.json()\n",
    "\n",
    "    print(\"\\n=== Variables in finish ===\")\n",
    "    for var in myjson_fi[\"variables\"]:\n",
    "        print(f\"{var['code']} - {var['text']} ({len(var['values'])} valores)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "615b5328-d43d-4964-a7ac-3b9c0a7fd188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data downloaded and saved in 'subject_choices_12ad.json''\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "query_template = {\n",
    "    \"query\": [],\n",
    "    \"response\": {\"format\": \"json\"}\n",
    "}\n",
    "\n",
    "query_item_template = {\n",
    "    \"code\": \"\",\n",
    "    \"selection\": {\n",
    "        \"filter\": \"item\",\n",
    "        \"values\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "with requests.Session() as session:\n",
    "    headers_url = f\"{base_url}/ava/{dataset_id}\"\n",
    "    fi_headers_url = headers_url.replace(\"/en/\", \"/fi/\")\n",
    "    response = session.get(fi_headers_url)\n",
    "    myjson_fi = response.json()\n",
    "\n",
    "    query = deepcopy(query_template)   # We build the query using all available values\n",
    "    for v in myjson_fi[\"variables\"]:\n",
    "        query_item = deepcopy(query_item_template)\n",
    "        query_item[\"code\"] = v[\"code\"]\n",
    "        query_item[\"selection\"][\"values\"] = v[\"values\"]  # selec all\n",
    "        query[\"query\"].append(query_item)\n",
    "\n",
    "    response = session.post(headers_url, json=query) # We download the data\n",
    "    data = response.json()\n",
    "\n",
    "    # Save the document\n",
    "    with open(\"subject_choices_12ad.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(\"✅ Data downloaded and saved in 'subject_choices_12ad.json''\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc0e70b-1a6e-40f7-aba7-d12ae5968d8a",
   "metadata": {},
   "source": [
    "## How to submit my solutions?\n",
    "\n",
    "Open a Terminal tab (e.g. <tt>File $\\rightarrow$ New $\\rightarrow$ Terminal</tt>, copy-paste the following into the Terminal command prompt, and press enter:\n",
    "<pre>\n",
    "  /home/varpha/dan/menu.py\n",
    "</pre>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
