{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebabbbab-c235-4789-a004-14e69d06ec9d",
   "metadata": {},
   "source": [
    "# Data Analytics Fall 2025 &mdash; Exercises 1\n",
    "\n",
    "### XXXXX XXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c82629f-43fd-4272-a4a2-97addd6614b6",
   "metadata": {},
   "source": [
    "## Problem 1. Documentation\n",
    "- Browse through the Python and Numpy documentation\n",
    "- Find a function that a) interests you, and b) has a messy documentation\n",
    "- Play with the function and find simple use cases\n",
    "- Explain the function to your anonymous peer reviewer.\n",
    "\n",
    "Please write a nice and clear explanation. Include some elementary examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f10ef9-abf7-48c2-8a66-33679ade981e",
   "metadata": {},
   "source": [
    "### numpy.bitwise_and"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1b8b95-e1ee-45d6-be87-dce7a7e65d71",
   "metadata": {},
   "source": [
    "#### Short description of the function\n",
    "\n",
    "The function chosen for this exercise is the (rather obscure) `numpy.bitwise_and`. Documentation is available at [https://numpy.org/doc/stable/reference/generated/numpy.bitwise_and.html](https://numpy.org/doc/stable/reference/generated/numpy.bitwise_and.html).\n",
    "\n",
    "Brief terminology beforehand: \n",
    "- The numbers we use in our day-to-day lives are *decimal numbers*, meaning that they exist as base10 numbers, represented by numbers between 0-9. \n",
    "- Bitwise operations are performed on *binary numbers*, which are numbers in base2, only represented by numbers 0 and 1. \n",
    "- Another relatively common format (but not used in this example) is *hexadecimal numbers*, which exist as base16 representations, written with numbers 0-9 and letters A-F. \n",
    "\n",
    "In short, `numpy.bitwise_and` compares the values of two input parameters as binary values, and returns a binary value representing the bits where both input values a value '1'. This is easier visualized by the example below: \n",
    "\n",
    "```\n",
    "Input a: 13 ---> 0000 1101  # This is the binary representation of decimal number 13\n",
    "Input b: 17 ---> 0001 0001  # This is the binary representation of decimal number 17\n",
    "--------------------------\n",
    "bitwise_and ---> 0000 0001  # Only the least significant bit is '1' in both a and b\n",
    "```\n",
    "\n",
    "The parameters for `bitwise_and` are turned into binary values, and the corresponding bits of the binary value are compared together. When both are '1', the resulting bit is '1'. In all other cases, the resulting bit is '0'. In the above case, the numbers 13 and 17 transformed into binary values only match at the very rightmost bit (the least significant bit), resulting in a resulting binary value of 00000001 (this also corresponds to integer 1). \n",
    "\n",
    "Another example: \n",
    "```\n",
    "Decimal 21  ---> 0001 0101\n",
    "Decimal 55  ---> 0011 0111\n",
    "--------------------------\n",
    "bitwise_and ---> 0001 0101 <--- Decimal 21\n",
    "```\n",
    "Taking a `bitwise_and` of decimal numbers 55 and 21 results in a decimal number 21, as all the '1' bits in the binary representation of decimal 21 also occur at the same locations in the binary representation of decimal number 55."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f1a03a-09e2-4675-97ef-f008e6366a29",
   "metadata": {},
   "source": [
    "#### Function signature and parameter explanation\n",
    "\n",
    "The function definition for bitwise_and is as follows: \n",
    "```python\n",
    "numpy.bitwise_and(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj]) = <ufunc 'bitwise_and'>\n",
    "```\n",
    "\n",
    "The function requires two mandatory parameters, `x1`and `x2`. Both are required to be `array_like`, that is, both parameters must be able to be converted to NumPy arrays. Some common object types that fit this description are arrays (of course), NumPy arrays, lists and tuples. Parameters `x1` and `x2` are compared to each other in the `bitwise_and` operation.\n",
    "\n",
    "The `bitwise_and` function also takes in some optional parameters that have to be specified by keywords (as indicated by the `/` as the third parameter in the function signature). The optional parameters are as follows: \n",
    "- `out`, default value `None`: This parameter can be used to specify a variable where the output is to be stored (needs to be of same shape as the inputs). With default value, a new array is returned by the function. \n",
    "- `where`, default value `True`: This can be used to set a condition on when the `bitwise_and` operation is to be performed. If the condition specified here evaluates to `False`, the value in the output array will not be modified (or will remain uninitialized in an empty array). \n",
    "- `casting`, default value `same_kind`: This parameter controls on whether `bitwise_and` can be performed between different data types, for example, between integers and floats. Possible values: \n",
    "    - `same_kind`: data type of the first input array is maintained\n",
    "    - `no`: error is raised if data types for the two arrays are different\n",
    "    - `equiv`: casting between integer and floating-point types is allowed when necessary\n",
    "- `order`, default value `K`: this has to do with memory layouts for the output array. We're not even going to go there, leave this as default unless you really need to play around. \n",
    "- `dtype`, default value `None`: this parameter can be used to manually set output data type. With default value, data type will be inferred from array contents. \n",
    "- `subok`, default value `True`: this will only be needed if you extend `ndarray`to create subclasses - when `True`, the output will be of that extended subclass, with `False` the output array will default to the unextended `ndarray` class. If you have no idea what I'm talking about here, you will not need to touch this parameter. \n",
    "\n",
    "The universal function (`<ufunc 'bitwise_and'>`) implements the operator `&` that can be used directly instead of calling the function separately. \n",
    "\n",
    "Instead of:\n",
    "```python\n",
    "result = np.bitwise_and(array1, array2)\n",
    "```\n",
    "you can write:\n",
    "```python\n",
    "result = array1 & array2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5882499a-c2e4-4d7d-b657-0098ff004f5f",
   "metadata": {},
   "source": [
    "#### Use cases for numpy.bitwise_and()\n",
    "\n",
    "\"All this sounds awfully complex, why should I use `bitwise_and`?\" you may ask - well, here's why. \n",
    "\n",
    "##### Flagging\n",
    "\n",
    "A flag is a value that is either `True` or `False`, and can be used to bind additional information to a row of data. Information about multiple flags can be embedded into a single column of data by utilizing binary values and bitwise operations. \n",
    "\n",
    "Consider a dataset on performed experiments, where the following observations have been made, and incorporated as flags in binary values in field `FLAGS`: \n",
    "- FLAG_A: `0001` (Measuring error)\n",
    "- FLAG_B: `0010` (Test performer fell asleep mid-test)\n",
    "- FLAG_C: `0100` (Technical malfunction mid-test)\n",
    "- FLAG_D: `1000` (Experiment target ate the measuring device)\n",
    "\n",
    "If you have a value of `0110` in field `FLAGS`, it would imply that during this specific experiment, a technical malfunction occurred, and the test performer fell asleep. A value of `1001` would indicate that there was a measuring error, and the experiment target ate the measuring device. A value of `0000` would indicate no issues were encountered during the experiment. The number of bits can be increased as new flags are needed, without having to increase the number of columns in the dataset. \n",
    "\n",
    "`bitwise_and` can be used to fetch all experiments with specific flags: \n",
    "\n",
    "```python\n",
    "experiments_with_measuring_errors = np.bitwise_and(all_experiments, FLAG_A)\n",
    "```\n",
    "Example code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf75c9-b56a-4730-ba7c-a0e043f170e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "\n",
    "# Flag definitions\n",
    "FLAG_A = 0b0001 # Measuring error\n",
    "FLAG_B = 0b0010 # Test performer fell asleep mid-test\n",
    "FLAG_C = 0b0100 # Technical malfunction\n",
    "FLAG_D = 0b1000 # Experiment target ate measuring device\n",
    "\n",
    "# Initialize np.array for experiment flags\n",
    "experiment_flags = [0b0000, 0b0110, 0b0001, 0b0000, 0b1000, 0b1111, 0b1100, 0b0101, 0b0100]\n",
    "# Note that these print out as integers, as each binary value can be represented as an integer\n",
    "experiment_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad2703-4a31-4a49-aaee-139ed8c77854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter with flags\n",
    "technical_malfunction = np.bitwise_and(experiment_flags, FLAG_C) == FLAG_C\n",
    "measuring_device_eaten = np.bitwise_and(experiment_flags, FLAG_D) == FLAG_D\n",
    "\n",
    "print(\"Technical malfunction:\\t\", *technical_malfunction)\n",
    "print(\"Measuring device eaten:\\t\", *measuring_device_eaten)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a5a7d1-733d-465e-9bba-6b7f0fbbd34d",
   "metadata": {},
   "source": [
    "##### Filtering\n",
    "\n",
    "`bitwise_and` can be used in applying filters to a dataset, in a similar way that boolean masking works. `bitwise_and` can be used to create a `True`/`False` filter, which can then be applied to an array to only retain values within wanted bounds. \n",
    "\n",
    "The example below uses `bitwise_and` to retrieve temperatures at which water would be in a liquid state (between 0 and 100 degrees Celcius). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300fe0b4-a455-453f-bf30-131d5d310e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up an array of temperatures\n",
    "temperatures = np.array([20.0, -15.0, 0.0, 1.2, 4.5, 85.0, 102.0, -0.2, 44.3, 24.4, -11.2, -130.1, 2034.4])\n",
    "\n",
    "# Create True/False filter arrays for upper and lower limits of temperatures\n",
    "above_freezing = temperatures > 0.0\n",
    "below_boiling = temperatures < 100.0\n",
    "\n",
    "# Current content of filters:\n",
    "print(\"Temperatures above freezing:\\t\", *above_freezing)\n",
    "print(\"Temperatures below boiling:\\t\", *below_boiling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39945c4d-073d-46d9-8556-37b94966f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine filter masks to one filter, where temperatures are included only if both separate filters indicate \"True\"\n",
    "liquid_state = above_freezing & below_boiling # Operator '&' used here in place of 'bitwise_and(above_freezing, below_boiling)'\n",
    "print(\"Combined filter:\\t\", *liquid_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0f2c83-821f-48fe-b991-a6418a085997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filter to temperatures (applying rounding to keep decimals when printing)\n",
    "filtered_temperatures = np.round(temperatures[liquid_state], 2)\n",
    "\n",
    "# Display results\n",
    "print(\"Temperatures at which water is in liquid state:\\t\", *filtered_temperatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae02082f-b5f7-47f0-a132-66aaebbe3c27",
   "metadata": {},
   "source": [
    "##### Low-level data access\n",
    "\n",
    "Low-level data access works in a similar way as handling flags, no separate code is provided here. \n",
    "\n",
    "##### Other uses in different fields\n",
    "\n",
    "`bitwise_and` can also be used for other tasks that we will most likely not encounter during this course. Bitwise operations are common in the field of encryption. Images can be processed and analyzed with bitwise operations. Network and signal processing also rely to bitwise operations to some extent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe4d8d-c0fc-43f7-b336-3fc95dd0dd76",
   "metadata": {},
   "source": [
    "## Problem 2. Map, Lambda, Groupby\n",
    "In this problem, only plain python may be used, no numpy.<br/>\n",
    "The following links may be helpful:\n",
    "- [sorting howto](https://docs.python.org/3/howto/sorting.html)\n",
    "- [lambda sorting](https://blogboard.io/blog/knowledge/python-sorted-lambda)\n",
    "- [itertools groupby](https://stackoverflow.com/questions/773/how-do-i-use-itertools-groupby).\n",
    "\n",
    "Using the code cell below, read a csv (real wind turbine data) into a list of dicts.<br/>\n",
    "Then do the following:\n",
    "- a) using map, convert the timestamps into the format <b>MM/dd/yyyy HH:mm:ss</b>, e.g. 11/04/2018 09:10:43\n",
    "- b) using sorted and lambda, sort the rows according to increasing rotorspeed\n",
    "- c) add a column called <b><i>WindSpeed_Group</i></b> that contains the letter A, B or C, where A = less than 5mps, B = 5-10mps, C = more than 10mps. Try to use [itertools.groupby](https://docs.python.org/3/library/itertools.html#itertools.groupby) (although it may not be very smart).\n",
    "\n",
    "In your handin, include the code that does a) - c) above. No need to save the modified data. Here is the code for reading the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb4d7b0-f176-4df6-abed-f680ff3ee56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getuser\n",
    "import csv\n",
    "user = getuser()\n",
    "csv_location = f'/home/varpha/dan/private/{user}' + \\\n",
    "                f'/exrc_01/data/prob2_{user}.csv'\n",
    "with open(csv_location) as handle:\n",
    "    mydata = list(csv.DictReader(handle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cece8eaa-d2f4-44e7-a39c-5d06dfb7fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data that was read in\n",
    "print(mydata[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0982a9c9-1beb-4125-9686-0859e188cf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for part b)\n",
    "# Get the lowest and highest rotor speeds\n",
    "min_rotor_speed = min([x[\"RotorSpeedAve\"] for x in mydata])\n",
    "max_rotor_speed = max([x[\"RotorSpeedAve\"] for x in mydata])\n",
    "print(f\"Minimum: {min_rotor_speed}, maximum: {max_rotor_speed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a2d0bf-d984-49ac-884c-77bd89ec1cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------- #\n",
    "# a) convert the timestamps into format MM/dd/yyyy HH:mm:ss with 'map'   #\n",
    "# ---------------------------------------------------------------------- #\n",
    "\n",
    "# Note: no DateTime, just string manipulation. This could also have been done as a two-liner,\n",
    "# but it would not include the use of 'map' as required by the problem. \n",
    "\n",
    "# Split off the decimals from each timstamps, only pick the part before the decimal point.\n",
    "# Make these into a list of dictionaries containing only TimeStamp key ({\"TimeStamp\": <new timestamp>}),\n",
    "# as this makes updating the original list of dictionaries easier later on.\n",
    "new_timestamps = list(map(lambda x: {\"TimeStamp\": x[\"TimeStamp\"].split('.')[0]}, mydata))\n",
    "\n",
    "# Check that timestamps look correct for the first few dicts\n",
    "print(new_timestamps[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdec825-1865-4edc-a24f-b915db6d52c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match the original dictionaries with updated timestamps,\n",
    "# loop through each dictionary and update the new timestamp into the dictionary\n",
    "for original, new_ts in zip(mydata, new_timestamps):\n",
    "    original.update(new_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e570ae82-2b3d-4e4c-9300-8e740235e880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check to make sure that data has updated\n",
    "print(mydata[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66730e6-a2b9-4196-a83e-20c494b20c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "# b) using sorted and lambda, sort the rows according to increasing rotorspeed #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "\n",
    "# Save sorted list to new variable\n",
    "sorted_by_rotorspeed = sorted(mydata, key=lambda row: row['RotorSpeedAve'])\n",
    "# Print the first row:\n",
    "print(f\"First row (slowest):\\t{sorted_by_rotorspeed[0]}\\n\")\n",
    "# Print the last row:\n",
    "print(f\"Last row (fastest):\\t{sorted_by_rotorspeed[-1]}\")\n",
    "\n",
    "# Note: it seems like my data was from a time period where there was no wind, or just no data in general.\n",
    "# The code still works - see the min and max values from just after data load. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f616d6-8cce-4629-9f9d-38bc92c3c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------- #\n",
    "# c) add a column called WindSpeed_Group that contains the letter A, B or C, #\n",
    "# where A = less than 5mps, B = 5-10mps, C = more than 10mps. Try to use     #\n",
    "# itertools.groupby (although it may not be very smart).                     #\n",
    "# -------------------------------------------------------------------------- #\n",
    "\n",
    "# Import groupby\n",
    "from itertools import groupby\n",
    "\n",
    "# Helper function for returning category based on wind speed\n",
    "def get_wind_category(speed):\n",
    "    # Handle missing values\n",
    "    if speed == '':\n",
    "        return ''\n",
    "    # Return correct category according to wind speed\n",
    "    elif float(speed) < 5:\n",
    "        return 'A'\n",
    "    elif float(speed) < 10:\n",
    "        return 'B'\n",
    "    elif float(speed) >= 10:\n",
    "        return 'C'\n",
    "\n",
    "# This makes it a bit cleaner in code down below to cast windspeed into floats, \n",
    "# and account for missing values at the same time\n",
    "def get_numerical_windspeed(speed):\n",
    "    if speed == '':\n",
    "        return 0\n",
    "    else: \n",
    "        return float(speed)\n",
    "    \n",
    "# Sort data by windspeed\n",
    "sorted_by_windspeed = sorted(mydata, key=lambda x: get_numerical_windspeed(x['WindSpeed_mps']))\n",
    "\n",
    "# Initialize an empty dict template where grouped data can be stored\n",
    "wind_category_data = {\n",
    "    '': [],\n",
    "    'A': [],\n",
    "    'B': [],\n",
    "    'C': []\n",
    "}\n",
    "\n",
    "windspeed_groups = []\n",
    "\n",
    "# Group by wind speed category\n",
    "for key, val in groupby(sorted_by_windspeed, lambda x: get_wind_category(x['WindSpeed_mps'])):\n",
    "    # I would prefer to add the category here while iterating through all items,\n",
    "    # but it seems to be a pain in the ass to manipulate the original list here, or even\n",
    "    # individual items. \n",
    "    \n",
    "    # Add rows under their correct key in Windspeed-grouped dictionary\n",
    "    wind_category_data[key].extend(list(val))\n",
    "    \n",
    "# Sanity check: number of rows under each wind category\n",
    "print(\"Number of rows by category:\")\n",
    "for key in wind_category_data.keys():\n",
    "    print(key if key != '' else '-', len(wind_category_data[key]))\n",
    "    \n",
    "\n",
    "# (The question is correct, this is a pretty stupid/complex way to do this. I added the easy way after this.)\n",
    "\n",
    "# As the sorted_by_windspeed list is in order from least to most wind, we know that\n",
    "# all the categories will be in order on that list too, so we can just simply update\n",
    "# the correct number of rows on the sorted list without having to even check the windspeeds. \n",
    "cat_a_start_idx = len(wind_category_data[''])\n",
    "cat_b_start_idx = cat_a_start_idx + len(wind_category_data['A'])\n",
    "cat_c_start_idx = cat_b_start_idx + len(wind_category_data['B'])\n",
    "\n",
    "# Update empty groups\n",
    "for row in sorted_by_windspeed[0:cat_a_start_idx]:\n",
    "    row.update({\"WindSpeed_Group\": ''})\n",
    "\n",
    "# Update category A groups\n",
    "for row in sorted_by_windspeed[cat_a_start_idx:cat_b_start_idx]:\n",
    "    row.update({\"WindSpeed_Group\": 'A'})\n",
    "\n",
    "# Update category B groups\n",
    "for row in sorted_by_windspeed[cat_b_start_idx:cat_c_start_idx]:\n",
    "    row.update({\"WindSpeed_Group\": 'B'})\n",
    "\n",
    "# Update category C groups\n",
    "for row in sorted_by_windspeed[cat_c_start_idx:]:\n",
    "    row.update({\"WindSpeed_Group\": 'C'})\n",
    "\n",
    "# Print an example row from sorted list to show that group has been added\n",
    "print('---------------------------')\n",
    "print(\"Last row from sorted list:\")\n",
    "print(sorted_by_windspeed[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb03502-2d20-447f-88f2-922832e64a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Part c) alternative solution (without groupby) ####\n",
    "\n",
    "# Helper function for returning category based on wind speed\n",
    "def get_wind_category(speed):\n",
    "    # Handle missing values\n",
    "    if speed == '':\n",
    "        return ''\n",
    "    # Return correct category according to wind speed\n",
    "    elif float(speed) < 5:\n",
    "        return 'A'\n",
    "    elif float(speed) < 10:\n",
    "        return 'B'\n",
    "    elif float(speed) >= 10:\n",
    "        return 'C'\n",
    "\n",
    "for row in mydata:\n",
    "    row.update({\"WindSpeed_Group\": get_wind_category(row[\"WindSpeed_mps\"])})\n",
    "\n",
    "# Example row of data\n",
    "print(mydata[0])\n",
    "\n",
    "# Much shorter, much cleaner. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc3925-ea46-4069-a059-1adc344dd929",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem 3. Vectorization\n",
    "- Some [general info](https://www.askpython.com/python-modules/numpy/vectorization-numpy)\n",
    "- The code in <b>dan/public/exrc_01/integrator.py</b> contains rudimentary code,<br/>\n",
    "  written in plain python, that numerically integrates a (math) function<br/>\n",
    "  $f\\colon \\mathbb{R} \\to \\mathbb{R}$ over an interval $[a,b]$.\n",
    "- Rewrite the code using numpy and vectorization.\n",
    "- Introduce timings to measure the gain of vectorization.\n",
    "- Use the (math) function $f(x)=- 8 x^{11} - 9 x^{10} + 9 x^{9} - 15$ and interval $[a,b] = [-17, 28]$ to test the code.\n",
    "- Increase the number of subintervals in order to obtain a noticeable difference in the timings.\n",
    "\n",
    "In your handin, include the rewritten code along with the timing measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c835ce8d-475f-469a-94e5-76f98a345700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original code:\n",
    "\n",
    "def create_mesh(a, b, n):\n",
    "    return [a+i*(b-a)/n for i in range(n)]\n",
    "\n",
    "\n",
    "def integrate(f, a, b, n):\n",
    "    sum_of_rectangles = 0\n",
    "    left_endpoints = create_mesh(a,b,n)\n",
    "    mesh_width = (b-a)/n\n",
    "    for left_endpoint in left_endpoints:\n",
    "        midpoint = left_endpoint + mesh_width/2\n",
    "        height = f(midpoint)\n",
    "        sum_of_rectangles += height * mesh_width\n",
    "    return sum_of_rectangles\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return 3*x**2 - 5\n",
    "\n",
    "### main ###\n",
    "\n",
    "# integrate f over [-1,4], dividing the interval to 1000 subintervals\n",
    "myresult = integrate(f,-1,4,1000)\n",
    "print(myresult)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7e4112-fb87-41df-9ced-4a3c9573f42d",
   "metadata": {},
   "source": [
    "#### Note on refactored solution\n",
    "\n",
    "The original implementation uses the following for create_mesh(): `[a+i*(b-a)/n for i in range(n)]`. What this basically does, is that it divides the x-axis from left to right (a to b) into *n* equal segments (number of subintervals, basically), and returns the starting points of all these subintervals on the x-axis. The return value is a list of x-values. \n",
    "\n",
    "Numpy has a function that does the same thing natively: [numpy.linspace](https://numpy.org/doc/stable/reference/generated/numpy.linspace). I have used this function here for simplicity, as I can also get `mesh_width` out of this function without having to calculate it separately with `retstep=True`. The endpoint is excluded with `endpoint=False`, so that the function only returns the left hand side points on the linear space. \n",
    "\n",
    "Linspace seems to be slightly inaccurate at large numbers over short intervals. At 500 steps within interval $[-1, 4]$ the results between original and new were still the same, but increasing the number of steps started to show tiny differences between results. The original code for `create_mesh` could also have been written as follows: \n",
    "\n",
    "```python \n",
    "return a + np.array(range(n)) * (b - a) / n\n",
    "```\n",
    "\n",
    "`numpy.linspace` was kept in this solution because it is more intuitive to read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede8c2a4-231b-4656-8a49-cd0722132b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Rewritten code (functions renamed):\n",
    "\n",
    "def create_mesh_v(a, b, n):\n",
    "    return np.linspace(a, b, n, endpoint=False, retstep=True)    \n",
    "\n",
    "def integrate_v(f_v, a, b, n):\n",
    "    left_endpoints, mesh_width = create_mesh_v(a, b, n)\n",
    "    midpoints = left_endpoints + mesh_width / 2\n",
    "    heights = f_v(midpoints)\n",
    "    sum_of_rectangles = np.sum(heights * mesh_width)\n",
    "    return sum_of_rectangles\n",
    "\n",
    "# This could have been left out, f(x) from original code is still valid for this solution too.     \n",
    "def f_v(x):\n",
    "        return 3*x**2 - 5\n",
    "## Main\n",
    "\n",
    "integrate_v(f, -1, 4, 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c0d44d-5d61-4a32-b0a3-c5920c6f60b5",
   "metadata": {},
   "source": [
    "#### Code and timing tests\n",
    "\n",
    "$f(x)=- 8 x^{11} - 9 x^{10} + 9 x^{9} - 15$ and interval $[a,b] = [-17, 28]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf6b4a-ab18-4b09-8950-20b3e6ef17d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Overwriting function definitions for tests\n",
    "def f(x): \n",
    "    return -8*x**11 - 9*x**10 + 9*x**9 -15\n",
    "\n",
    "def f_v(x):\n",
    "    return -8*x**11 - 9*x**10 + 9*x**9 -15\n",
    "\n",
    "# ----------------------\n",
    "# Test 1: Original code\n",
    "# ----------------------\n",
    "\n",
    "start_time = time.time()\n",
    "result = integrate(f, -17, 28, 100000)\n",
    "orig_duration = time.time() - start_time\n",
    "\n",
    "print(f\"Integration result: {result}, calculation time: {orig_duration}\")\n",
    "\n",
    "# ------------------------\n",
    "# Test 2: Refactored code\n",
    "# ------------------------\n",
    "\n",
    "start_time = time.time()\n",
    "result = integrate_v(f_v, -17, 28, 100000)\n",
    "vect_duration = time.time() - start_time\n",
    "\n",
    "print(f\"Integration result: {result}, calculation time: {vect_duration}\")\n",
    "\n",
    "# Print performance improvement statistics\n",
    "print(f\"Vectorization changed computation time by approximately {round((vect_duration - orig_duration)/orig_duration * 100, 2)} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0670cdbf-167d-45b5-b2b3-24e1bc8ae4e6",
   "metadata": {},
   "source": [
    "## Problem 4. Numpy arrays\n",
    "\n",
    "- The directory <tt>dan/private/exrc_01/data</tt><br/>\n",
    "  contains a csv file (<tt>prob4_XXXXX.csv</tt>) with some weather data.\n",
    "- a) Use [numpy.genfromtxt](https://numpy.org/doc/stable/reference/generated/numpy.genfromtxt.html) to read the file into a 2-dimensional numpy array.<br/>\n",
    "  Use dtype=str in order to not lose the headers.\n",
    "- b) Use Boolean masking to drop the rows that contain <tt>nan</tt> entries.\n",
    "- c) Convert the time entries (standard timestamp) into a human-readable format of your choice.\n",
    "- d) Add a new row that contains the averages of the columns, except <tt>nan</tt> for the time column.\n",
    "\n",
    "In your handin, include the code that does a) - d) above. Do not include any saved data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f9063c-027b-4a9f-bffb-51e2127dc5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# a) use numpy.genfromtxt to read the file into a 2-dimensional numpy array\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Read in data, specify delimiter as comma only\n",
    "data = np.genfromtxt('data/prob4_XXXXX.csv', dtype=str, delimiter=',')\n",
    "\n",
    "# Sanity check with first few rows\n",
    "data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5047ef68-c24a-4dcb-8b01-45ee8033a9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------\n",
    "# b) use boolean masking to drop the rows that contain nan entries\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# Use np.any() to search if any values in the row contain 'nan', exclude if so\n",
    "filtered = data[~np.any(data == 'nan', axis=1)]\n",
    "\n",
    "# Sanity\n",
    "print(f\"Rows in original data: {len(data)}\\nRows in filtered data: {len(filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0104a260-aee8-48a1-898e-811aeb4aa0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# c) Convert time entries into human-readable format of choice\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Get timestamp utilities\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a helper function to convert timestamp from epoch string to human readable string\n",
    "def convert_timestamp(timestamp_str):\n",
    "    return datetime.utcfromtimestamp(float(timestamp_str)).strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "\n",
    "# Vectorize the helper function (so that it will apply to each item of the array separately,\n",
    "# instead of trying to handle the entire np.array at once)\n",
    "timestamp_converter = np.vectorize(convert_timestamp)\n",
    "\n",
    "# Replace the timestamp formats (ignore header row)\n",
    "filtered[1:, 3] = timestamp_converter(filtered[1:, 3])\n",
    "\n",
    "# Sanity check to make sure that correct things changed\n",
    "filtered[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b857470-97dc-4be6-9e5a-ca03601ba13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d) Add a new row that contains averages of the columns, except nan for the time column\n",
    "summary = np.concatenate([\n",
    "    np.mean((filtered[1:, 0:3]).astype(float), axis=0),\n",
    "    ['nan'],\n",
    "    np.mean((filtered[1:, 4:]).astype(float), axis=0)\n",
    "    ])\n",
    "\n",
    "# Append summary row to the end of the filtered data (vstack = vertical stack)\n",
    "filtered = np.vstack([filtered, summary])\n",
    "\n",
    "# Sanity check with last two rows to ensure that summary row was updated\n",
    "filtered[-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf511d-2bff-4d3b-8cd3-2231732df527",
   "metadata": {},
   "source": [
    "## Problem 5. Data download\n",
    "- Start by exploring / running the code in <tt>dan/public/exrc_01/statfi.py</tt>\n",
    "- Choose a topic that interests you. Then try to download a \"lot\" of data of data of that topic (not adoptions please!). Here a lot means something like 500kB - 2MB range. (It's not really a lot but enough that the downloaded data is hard to grasp manually.)\n",
    "- Save your data in one or several json files.\n",
    "\n",
    "In your handin, include the code that you used (no saved data). You also don't need to do anything to your saved data (just be able to download it).\n",
    "Also, tell a few words about your experiences. What problems, if any, did you encounter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e840f7b3-1bf2-45cc-91e5-f938178eb505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "\n",
    "target_data_types = ['Real estate prices']\n",
    "lang = 'en'\n",
    "api_url = f\"https://statfin.stat.fi/PXWeb/api/v1/{lang}/StatFin\"\n",
    "   \n",
    "# Main program\n",
    "\n",
    "with requests.Session() as session:\n",
    "    # Get some configurations out of the way first:\n",
    "    response = session.get('https://statfin.stat.fi/PXWeb/api/v1/fi/?config')\n",
    "    maxvalues = response.json()['maxValues'] # Maximum number of records that can be fetched at once\n",
    "    \n",
    "    # Fetch available data\n",
    "    print(f\"Fetching list of endpoints from StatFin...\")\n",
    "    response = session.get(api_url)\n",
    "    print(f\"Done.\")\n",
    "        \n",
    "    # Determine correct API endpoints (identifiers) for the data to fetch\n",
    "    print(f\"Searching response for target data types: {target_data_types}\")\n",
    "    identifiers = [entry['id'] for entry in response.json() if entry['text'] in target_data_types]\n",
    "    print(f\"Found the following identifiers: {identifiers}\")\n",
    "    \n",
    "    # Get data for all tags\n",
    "    for tag in identifiers:\n",
    "        # Gather filenames for each identifier\n",
    "        response = session.get(f\"{api_url}/{tag}\")\n",
    "        files = [row['id'] for row in response.json()]\n",
    "        \n",
    "        # Fetch the wanted data (in this case: 'statfin_kihi_pxt_13zt.px',\n",
    "        # price indexes for existing real estates on a quarterly basis from 1985)        \n",
    "        file = 'statfin_kihi_pxt_13zt.px' # Note: hardcoding an interesting file, instead of going dynamic with this.\n",
    "        print(f\"Getting contents for file '{file}'\")\n",
    "        response = session.get(f\"{api_url}/{tag}/{file}\")\n",
    "        variables = [var['text'] for var in response.json()['variables']]\n",
    "        print(f\"File '{file}' contained the following variables: {variables}\")\n",
    "        \n",
    "        # Craft request to get actual data with given variables\n",
    "        print(\"Building query...\")\n",
    "        # Query base\n",
    "        query = {\n",
    "            \"query\": [],\n",
    "            \"response\": {\n",
    "                \"format\": \"json\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Initialize values count\n",
    "        total_values = 1\n",
    "        \n",
    "        # Add query items to base\n",
    "        for var in response.json()['variables']:\n",
    "            query_item = {\n",
    "                \"code\": var['code'],\n",
    "                \"selection\": {\n",
    "                    \"filter\": \"item\",\n",
    "                    \"values\": var['values']\n",
    "                }\n",
    "            }\n",
    "            query['query'].append(query_item)\n",
    "            \n",
    "            # Update value count\n",
    "            total_values *= len(var['values'])\n",
    "        \n",
    "        # Stop handling if values exceed limits\n",
    "        if total_values > maxvalues:\n",
    "            print(f\"Query too big. Maximum: {maxvalues} values, query contained {total_values} values.\")\n",
    "        \n",
    "        # Send query\n",
    "        print(f\"Sending query for file {file}...\")\n",
    "        response = session.post(f\"{api_url}/{tag}/{file}\", json=query)\n",
    "        print(f\"Received HTTP response code {response.status_code}\")\n",
    "        \n",
    "        # Save file contents if response was OK\n",
    "        if response.status_code ==  200:\n",
    "            print(f\"Writing response contents to data_{file}.json...\")\n",
    "            with open(f\"data_{file}.json\", \"w\") as handle:\n",
    "                json.dump(response.json(), handle, indent=4)\n",
    "                print(\"Write complete.\")\n",
    "\n",
    "print(\"Data download and write process complete for all files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3797a2-5930-487c-9595-88b97ba7716c",
   "metadata": {},
   "source": [
    "#### Notes on exercise 5\n",
    "\n",
    "I rewrote all the code, but used the original code as a guide - I learn a lot better by doing from scratch by myself. The main reason for rewriting the code was that I wanted to make it more dynamic, configurable with one or two parameters only. The end result is almost that - currently the filename to be downloaded is hardcoded. There's a few possibilities to change that from hardcoded into a more dynamic functionality - the code could list all the files and their descriptions to the user and ask to pick which one to download (and use the identifier as a variable), or the code could simply loop through all files and download everything, or the code could arbitrarily pick the first (or a random file) to download. Just a matter of how to populate `file` variable in the code. \n",
    "\n",
    "I also preferred to create the query item on the fly during the data fetch - there was no need to fret over value and variable referencing, and no need to bring in copy module. A new query is initialized every time data is fetched, and all is good. \n",
    "\n",
    "I also added a check to make sure that we get data (`response_code`=`200`) before writing it to disk.\n",
    "\n",
    "And I also changed the hardcoded URLs into a variable that was reused throughout the code. \n",
    "\n",
    "This was actually probably the easiest out of the problems in this exercise set, as it is the closest to what I have experience with. No major difficulties were encountered along the way, though I did debug my own code by modifying and running the original code at some points. The first POST query gave me a response code of HTTP 400, but a comparison of query JSON produced by the original and my own code revealed that I had forgot to put in a \"filter\" key within the query item list. That fixed, everything worked like a charm. Overall, a fairly simple and useful exercise. \n",
    "\n",
    "The downloaded file is 499 KB in size, but I hope that it's close enough for the requirements of the exercise (500 KB to 2 MB of data). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
