{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebabbbab-c235-4789-a004-14e69d06ec9d",
   "metadata": {},
   "source": [
    "# Data Analytics Fall 2025 &mdash; Exercises 1\n",
    "\n",
    "### Dinesh Bisht (last modified: Sun 31 Aug)\n",
    "\n",
    "### Deadline: Around Tue 16 Sep (to be specified)\n",
    "\n",
    "- Five problems\n",
    "- Minor variations between users\n",
    "- Theme: Python & Numpy (no Pandas allowed)\n",
    "- Theory: see <tt>public/exrc_01</tt>\n",
    "- Make a copy of the original notebook (e.g. <tt>File $\\rightarrow$ Save Notebook As</tt>)<br/>\n",
    "  and add your answers (new cells) there\n",
    "- Please make both your code and your notebook readable\n",
    "- Keep your folder structure up to date by running the config script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde89ee5-64f7-4289-8065-d36dfc25c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('/usr/bin/bash /home/varpha/dan/config.sh');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c82629f-43fd-4272-a4a2-97addd6614b6",
   "metadata": {},
   "source": [
    "## Problem 1. Documentation\n",
    "- Browse through the Python and Numpy documentation\n",
    "- Find a function that a) interests you, and b) has a messy documentation\n",
    "- Play with the function and find simple use cases\n",
    "- Explain the function to your anonymous peer reviewer.\n",
    "\n",
    "Please write a nice and clear explanation. Include some elementary examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f3324d-8f4d-435c-b474-f65700f5a8d4",
   "metadata": {},
   "source": [
    "## Solution 1\n",
    "- Used chatgpt to format and organise the content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45abae67-4257-4806-8331-349f9dce2233",
   "metadata": {},
   "source": [
    "# NumPy `isin` Function Guide\n",
    "\n",
    "The **`numpy.isin`** function tests if each element of an array is present in a second array. It returns a boolean array of the same shape as the first input array, where `True` indicates that the element was found in the second array and `False` indicates it was not.\n",
    "\n",
    "\n",
    "## Basic Syntax\n",
    "\n",
    "``` python\n",
    "numpy.isin(element, test_elements)\n",
    "```\n",
    "\n",
    "-   **element**: The input array whose elements you want to test. This can be an array-like object (e.g., a NumPy array or a list).\n",
    "-   **test_elements**: The array of values you're searching for. This can also be an array-like object.\n",
    "\n",
    "\n",
    "\n",
    "## Basic Examples\n",
    "\n",
    "### 1. Membership Check\n",
    "\n",
    "``` python\n",
    "import numpy as np\n",
    "\n",
    "a = np.array([10, 20, 30, 40, 50])\n",
    "b = [20, 40, 60]\n",
    "\n",
    "result = np.isin(a, b)\n",
    "print(result)\n",
    "```\n",
    "\n",
    "**Output:** [False  True False  True False]\n",
    "\n",
    "**Explanation**: Only `20` and `40` from array `a` are present in array `b`. The resulting boolean array can be used as a mask to filter the original array. It is very common and powerful use case.\n",
    "\n",
    "\n",
    "### 2. Filtering Elements\n",
    "\n",
    "``` python\n",
    "a = np.array([10, 20, 30, 40, 50])\n",
    "b = [20, 40]\n",
    "\n",
    "filtered = a[np.isin(a, b)]\n",
    "print(filtered)\n",
    "```\n",
    "\n",
    "**Output:** [20 40]\n",
    "\n",
    "**Explanation**: We used `isin` as a boolean mask to extract elements from `a` that are in `b`.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "`np.isin` is super useful when you want to perform data cleansing, filtering, removing invalid values or outliers. In short use it whenever you need to keep or remove values based on membership.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe4d8d-c0fc-43f7-b336-3fc95dd0dd76",
   "metadata": {},
   "source": [
    "## Problem 2. Map, Lambda, Groupby\n",
    "In this problem, only plain python may be used, no numpy.<br/>\n",
    "The following links may be helpful:\n",
    "- [sorting howto](https://docs.python.org/3/howto/sorting.html)\n",
    "- [lambda sorting](https://blogboard.io/blog/knowledge/python-sorted-lambda)\n",
    "- [itertools groupby](https://stackoverflow.com/questions/773/how-do-i-use-itertools-groupby).\n",
    "\n",
    "Using the code cell below, read a csv (real wind turbine data) into a list of dicts.<br/>\n",
    "Then do the following:\n",
    "- a) using map, convert the timestamps into the format <b>MM/dd/yyyy HH:mm:ss</b>, e.g. 11/04/2018 09:10:43\n",
    "- b) using sorted and lambda, sort the rows according to increasing rotorspeed\n",
    "- c) add a column called <b><i>WindSpeed_Group</i></b> that contains the letter A, B or C, where A = less than 5mps, B = 5-10mps, C = more than 10mps. Try to use [itertools.groupby](https://docs.python.org/3/library/itertools.html#itertools.groupby) (although it may not be very smart).\n",
    "\n",
    "In your handin, include the code that does a) - c) above. No need to save the modified data. Here is the code for reading the raw data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb4d7b0-f176-4df6-abed-f680ff3ee56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getuser\n",
    "import csv\n",
    "user = getuser()\n",
    "csv_location = f'/home/varpha/dan/private/{user}' + \\\n",
    "                f'/exrc_01/data/prob2_{user}.csv'\n",
    "with open(csv_location) as handle:\n",
    "    mydata = list(csv.DictReader(handle))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5725577-6041-4752-8388-8aed735d1f45",
   "metadata": {},
   "source": [
    "## Solution 2\n",
    "### Used below documents for reference:\n",
    "- [datetime — Basic date and time types](https://docs.python.org/3/library/datetime.html)\n",
    "- [sorting howto](https://docs.python.org/3/howto/sorting.html)\n",
    "\n",
    "### Problem faced\n",
    "- While working on task c) its been found that there are some rows where <b><i>WindSpeed_mps</i></b> value blank and caused the issue. For such cases I added NA for <b><i>WindSpeed_Group</i></b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836333b2-58db-4a48-819f-87d1aaf10c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from getpass import getuser\n",
    "from operator import itemgetter\n",
    "import csv\n",
    "\n",
    "user = getuser()\n",
    "csv_location = f'/home/varpha/dan/private/{user}' + \\\n",
    "                f'/exrc_01/data/prob2_{user}.csv'\n",
    "\n",
    "with open(csv_location) as handle:\n",
    "    mydata = list(csv.DictReader(handle))\n",
    "\n",
    "\n",
    "## mydata = mydata[0:20] # For debugging with sample data\n",
    "\n",
    "\n",
    "# Convert the TimeStamp to MM/dd/yyyy HH:mm:ss format\n",
    "update_mydata = list(map(\n",
    "    lambda data: {\n",
    "        **data,\n",
    "        \"TimeStamp\": datetime.strptime(data[\"TimeStamp\"], \"%Y-%m-%d %H:%M:%S.%f\").strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "    },\n",
    "    mydata\n",
    "))\n",
    "\n",
    "## print(update_mydata) # For debugging results with sample data\n",
    "\n",
    "# Sort the data with RotorSpeed_rpm\n",
    "sort_mydata = sorted(update_mydata,key=itemgetter(\"RotorSpeed_rpm\"))\n",
    "print(sort_mydata)\n",
    "\n",
    "# Add new column WindSpeed_Group\n",
    "def assign_group(data):\n",
    "    try:\n",
    "        ws = float(data[\"WindSpeed_mps\"])\n",
    "        if ws < 5:\n",
    "            group = \"A\"\n",
    "        elif ws <= 10:\n",
    "            group = \"B\"\n",
    "        else:\n",
    "            group = \"C\"\n",
    "    except ValueError:\n",
    "        group = \"NA\"   # Handle empty or bad values\n",
    "\n",
    "    return {**data, \"WindSpeed_Group\": group}\n",
    "\n",
    "\n",
    "add_mydata = list(map(assign_group, update_mydata))\n",
    "\n",
    "\n",
    "## print(add_mydata) # For debugging results with sample data\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc3925-ea46-4069-a059-1adc344dd929",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem 3. Vectorization\n",
    "- Some [general info](https://www.askpython.com/python-modules/numpy/vectorization-numpy)\n",
    "- The code in <tt>dan/public/exrc_01/integrator.py</tt> contains rudimentary code,<br/>\n",
    "  written in plain python, that numerically integrates a (math) function<br/>\n",
    "  $f\\colon \\mathbb{R} \\to \\mathbb{R}$ over an interval $[a,b]$.\n",
    "- Rewrite the code using numpy and vectorization.\n",
    "- Introduce timings to measure the gain of vectorization.\n",
    "- Use the (math) function $f(x)=10 x^{11} + 6 x^{9} - 12 x^{6} - 10$ and interval $[a,b] = [-7, 15]$ to test the code.\n",
    "- Increase the number of subintervals in order to obtain a noticeable difference in the timings.\n",
    "\n",
    "In your handin, include the rewritten code along with the timing measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49816e8-3363-4f07-b1b7-93b5ff0165f8",
   "metadata": {},
   "source": [
    "## Solution 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8475fe11-d905-45fa-94f8-2d3ce699312e",
   "metadata": {},
   "source": [
    "### Option 1: Without NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed960d2b-6cb5-4ace-a2f1-e8f605e92d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def create_mesh(a, b, n):\n",
    "    return [a+i*(b-a)/n for i in range(n)]\n",
    "\n",
    "\n",
    "def integrate(f, a, b, n):\n",
    "    sum_of_rectangles = 0\n",
    "    left_endpoints = create_mesh(a,b,n)\n",
    "    mesh_width = (b-a)/n\n",
    "    for left_endpoint in left_endpoints:\n",
    "        midpoint = left_endpoint + mesh_width/2\n",
    "        height = f(midpoint)\n",
    "        sum_of_rectangles += height * mesh_width\n",
    "    return sum_of_rectangles\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return 3*x**2 - 5\n",
    "\n",
    "\n",
    "### main ###\n",
    "\n",
    "start_time = time.time()\n",
    "# integrate f over [-1,4], dividing the interval to 1000 subintervals\n",
    "myresult = integrate(f,-1,4,10000000)\n",
    "print(myresult) # 39.99999999999631\n",
    "end_time = time.time()\n",
    "print(f'Excecution Time {end_time - start_time}') # Output: Excecution Time 3.9350855350494385"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfde47-1b6e-45bc-a424-18fb1e94e6cf",
   "metadata": {},
   "source": [
    "### Option 2: With NumPy\n",
    "- NumPy solution uses <b><i>Left Endpoint Rule </b></i> unlike <b><i>Midpoint rule </b></i> which is used in Option 1 so the numerical result ie output differs slightly. Objective of this problem was to demonstrate the power of numpy vectorization and the huge performance improvement over a Python loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f354cab0-f386-43c5-a01a-054a9d78ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def integrate(f, a, b, n):\n",
    "    left_endpoints = np.linspace(a,b,n)\n",
    "    mesh_width = (b-a)/n\n",
    "    return np.sum(f(left_endpoints) * mesh_width)\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return 3*x**2 - 5\n",
    "\n",
    "\n",
    "### main ###\n",
    "\n",
    "start_time = time.time()\n",
    "# integrate f over [-1,4], dividing the interval to 1000 subintervals\n",
    "myresult = integrate(f,-1,4,10000000)\n",
    "print(myresult) # Output: 40.000006250000624\n",
    "end_time = time.time()\n",
    "print(f'Excecution Time {end_time - start_time}') # Output: Excecution Time 0.10708189010620117"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0670cdbf-167d-45b5-b2b3-24e1bc8ae4e6",
   "metadata": {},
   "source": [
    "## Problem 4. Numpy arrays\n",
    "\n",
    "- The directory <tt>dan/private/exrc_01/data</tt><br/>\n",
    "  contains a csv file (<tt>prob4_ah4323.csv</tt>) with some weather data.\n",
    "- a) Use [numpy.genfromtxt](https://numpy.org/doc/stable/reference/generated/numpy.genfromtxt.html) to read the file into a 2-dimensional numpy array.<br/>\n",
    "  Use dtype=str in order to not lose the headers.\n",
    "- b) Use Boolean masking to drop the rows that contain <tt>nan</tt> entries.\n",
    "- c) Convert the time entries (standard timestamp) into a human-readable format of your choice.\n",
    "- d) Add a new row that contains the averages of the columns, except <tt>nan</tt> for the time column.\n",
    "\n",
    "In your handin, include the code that does a) - d) above. Do not include any saved data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b53baf-7118-4dab-a83d-5a11015af150",
   "metadata": {},
   "source": [
    "## Solution 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1425e17-1b13-4a1f-8b9c-91bdaf79dacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0t/k860jmxx4d5__9h3p08nh77m0000gn/T/ipykernel_22424/2293582649.py:30: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  dt = datetime.datetime.utcfromtimestamp(ts)   # convert to UTC datetime\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "from getpass import getuser\n",
    "import csv\n",
    "\n",
    "\n",
    "user = getuser()\n",
    "csv_location = f'/home/varpha/dan/private/{user}' + \\\n",
    "                f'/exrc_01/data/prob4_{user}.csv'\n",
    "csv_location = \"prob4_ah4323.csv\"\n",
    "# Read entire CSV as strings to create a 2d array\n",
    "data = np.genfromtxt(csv_location, delimiter=\",\", dtype=str)\n",
    "\n",
    "## np.savetxt(\"output_data.csv\", data, delimiter=\",\", fmt=\"%s\") ## For debugging to see the output\n",
    "\n",
    "# Remove rows containing \"nan\" (string form)\n",
    "clean_data = data[~np.any(data == \"nan\", axis=1)]\n",
    "\n",
    "## np.savetxt(\"output_clean_data.csv\", clean_data, delimiter=\",\", fmt=\"%s\") ## For debugging to see the output\n",
    "\n",
    "# Keep the header and rows separately\n",
    "header, rows = clean_data[0], clean_data[1:]\n",
    "\n",
    "# Find the index of the \"time\" column\n",
    "time_col_idx = np.where(header == \"time\")[0][0]\n",
    "\n",
    "# Converted time series data to MM/dd/yyyy HH:mm:ss format\n",
    "for row in rows:\n",
    "        ts = float(row[time_col_idx])\n",
    "        dt = datetime.datetime.utcfromtimestamp(ts)   # convert to UTC datetime\n",
    "        row[time_col_idx] = dt.strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "\n",
    "# Compute the averages of the columns, except nan for the time column\n",
    "averages = []\n",
    "for i, colname in enumerate(header):\n",
    "    if i == time_col_idx:\n",
    "        averages.append(\"nan\")  # placeholder for time column\n",
    "    else:\n",
    "        col_vals = rows[:, i].astype(float)\n",
    "        averages.append(f\"{np.mean(col_vals):.6f}\")\n",
    "\n",
    "# Stack header + formatted rows back + averages\n",
    "formatted_data_with_header_averages = np.vstack([header, rows, averages]) \n",
    "\n",
    "np.savetxt(\"output_formatted_data.csv\", formatted_data_with_header_averages, delimiter=\",\", fmt=\"%s\") ## For debugging to see the output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf511d-2bff-4d3b-8cd3-2231732df527",
   "metadata": {},
   "source": [
    "## Problem 5. Data download\n",
    "- Start by exploring / running the code in <tt>dan/public/exrc_01/statfi.py</tt>\n",
    "- Choose a topic that interests you. Then try to download a \"lot\" of data of data of that topic. Here a lot means something like 500kB - 2MB range. (It's not really a lot but enough that the downloaded data is hard to grasp manually.)\n",
    "- Save your data in one or several json files.\n",
    "\n",
    "In your handin, include the code that you used (no saved data).\n",
    "Also, tell a few words about your experiences. What problems, if any, did you encounter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ac2a68-82bb-4dc1-9879-068bace12333",
   "metadata": {},
   "source": [
    "## Solution 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d66010-ecb9-4546-916f-043a3635b0ab",
   "metadata": {},
   "source": [
    "## Overall Experience\n",
    "The overall experience was smooth, mainly because the topic I selected (**Railway statistics – rtie**) was not too large. As a result, I did not encounter major issues while working with it.\n",
    "\n",
    "## Key Observation\n",
    "While running the code, I noticed that the chosen language (`en` for English or `fi` for Finnish) only affects the **labels** displayed in the metadata.  \n",
    "\n",
    "However, the actual data extraction always depends on the **variable codes**, which remain constant and are defined in **Finnish**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c58afa9-a14f-4c00-9e1f-9929d73525a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### imports #####\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "\n",
    "# this has to do with pass by value / reference\n",
    "from copy import deepcopy\n",
    "\n",
    "##### config #####\n",
    "\n",
    "english = True\n",
    "# english = False\n",
    "\n",
    "\n",
    "##### helpers #####\n",
    "\n",
    "\n",
    "# notebook replacement of sys.exit()\n",
    "# call with raise StopExecution\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "\n",
    "query_template = {\n",
    "    \"query\": [], # list of query items\n",
    "    \"response\": {\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "}\n",
    "\n",
    "query_item_template = {\n",
    "    \"code\": \"\", # variable\n",
    "    \"selection\": {\n",
    "        \"filter\": \"item\",\n",
    "        \"values\": [] # list of strings\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "##### main #####\n",
    "\n",
    "\n",
    "with requests.Session() as session:\n",
    "\n",
    "    '''\n",
    "    first, some browsing in order to get the correct database\n",
    "    you can do this with a browser too (but translation may become an issue)\n",
    "    '''\n",
    "\n",
    "    lang_id = 'en' if english else 'fi'\n",
    "    base_url = f'https://pxdata.stat.fi/PXWeb/api/v1/{lang_id}/StatFin'\n",
    "    response = session.get(base_url)\n",
    "\n",
    "    for item in response.json():\n",
    "        print(item['id'], item['text'])\n",
    "\n",
    "    # stop execution\n",
    "    # raise StopExecution\n",
    "\n",
    "    '''\n",
    "    next, append the id of your thing of interest to the url\n",
    "    (EDIT the adopt below)\n",
    "    '''\n",
    "\n",
    "    catalogue_url = f'{base_url}/rtie'\n",
    "    response = session.get(catalogue_url)\n",
    "\n",
    "    '''\n",
    "    check what .px files are available in the \"catalogue\"\n",
    "    '''\n",
    "    for item in response.json():\n",
    "        print(item['id'], item['text'])\n",
    "\n",
    "    # stop execution\n",
    "    # raise StopExecution\n",
    "\n",
    "    '''\n",
    "    once you decide what .px file interests you, \n",
    "    EDIT it below in order to fetch the available data headers\n",
    "\n",
    "    '''\n",
    "\n",
    "    headers_url = f'{base_url}/rtie/statfin_rtie_pxt_12lz.px'\n",
    "    response = session.get(headers_url)\n",
    "\n",
    "    myjson = response.json()\n",
    "    print()\n",
    "    print('variables:', len(myjson['variables']))\n",
    "    print()\n",
    "    for var in myjson['variables']:\n",
    "        print(var['text'])\n",
    "    print()\n",
    "\n",
    "    if english:\n",
    "        tmp_url = headers_url.replace('/en/','/fi/')\n",
    "        response = session.get(tmp_url)\n",
    "        myjson = response.json()\n",
    "        print()\n",
    "        print('the corresponding variables in finnish (may needed in the actual query):')\n",
    "        print()\n",
    "        for var in myjson['variables']:\n",
    "            print(var['text'])\n",
    "        print()\n",
    "\n",
    "    # stop execution\n",
    "    # raise StopExecution\n",
    "\n",
    "    '''\n",
    "    okay, but then things get more serious as we build the actual query for the data\n",
    "\n",
    "    first, fetch the maximum values that one can download\n",
    "    (this is kind of hi-tech, got it from the documentation)\n",
    "    (which typically sucks in free & public apis like this)\n",
    "    '''\n",
    "    response = session.get(f'https://statfin.stat.fi/PXWeb/api/v1/{lang_id}/?config')\n",
    "    maxvalues = response.json()['maxValues']\n",
    "\n",
    "    '''\n",
    "    query building (we don't request anything yet)\n",
    "    please edit only the \"for myvar\" line\n",
    "    '''\n",
    "    query = deepcopy(query_template)\n",
    "    total_values = 1\n",
    "    for myvar in ['Vetokalustolaji', 'Vuosi', 'Tiedot']: # EDIT this line and Value must be in Finnish\n",
    "        myvalues = []\n",
    "        query_item = deepcopy(query_item_template)\n",
    "        for v in myjson['variables']:\n",
    "            if v['code'] == myvar:\n",
    "                myvalues = v['values']\n",
    "        total_values = total_values * len(myvalues)\n",
    "        query_item['code'] = myvar\n",
    "        query_item['selection']['values'] = myvalues\n",
    "        query['query'].append(query_item)\n",
    "    if total_values > maxvalues:\n",
    "        print('your query is too big, try again with fewer variables')\n",
    "        raise StopExecution\n",
    "\n",
    "\n",
    "    '''\n",
    "    obtain the actual data with a \"post\" request\n",
    "    that's like submitting a web form\n",
    "    and cannot be done by gui browsing anymore\n",
    "    '''\n",
    "    response = session.post(headers_url, json=query)\n",
    "\n",
    "    '''\n",
    "    finally, dump the data to a file\n",
    "    '''\n",
    "    myjson = response.json()\n",
    "    with open('test.json', 'w') as handle:\n",
    "        json.dump(myjson, handle, indent=4)\n",
    "\n",
    "    print(\"file created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc0e70b-1a6e-40f7-aba7-d12ae5968d8a",
   "metadata": {},
   "source": [
    "## How to submit my solutions?\n",
    "\n",
    "Open a Terminal tab (e.g. <tt>File $\\rightarrow$ New $\\rightarrow$ Terminal</tt>, copy-paste the following into the Terminal command prompt, and press enter:\n",
    "<pre>\n",
    "  /home/varpha/dan/menu.py\n",
    "</pre>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myproject_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
