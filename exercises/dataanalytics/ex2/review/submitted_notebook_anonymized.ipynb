{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebabbbab-c235-4789-a004-14e69d06ec9d",
   "metadata": {},
   "source": [
    "# Data Analytics Fall 2025 &mdash; Exercises 2\n",
    "\n",
    "### XXXXX XXXXX\n",
    "\n",
    "Last modified: Tue 16 Sep before session\n",
    "\n",
    "- Five problems + round 1 peer review\n",
    "- Theme: data wrangling with **pandas** (please use pandas in each problem)\n",
    "- Please make both your code and your notebook readable\n",
    "- Keep your originals up to date by running the code cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde89ee5-64f7-4289-8065-d36dfc25c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('/usr/bin/bash /home/varpha/dan/config.sh');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2722411d-45e4-4faa-bafa-06aa0d4bf31b",
   "metadata": {},
   "source": [
    "## Round 1 peer review\n",
    "\n",
    "- Submit your round 01 solutions in the **round 01** menu <br>\n",
    "  (run `/home/varpha/dan/menu.py` in the terminal).\n",
    "- After some effort, you should be able to access an anonymized submission of another student.\n",
    "\n",
    "Write a few paragraphs of text (plain or markdown) into your favourite text editor and submit that text in  the **round 02** menu. \n",
    "\n",
    "Please address the following issues:\n",
    " \n",
    "- Are the solutions okay? Can you understand / run the code?<br/>\n",
    "  (as opposed to some wishful brainless copy-pasting done in a hurry)\n",
    "- What do you think about the solutions? To what extent has AI been used blindly without explaining the usage?\n",
    "- How many points out of 5 do the solutions deserve as a whole?\n",
    "- How many points out of 5 would you give to yourself and why?\n",
    "- Any feedback or comments to Harri?\n",
    "  \n",
    "Harri will read and grade your reviews as follows:\n",
    "- nonexistent or nearly so = 0p\n",
    "- something written = 1-2p\n",
    "- well written 3p."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe4d8d-c0fc-43f7-b336-3fc95dd0dd76",
   "metadata": {},
   "source": [
    "## Problem 1. Profiles\n",
    "\n",
    "The file `private/exrc_02/data/XXXXX_prob01_profiles.csv` contains some user profiles.\n",
    "Read the csv into a pandas DataFrame and reorganize it as follows:\n",
    "\n",
    "a) Separate the name and address columns so that there are separate columns for\n",
    "- first name\n",
    "- last name\n",
    "- street address\n",
    "- state\n",
    "- postal code.\n",
    "\n",
    "Keep also the ssn, username, sex, mail and birthdate columns. Drop all the other columns.\n",
    "\n",
    "b) Print all entries where the last name begins with the letter A, sorted by:\n",
    "- sex (ladies first)\n",
    "- state (alphabetically)\n",
    "- age (youngest first).\n",
    "\n",
    "I.e. print the entries three times, in these three different ways. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90649494-34b7-4f95-8e6e-fd369f7da7b2",
   "metadata": {},
   "source": [
    "## Use of AI in this exercise\n",
    "I leveraged AI tools to:\n",
    "- Learn new concepts related to tasks\n",
    "- Brainstorm solutions\n",
    "- Generate and adapt sample code to solve the problems in different ways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78afa0ec-ae08-42dc-b10c-6a969a76bcac",
   "metadata": {},
   "source": [
    "## Solution 1. Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9c32ac6-9c55-4e99-948d-f8af7771b750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------\n",
      "   All entries where the last name begins with the letter A, sorted by sex (ladies first)   \n",
      "--------------------------------------------------------------------------------------------\n",
      "        ssn        username first_name last_name sex                   street_address state postal_code                      mail  birthdate\n",
      "618-23-7085   michaelmartin     Alyssa  Anderson   F     12509 Jordan Rapid Suite 221    RI       67312       kmiller@hotmail.com 1997-08-23\n",
      "485-57-5914 robersonbeverly   Patricia   Andrade   F                 037 Warner Place    VI       46571    brownjames@hotmail.com 1975-12-30\n",
      "701-62-8909       michael06   Kathleen    Arroyo   F      7433 Joshua Light Suite 423    GU       35854      hollyweber@yahoo.com 1949-02-24\n",
      "082-24-7955     ecunningham     Gloria   Aguilar   F        435 Escobar Wall Apt. 304    ME       71133 jennifersanchez@gmail.com 1912-02-22\n",
      "475-87-5012 justinrodriguez    Jessica  Anderson   F     323 Vasquez Village Apt. 866    ND       62196       msnyder@hotmail.com 1938-02-06\n",
      "579-46-9356       tanyakemp       Beth  Arellano   F    3585 Tanner Mission Suite 928    NM       91034        ldoyle@hotmail.com 1916-08-11\n",
      "392-79-6259       phyllis60    Chelsea     Adams   F      560 Cynthia Summit Apt. 271    MN       51695     kelseydavis@gmail.com 1946-12-12\n",
      "430-97-1375       fbartlett     Andrea   Andrews   F     620 Sanders Summit Suite 382    NY       95796       natalie15@yahoo.com 1929-10-24\n",
      "607-43-8328 destinygonzalez    Melinda     Allen   F                   736 Reese Loaf    FL       60678         bross@hotmail.com 1921-02-21\n",
      "606-20-1906        afreeman    Rebecca     Allen   F                 827 Jason Island    PA       29406   cherylbuckley@gmail.com 1958-04-02\n",
      "839-68-0180     justingrant    Spencer    Arnold   M         2292 Dyer Park Suite 698    HI       30374  millerjennifer@gmail.com 2002-06-11\n",
      "863-94-6652          bsmith      David  Anderson   M 9299 Kimberly Mountains Apt. 160    GA       66995         cjensen@gmail.com 2007-05-06\n",
      "891-40-6262       htrujillo     Jerome    Adkins   M        5763 Ashley View Apt. 054    NJ       33403        teresa97@gmail.com 1938-07-02\n",
      "417-66-5820        vbaldwin    Michael     Allen   M        8032 Yvonne Cove Apt. 472    MP       87536       danderson@gmail.com 1960-05-24\n",
      "557-08-4915       jeffrey73      Roger  Anderson   M    1900 Bradley Skyway Suite 346    WY       83697    mandystone@hotmail.com 2019-12-14\n",
      "202-81-9617    lawsoneugene     Darren Alexander   M     67790 Brandon Wells Apt. 209    FM       92577    sblankenship@gmail.com 1964-06-17\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "   All entries where the last name begins with the letter A, sorted by state (alphabetically)   \n",
      "------------------------------------------------------------------------------------------------\n",
      "        ssn        username first_name last_name sex                   street_address state postal_code                      mail  birthdate\n",
      "607-43-8328 destinygonzalez    Melinda     Allen   F                   736 Reese Loaf    FL       60678         bross@hotmail.com 1921-02-21\n",
      "202-81-9617    lawsoneugene     Darren Alexander   M     67790 Brandon Wells Apt. 209    FM       92577    sblankenship@gmail.com 1964-06-17\n",
      "863-94-6652          bsmith      David  Anderson   M 9299 Kimberly Mountains Apt. 160    GA       66995         cjensen@gmail.com 2007-05-06\n",
      "701-62-8909       michael06   Kathleen    Arroyo   F      7433 Joshua Light Suite 423    GU       35854      hollyweber@yahoo.com 1949-02-24\n",
      "839-68-0180     justingrant    Spencer    Arnold   M         2292 Dyer Park Suite 698    HI       30374  millerjennifer@gmail.com 2002-06-11\n",
      "082-24-7955     ecunningham     Gloria   Aguilar   F        435 Escobar Wall Apt. 304    ME       71133 jennifersanchez@gmail.com 1912-02-22\n",
      "392-79-6259       phyllis60    Chelsea     Adams   F      560 Cynthia Summit Apt. 271    MN       51695     kelseydavis@gmail.com 1946-12-12\n",
      "417-66-5820        vbaldwin    Michael     Allen   M        8032 Yvonne Cove Apt. 472    MP       87536       danderson@gmail.com 1960-05-24\n",
      "475-87-5012 justinrodriguez    Jessica  Anderson   F     323 Vasquez Village Apt. 866    ND       62196       msnyder@hotmail.com 1938-02-06\n",
      "891-40-6262       htrujillo     Jerome    Adkins   M        5763 Ashley View Apt. 054    NJ       33403        teresa97@gmail.com 1938-07-02\n",
      "579-46-9356       tanyakemp       Beth  Arellano   F    3585 Tanner Mission Suite 928    NM       91034        ldoyle@hotmail.com 1916-08-11\n",
      "430-97-1375       fbartlett     Andrea   Andrews   F     620 Sanders Summit Suite 382    NY       95796       natalie15@yahoo.com 1929-10-24\n",
      "606-20-1906        afreeman    Rebecca     Allen   F                 827 Jason Island    PA       29406   cherylbuckley@gmail.com 1958-04-02\n",
      "618-23-7085   michaelmartin     Alyssa  Anderson   F     12509 Jordan Rapid Suite 221    RI       67312       kmiller@hotmail.com 1997-08-23\n",
      "485-57-5914 robersonbeverly   Patricia   Andrade   F                 037 Warner Place    VI       46571    brownjames@hotmail.com 1975-12-30\n",
      "557-08-4915       jeffrey73      Roger  Anderson   M    1900 Bradley Skyway Suite 346    WY       83697    mandystone@hotmail.com 2019-12-14\n",
      "\n",
      "------------------------------------------------------------------------------------------------\n",
      "    All entries where the last name begins with the letter A, sorted by age (youngest first)    \n",
      "------------------------------------------------------------------------------------------------\n",
      "        ssn        username first_name last_name sex                   street_address state postal_code                      mail  birthdate\n",
      "557-08-4915       jeffrey73      Roger  Anderson   M    1900 Bradley Skyway Suite 346    WY       83697    mandystone@hotmail.com 2019-12-14\n",
      "863-94-6652          bsmith      David  Anderson   M 9299 Kimberly Mountains Apt. 160    GA       66995         cjensen@gmail.com 2007-05-06\n",
      "839-68-0180     justingrant    Spencer    Arnold   M         2292 Dyer Park Suite 698    HI       30374  millerjennifer@gmail.com 2002-06-11\n",
      "618-23-7085   michaelmartin     Alyssa  Anderson   F     12509 Jordan Rapid Suite 221    RI       67312       kmiller@hotmail.com 1997-08-23\n",
      "485-57-5914 robersonbeverly   Patricia   Andrade   F                 037 Warner Place    VI       46571    brownjames@hotmail.com 1975-12-30\n",
      "202-81-9617    lawsoneugene     Darren Alexander   M     67790 Brandon Wells Apt. 209    FM       92577    sblankenship@gmail.com 1964-06-17\n",
      "417-66-5820        vbaldwin    Michael     Allen   M        8032 Yvonne Cove Apt. 472    MP       87536       danderson@gmail.com 1960-05-24\n",
      "606-20-1906        afreeman    Rebecca     Allen   F                 827 Jason Island    PA       29406   cherylbuckley@gmail.com 1958-04-02\n",
      "701-62-8909       michael06   Kathleen    Arroyo   F      7433 Joshua Light Suite 423    GU       35854      hollyweber@yahoo.com 1949-02-24\n",
      "392-79-6259       phyllis60    Chelsea     Adams   F      560 Cynthia Summit Apt. 271    MN       51695     kelseydavis@gmail.com 1946-12-12\n",
      "891-40-6262       htrujillo     Jerome    Adkins   M        5763 Ashley View Apt. 054    NJ       33403        teresa97@gmail.com 1938-07-02\n",
      "475-87-5012 justinrodriguez    Jessica  Anderson   F     323 Vasquez Village Apt. 866    ND       62196       msnyder@hotmail.com 1938-02-06\n",
      "430-97-1375       fbartlett     Andrea   Andrews   F     620 Sanders Summit Suite 382    NY       95796       natalie15@yahoo.com 1929-10-24\n",
      "607-43-8328 destinygonzalez    Melinda     Allen   F                   736 Reese Loaf    FL       60678         bross@hotmail.com 1921-02-21\n",
      "579-46-9356       tanyakemp       Beth  Arellano   F    3585 Tanner Mission Suite 928    NM       91034        ldoyle@hotmail.com 1916-08-11\n",
      "082-24-7955     ecunningham     Gloria   Aguilar   F        435 Escobar Wall Apt. 304    ME       71133 jennifersanchez@gmail.com 1912-02-22\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from getpass import getuser\n",
    "\n",
    "user = getuser()\n",
    "csv_location = f'/home/varpha/dan/private/{user}' + \\\n",
    "                f'/exrc_02/data/{user}_prob01_profiles.csv'\n",
    "# csv_location =  \"FILE_NAME\" ### GIVE FILE_NAME TO RUN THE CODE LOCALLY\n",
    "\n",
    "# Load input CSV data related to user profiles into pandas DataFrame\n",
    "df = pd.read_csv(csv_location)\n",
    "\n",
    "# # Get basic information about data\n",
    "# print(df.info()) # prints concise summary about DataFrame's structure\n",
    "# print(df.head()) # prints first five rows - default\n",
    "\n",
    "################################### Task a) Data Wrangling  ########################################\n",
    "\n",
    "# # Print sample data for 'name' and 'address' columns\n",
    "# print(df[[\"name\", \"address\"]].head())\n",
    "\n",
    "# Split 'name' column into 'first_name' and 'last_name' columns\n",
    "# Used string's ´split()´ method\n",
    "split_name_col = df[\"name\"].str.split(pat=\" \", n=1, expand=True)\n",
    "df[\"first_name\"] = split_name_col[0]\n",
    "df[\"last_name\"] = split_name_col[1]\n",
    "\n",
    "# # Print sample data for debugging post splitting the 'name' column\n",
    "# print(df[[\"name\", \"first_name\", \"last_name\"]].head())\n",
    "\n",
    "# Split 'address' column into 'street_address' and rest of the address data.\n",
    "split_addr_cols = df[\"address\"].str.split(pat=r\"\\s*\\n\\s*\", n=1, expand=True)\n",
    "df[\"street_address\"] = split_addr_cols[0]\n",
    "\n",
    "# Split rest adress data into 'state' and 'postal code' columns\n",
    "state_and_postal_code = split_addr_cols[1].str.split(\",\", n=1, expand=True)[1]\n",
    "df[\"state\"] = state_and_postal_code.str.split().str[0]\n",
    "df[\"postal_code\"] = state_and_postal_code.str.split().str[1]\n",
    "\n",
    "# # Print sample data for debugging post splitting the 'address' column\n",
    "# print(df[['address','street_address', 'state', 'postal_code']].head())\n",
    "\n",
    "# Create cleaned dataframe with required columns\n",
    "cleaned_df = df[[\n",
    "    \"ssn\", \"username\", \"first_name\", \"last_name\",\n",
    "    \"sex\", \"street_address\", \"state\", \"postal_code\",\n",
    "    \"mail\", \"birthdate\"\n",
    "]]\n",
    "\n",
    "# # Print sample cleaned data for debugging post extracting the required columns\n",
    "# print(cleaned_df.head())\n",
    "\n",
    "############################################## Task b) #############################################\n",
    "\n",
    "# Extract the entries where 'last name' begins with the letter A\n",
    "# Used strings' ´startswith()´ method\n",
    "filtered_df = cleaned_df[cleaned_df[\"last_name\"].str.startswith('A')].copy()\n",
    "\n",
    "# # Print sample data for debugging\n",
    "# print(filtered_df.head())\n",
    "\n",
    "# Sort by 'sex' column (ladies first) where name begins with the letter A and print result\n",
    "# Used Pandas DataFrame method ´sort_values()´ for sorting the data\n",
    "sorted_by_sex = filtered_df.sort_values(by='sex')\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "print(\"   All entries where the last name begins with the letter A, sorted by sex (ladies first)   \")\n",
    "print(\"--------------------------------------------------------------------------------------------\")\n",
    "# set Index to False not display 'index' column\n",
    "print(sorted_by_sex.to_string(index=False))\n",
    "\n",
    "print()  # print a blank line for better display\n",
    "\n",
    "\n",
    "# Sort by 'state' column (alphabetically) where name begins with the letter A and print result\n",
    "sorted_by_state = filtered_df.sort_values(by='state')\n",
    "\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "print(\"   All entries where the last name begins with the letter A, sorted by state (alphabetically)   \")\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "print(sorted_by_state.to_string(index=False))\n",
    "print()  # insert a blank line for better display\n",
    "\n",
    "# Sort by 'birthdate' ie age (youngest first) where name begins with the letter A and print result\n",
    "sorted_by_age = filtered_df.sort_values(by='birthdate', ascending=False)\n",
    "\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "print(\"    All entries where the last name begins with the letter A, sorted by age (youngest first)    \")\n",
    "print(\"------------------------------------------------------------------------------------------------\")\n",
    "print(sorted_by_age.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c82629f-43fd-4272-a4a2-97addd6614b6",
   "metadata": {},
   "source": [
    "## Problem 2. Weather (part 1/2)\n",
    "\n",
    "The file `private/exrc_02/data/XXXXX_prob02_weather.csv` contains hourly weather observations from Helsinki during one month, downloaded from [fmi.fi](https://en.ilmatieteenlaitos.fi/open-data-manual-fmi-wfs-services) (not recommended).\n",
    "\n",
    "First, please do some data cleaning and reorganizing as you find suitable. Then, please answer the following questions:\n",
    "\n",
    "a) How many percentages of the `(tmax+tmin)/2` observations are at most one standard deviation away from the total average of `(tmax+tmin)/2`?\n",
    "\n",
    "b) Find the top-5 timestamps for the difference between `tmax` and `tmin`, i.e. for `tmax-tmin`. For the found rows, print out the following information: timestamp, max temperature, min temperature and temperature difference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e192514-8564-4e20-9d8d-af4c524e729b",
   "metadata": {},
   "source": [
    "## Solution 2. Weather (part 1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54f347e-6349-43c2-869b-eeb15df5a9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.0% of the (tmax+tmin)/2 observations are at most one standard deviation away from the total average of (tmax+tmin)/2\n",
      "\n",
      "------------------------------------------------------------------------------\n",
      "   Information of Top-5 timestamps for the difference between tmax and tmin   \n",
      "------------------------------------------------------------------------------\n",
      "                Time  tmax  tmin  tmax-tmin\n",
      "2022-02-23T00:00:00Z  -2.8 -12.6        9.8\n",
      "2022-02-27T00:00:00Z   2.4  -7.0        9.4\n",
      "2022-02-04T00:00:00Z  -2.1 -11.2        9.1\n",
      "2022-02-12T00:00:00Z   2.3  -4.7        7.0\n",
      "2022-02-24T00:00:00Z   3.1  -3.1        6.2\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from getpass import getuser\n",
    "\n",
    "user = getuser()\n",
    "csv_location = f'/home/varpha/dan/private/{user}' + \\\n",
    "                f'/exrc_02/data/{user}_prob02_weather.csv'\n",
    "# csv_location =  \"FILE_NAME\" ### GIVE FILE_NAME TO RUN THE CODE LOCALLY\n",
    "\n",
    "# Load input CSV data related to hourly weather observations from Helsinki during one month\n",
    "df = pd.read_csv(csv_location)\n",
    "\n",
    "# # Get basic information about data\n",
    "# print(df.info()) # prints concise summary about DataFrame's structure\n",
    "# print(df.head()) # prints first five rows - default\n",
    "\n",
    "##########################################  Data Wrangling  ########################################\n",
    "# Select only required columns to remove the 'Unnamed' column\n",
    "df_cleaned = df[[\"Time\", \"ParameterName\", \"ParameterValue\"]]\n",
    "\n",
    "# # Print sample data for debugging\n",
    "# print(df_cleaned.head())\n",
    "\n",
    "# Transponse the data from long format to wide format and make 'Time' column to index\n",
    "# Used Pandas DataFrame method ´pivot()´\n",
    "df_transposed = df_cleaned.pivot(\n",
    "    index='Time', columns='ParameterName', values='ParameterValue')\n",
    "\n",
    "# # Print sample data for debugging\n",
    "# print(df_transposed.head())\n",
    "\n",
    "# Reset the index to make 'Time' column back to a regular column instead of the index\n",
    "df_transposed = df_transposed.reset_index()\n",
    "\n",
    "# # Print sample data for debugging\n",
    "# print(df_transposed.head().to_string(index=False)) # To see the transposed data as below\n",
    "'''\n",
    "                Time  TG_PT12H_min  rrday  snow  tday  tmax  tmin\n",
    "2022-02-01T00:00:00Z           NaN   -1.0  25.0  -7.7  -5.3  -8.9\n",
    "2022-02-01T01:00:00Z           NaN    NaN   NaN   NaN   NaN   NaN\n",
    "2022-02-01T02:00:00Z           NaN    NaN   NaN   NaN   NaN   NaN\n",
    "2022-02-01T03:00:00Z           NaN    NaN   NaN   NaN   NaN   NaN\n",
    "2022-02-01T04:00:00Z           NaN    NaN   NaN   NaN   NaN   NaN\n",
    "'''\n",
    "\n",
    "# Drop the columns 'TG_PT12H_min', 'rrday' and 'snow' (Not Needed)\n",
    "# Used Pandas DataFrame method ´drop()´\n",
    "df_transposed.drop(columns=[\"TG_PT12H_min\", \"rrday\",\n",
    "                   \"snow\", \"tday\"], inplace=True)\n",
    "\n",
    "# # Print sample data for debugging\n",
    "# print(df_transposed.head())\n",
    "\n",
    "# Drop rows which contains at least one missing value ie NaN\n",
    "# Used Pandas DataFrame method `dropna()`\n",
    "# Used Pandas DataFrame method `copy()` to create a true independent copy as a best practice\n",
    "df_transposed_cleaned = df_transposed.dropna().copy()\n",
    "\n",
    "# # Print sample data for debugging\n",
    "# print(df_transposed_cleaned.head())\n",
    "# print(df_transposed_cleaned.info())\n",
    "\n",
    "#############################################  Task a)  ###########################################\n",
    "\n",
    "# Copy the data for \"task a\" from cleaned data (df_transposed_cleaned)\n",
    "df_task_a = df_transposed_cleaned.copy()\n",
    "\n",
    "# Add a new column 'tmax+tmin/2' to store (tmax+tmin)/2 calculation\n",
    "# Used ´df.loc[:, \"col\"]' instead of ´df[\"col\"]´ to follow best practices\n",
    "df_task_a.loc[:, \"tmax+tmin/2\"] = (df_task_a[\"tmax\"] + df_task_a[\"tmin\"]) / 2\n",
    "\n",
    "# # Print sample data for debugging\n",
    "# print(df_task_a.head())\n",
    "\n",
    "# Calculate the average (mean) and standard deviation for 'tmax+tmin/2' column\n",
    "# Used Pandas DataFrame methods ´mean()´ to get average and ´std()´ to get standard deviation\n",
    "tavg_mean = df_task_a[\"tmax+tmin/2\"].mean()\n",
    "tavg_std = df_task_a[\"tmax+tmin/2\"].std()\n",
    "\n",
    "# Determine the lower and upper range for one standard deviation away from the mean for 'tmax+tmin/2' column\n",
    "lower_range = tavg_mean - tavg_std\n",
    "upper_range = tavg_mean + tavg_std\n",
    "\n",
    "# Calculate the number of tmax+tmin/2 observations within the range of one standard deviation\n",
    "# Used boolean masking and `shape` method with slicing to get the required count\n",
    "observations_within_range = df_task_a[(\n",
    "    df_task_a[\"tmax+tmin/2\"] >= lower_range) & (df_task_a[\"tmax+tmin/2\"] <= upper_range)].shape[0]\n",
    "\n",
    "# Get the total number of valid tmax+tmin/2 observations\n",
    "total_observations = df_task_a.shape[0]\n",
    "\n",
    "# Calculate the percentage\n",
    "percentage = (observations_within_range / total_observations) * 100\n",
    "\n",
    "print(f\"{percentage}% of the (tmax+tmin)/2 observations are at most one standard deviation away from the total average of (tmax+tmin)/2\")\n",
    "print()  # print a blank line before next print for better display\n",
    "\n",
    "\n",
    "############################################  Task b)  ############################################\n",
    "\n",
    "# Copy the data for \"task b\" from cleaned data (df_transposed_cleaned)\n",
    "df_task_b = df_transposed_cleaned.copy()\n",
    "\n",
    "# Add a new column 'tmax-tmin' to store (tmax - tmin) calculation\n",
    "# Used ´df.loc[:, \"col\"]' instead of ´df[\"col\"]´ to follow best practices\n",
    "df_task_b.loc[:, \"tmax-tmin\"] = df_task_b[\"tmax\"] - df_task_b[\"tmin\"]\n",
    "\n",
    "# # Print sample data for debugging\n",
    "# print(df_task_b.head())\n",
    "\n",
    "# Find and print top-5 timestamps for the difference between tmax and tmin\n",
    "# Used Pandas DataFrame method ´sort_values()´ for sorting the data\n",
    "# Used ´head(5)' to dispay 5 rows\n",
    "# Used ´to_string(index=False)´ not to display index for better output\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "print(\"   Information of Top-5 timestamps for the difference between tmax and tmin   \")\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "print(df_task_b.sort_values(by=[\"tmax-tmin\"],\n",
    "      ascending=False).head(5).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc3925-ea46-4069-a059-1adc344dd929",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem 3. Premier League Table\n",
    "The file `private/exrc_02/data/XXXXX_prob03_epl.csv` has some English Premier League results, downloaded using [this api](https://github.com/miquel-vv/football_data_api).\n",
    "\n",
    "Using the full data, generate a league table which has the team name as the index and columns as follows (a win gives 3 points, a draw gives 1 point, and a loss gives 0 points):\n",
    "* games played\n",
    "* wins\n",
    "* draws\n",
    "* defeats\n",
    "* goals for - goals against\n",
    "* points\n",
    "\n",
    "\n",
    "Sort it with points (most points win). If points are equal, then sorted by\n",
    "* goal difference (goals for - goals against)\n",
    "* goals for\n",
    "\n",
    "\n",
    "The expected result should look something like this (not the same data though):\n",
    "```\n",
    "                games  wins  draws  defeats   goals  points\n",
    "Man City           38    32      4        2  106-27     100\n",
    "Man United         38    25      6        7   68-28      81\n",
    "Tottenham          38    23      8        7   74-36      77\n",
    "Liverpool          38    21     12        5   84-38      75\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52620fe9-c203-4eb1-a178-837c09230135",
   "metadata": {},
   "source": [
    "## Solution 3. Premium League Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26071897-1292-4bd9-9493-ff41cee7b194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            games  wins  draws  defeats  goals  points\n",
      "team                                                                  \n",
      "Manchester City FC             38    29      6        3  99-26      93\n",
      "Liverpool FC                   38    28      8        2  94-26      92\n",
      "Chelsea FC                     38    21     11        6  76-33      74\n",
      "Tottenham Hotspur FC           38    22      5       11  69-40      71\n",
      "Arsenal FC                     38    22      3       13  61-48      69\n",
      "Manchester United FC           38    16     10       12  57-57      58\n",
      "West Ham United FC             38    16      8       14  60-51      56\n",
      "Leicester City FC              38    14     10       14  62-59      52\n",
      "Brighton & Hove Albion FC      38    12     15       11  42-44      51\n",
      "Wolverhampton Wanderers FC     38    15      6       17  38-43      51\n",
      "Newcastle United FC            38    13     10       15  44-62      49\n",
      "Crystal Palace FC              38    11     15       12  50-46      48\n",
      "Brentford FC                   38    13      7       18  48-56      46\n",
      "Aston Villa FC                 38    13      6       19  52-54      45\n",
      "Southampton FC                 38     9     13       16  43-67      40\n",
      "Everton FC                     38    11      6       21  43-66      39\n",
      "Leeds United FC                38     9     11       18  42-79      38\n",
      "Burnley FC                     38     7     14       17  34-53      35\n",
      "Watford FC                     38     6      5       27  34-77      23\n",
      "Norwich City FC                38     5      7       26  23-84      22\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from getpass import getuser\n",
    "from ast import literal_eval\n",
    "\n",
    "user = getuser()\n",
    "csv_location = f'/home/varpha/dan/private/{user}' + \\\n",
    "                f'/exrc_02/data/{user}_prob03_epl.csv'\n",
    "# csv_location =  \"FILE_NAME\" ### GIVE FILE_NAME TO RUN THE CODE LOCALLY\n",
    "\n",
    "# Load input CSV data related to some English Premier League results into pandas DataFrame\n",
    "df = pd.read_csv(csv_location)\n",
    "\n",
    "# # Get basic information about data\n",
    "# print(df.info()) # prints concise summary about DataFrame's structure\n",
    "# print(df.head()) # prints first five rows - default\n",
    "\n",
    "# Convert the 'fullTime' column data from string into a python dictionary\n",
    "# Used Pandas DataFrame method ´apply()´ to convert string into list using ´literal_eval´ method call\n",
    "df[\"fullTime\"] = df[\"fullTime\"].apply(literal_eval)\n",
    "\n",
    "# Extract goals fullTime 'fullTime' column to create cloumns 'homeGoals' and 'awayGoals'\n",
    "df[\"homeGoals\"] = df[\"fullTime\"].apply(lambda x: x[\"homeTeam\"])\n",
    "df[\"awayGoals\"] = df[\"fullTime\"].apply(lambda x: x[\"awayTeam\"])\n",
    "\n",
    "# Create two rows per match: one for home team and another for away team\n",
    "# Used Pandas DataFrame method ´rename()´ to rename the columns and make them unique for both rows\n",
    "# Used Pandas DataFrame method ´concat()´ to create the final data post concatinating home_team and away_team DataFrames\n",
    "home_team = df[[\"homeTeam\", \"homeGoals\", \"awayGoals\"]].rename(\n",
    "    columns={\"homeTeam\": \"team\", \"homeGoals\": \"gf\", \"awayGoals\": \"ga\"}\n",
    ")\n",
    "away_team = df[[\"awayTeam\", \"homeGoals\", \"awayGoals\"]].rename(\n",
    "    columns={\"awayTeam\": \"team\", \"awayGoals\": \"gf\", \"homeGoals\": \"ga\"}\n",
    ")\n",
    "match_data = pd.concat([home_team, away_team], ignore_index=True)\n",
    "\n",
    "# Print sample data for debugging\n",
    "# print(matches.head())\n",
    "\n",
    "# Define function ´result´ to find the output as win, loss or draw using gf (goals for) and ga (goals against) as an input\n",
    "\n",
    "\n",
    "def result(row):\n",
    "    if row.gf > row.ga:\n",
    "        return \"win\"\n",
    "    elif row.gf < row.ga:\n",
    "        return \"loss\"\n",
    "    else:\n",
    "        return \"draw\"\n",
    "\n",
    "\n",
    "# Create a new column 'result'\n",
    "# Used Pandas DataFrame method ´apply()´ to call the custom function ´result´ to determine value as 'win', 'loss' or 'draw'\n",
    "match_data[\"result\"] = match_data.apply(result, axis=1)\n",
    "\n",
    "# # Print sample data for debugging\n",
    "# print(matches.head())\n",
    "# print(matches.tail())\n",
    "\n",
    "# Group the match information at each team level\n",
    "# Used Pandas DataFrame method ´groupby()´\n",
    "# To get total number of games played (games), sum of goals for (gf) and sum of goals against (ga)\n",
    "match_summary = match_data.groupby(\"team\").agg(\n",
    "    games=(\"team\", \"count\"),\n",
    "    gf=(\"gf\", \"sum\"),\n",
    "    ga=(\"ga\", \"sum\")\n",
    ").copy()\n",
    "\n",
    "# # Print sample data for debugging\n",
    "# print(match_summary.head())\n",
    "\n",
    "# Count wins, draws, defeats per team\n",
    "match_summary[\"wins\"] = match_data.groupby(\n",
    "    \"team\")[\"result\"].apply(lambda x: (x == \"win\").sum())\n",
    "match_summary[\"draws\"] = match_data.groupby(\n",
    "    \"team\")[\"result\"].apply(lambda x: (x == \"draw\").sum())\n",
    "match_summary[\"defeats\"] = match_data.groupby(\n",
    "    \"team\")[\"result\"].apply(lambda x: (x == \"loss\").sum())\n",
    "\n",
    "# # Print sample data for debugging\n",
    "# print(match_summary.head())\n",
    "\n",
    "# Add new column 'points' to store the value for - a win gives 3 points, a draw gives 1 point, and a loss gives 0 points\n",
    "match_summary[\"points\"] = match_summary[\"wins\"] * 3 + \\\n",
    "    match_summary[\"draws\"] + match_summary[\"defeats\"] * 0\n",
    "\n",
    "# Add new column ´goals' to store value as goals for - goals against ie gf-ga\n",
    "match_summary[\"goals\"] = match_summary[\"gf\"].astype(\n",
    "    str) + \"-\" + match_summary[\"ga\"].astype(str)\n",
    "\n",
    "# Add new temporary column ´goal-diff' to store value as goals for - goals against - needed for sorting requirement\n",
    "match_summary[\"goal-diff\"] = match_summary[\"gf\"] - match_summary[\"ga\"]\n",
    "\n",
    "# Sort the data first with most points, then goals difference and then with goals against (ga)\n",
    "# Used Pandas DataFrame method sort_values() for sorting\n",
    "match_summary = match_summary.sort_values(\n",
    "    by=[\"points\", \"goal-diff\", \"ga\"],\n",
    "    ascending=[False, False, False]\n",
    ")\n",
    "\n",
    "# # Print sample data for debugging\n",
    "# print(match_summary.head())\n",
    "# print(match_summary.tail())\n",
    "\n",
    "# Select only required coloumns\n",
    "match_summary = match_summary[[\"games\", \"wins\",\n",
    "                               \"draws\", \"defeats\", \"goals\", \"points\"]]\n",
    "\n",
    "# Print the final summary as asked in the task\n",
    "print(match_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0670cdbf-167d-45b5-b2b3-24e1bc8ae4e6",
   "metadata": {},
   "source": [
    "## Problem 4. Weather (part 2/2)\n",
    "\n",
    "The file `private/exrc_02/data/XXXXX_prob04_weather.txt` \n",
    "has some (old) weather data from Jyväskylä 1959-2021, again downloaded from [fmi.fi](https://en.ilmatieteenlaitos.fi/open-data-manual-fmi-wfs-services) (not recommended).\n",
    "\n",
    "Calculate the \"snow sum\" (not an official meteorological term) for each winter by adding the snow depths for each day of that winter. Start from winter 1959-60 and end to 2019-20 since 1958-59 and 2020-21 are only partial.\n",
    "\n",
    "Notes:\n",
    "* You need to define \"winter\" by yourself.\n",
    "* FMI uses -1 as snow depth when \"there is absolutely no snow at all\". We don't want to reduce snow sum in that case, so replace -1 with 0.\n",
    "* For missing data, assume that the snow depth has been the same as during the previous day. (Fill any `NaN`s with the previous valid value.)\n",
    "\n",
    "Then produce a DataFrame that has the winter as the index (in form \"1959-1960\") and columns:\n",
    "* snow sum\n",
    "* snow sum rank among winters so that largest = 1\n",
    "* count of days where snow depth has been positive\n",
    "* max snow depth of the winter.\n",
    "\n",
    "\n",
    "The three first and the three last rows should look something like:\n",
    "```\n",
    "           Snow sum  rank  count  max\n",
    "Winter                               \n",
    "1959-1960      5593    18    169   65\n",
    "1960-1961      5082    28    162   60\n",
    "1961-1962      6644    12    156   78\n",
    "\n",
    "...\n",
    "\n",
    "2017-2018      6882     8    161   81\n",
    "2018-2019      4030    42    150   54\n",
    "2019-2020      1432    59    112   30\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc118410-04da-47c5-bdbc-e177e20ecbcb",
   "metadata": {},
   "source": [
    "## Solution 4. Weather (part 2/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b409321d-91e7-4062-b0c7-5f0b6658295a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Snow_sum  rank  count  max\n",
      "Winter                               \n",
      "1959-1960      5593    18    169   65\n",
      "1960-1961      5082    28    162   60\n",
      "1961-1962      6644    12    156   78\n",
      "...\n",
      "           Snow_sum  rank  count  max\n",
      "Winter                               \n",
      "2017-2018      6882     8    161   81\n",
      "2018-2019      4030    42    150   54\n",
      "2019-2020      1432    59    112   30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from getpass import getuser\n",
    "\n",
    "user = getuser()\n",
    "csv_location = f'/home/varpha/dan/private/{user}' + \\\n",
    "                f'/exrc_02/data/{user}_prob04_weather.csv'\n",
    "# csv_location =  \"FILE_NAME\" ### GIVE FILE_NAME TO RUN THE CODE LOCALLY\n",
    "\n",
    "# Load input CSV data related to old weather data from Jyväskylä 1959-2021\n",
    "df = pd.read_csv(csv_location)\n",
    "\n",
    "# # Get basic information about data\n",
    "# print(df.info()) # prints concise summary about DataFrame's structure\n",
    "# print(df.head()) # prints first five rows - default\n",
    "\n",
    "# Create new 'Date' column by combinining 'Year, 'Month' and 'Day' columns\n",
    "# Used Pandas DataFrame method ´to_datetime()´ to create valid calendar date as CCYY-MM-DD\n",
    "df[\"Date\"] = pd.to_datetime(df[[\"Year\", \"Month\", \"Day\"]])\n",
    "\n",
    "# Extract only 'Date' and 'Snow depth (cm)' columns\n",
    "df = df[[\"Date\", \"Snow depth (cm)\"]].copy()\n",
    "\n",
    "# # Print sample data and information about dataframe for debugging\n",
    "# print(df.head())\n",
    "# print(df.info())  # There are Null entries in 'Snow depth (cm)' column\n",
    "\n",
    "\n",
    "# Replace -1 with 0 on 'Snow depth (cm)' column\n",
    "# Used Pandas DataFrame method ´replace()´ to replace value of -1 to 0\n",
    "df[\"Snow depth (cm)\"] = df[\"Snow depth (cm)\"].replace(-1, 0)\n",
    "\n",
    "# Fill missing values in 'Snow depth (cm)' column with previous day's snow depth\n",
    "# Used Pandas DataFrame method ´ffill()´\n",
    "# Used .ffill() instead of .fillna(method=\"ffill\") since later one is depricated\n",
    "df[\"Snow depth (cm)\"] = df[\"Snow depth (cm)\"].ffill()\n",
    "\n",
    "# # Confirm no Null / NaN entries\n",
    "# print(df.info()) # No Null entries anymore\n",
    "\n",
    "\n",
    "# Define function to assign winter season as follow\n",
    "# Sept–Dec -> winter belongs to (year)–(year+1)\n",
    "# Jan–Aug -> winter belongs to (year-1)–year\n",
    "def assign_winter(date):\n",
    "    if date.month >= 9:\n",
    "        return f\"{date.year}-{date.year+1}\"\n",
    "    else:\n",
    "        return f\"{date.year-1}-{date.year}\"\n",
    "\n",
    "\n",
    "# Create new column 'Winter' in the form of (year)-(year+1)\n",
    "# Used Pandas DataFrame method ´apply()´ to call custom function ´assign_winter()´ get the required value\n",
    "df[\"Winter\"] = df[\"Date\"].apply(assign_winter)\n",
    "\n",
    "# # Print sample data for debugging\n",
    "# print(df.head())\n",
    "\n",
    "# Keep only data where winters 1959 through 2020 as per requirement\n",
    "# Used Pandas DataFrame method ´isin()´ to filter the data\n",
    "valid_winters = [f\"{y}-{y+1}\" for y in range(1959, 2020)]\n",
    "# print(\"valid winters\", valid_winters)\n",
    "df = df[df[\"Winter\"].isin(valid_winters)]\n",
    "\n",
    "# # Print sample data for debugging\n",
    "# print(df.head())  # print first 5 rows\n",
    "# print(df.tail())  # print last 5 rows\n",
    "\n",
    "# Group the match information at each winter range level\n",
    "# Used Pandas DataFrame method ´groupby()´ to groupby at ´Winter´ column\n",
    "snow_stats = df.groupby(\"Winter\").agg(\n",
    "    # Create a new column ´Snow_sum´ with sum of snow depth\n",
    "    Snow_sum=(\"Snow depth (cm)\", \"sum\"),\n",
    "    # Create column ´count´ count of days where snow depth has been positive\n",
    "    count=(\"Snow depth (cm)\", lambda x: (x > 0).sum()),\n",
    "    # Create column ´max´ with max snow depth of the winter\n",
    "    max=(\"Snow depth (cm)\", \"max\")\n",
    ")\n",
    "\n",
    "# # Print sampe data for debugging\n",
    "# print(snow_stats.head())\n",
    "\n",
    "# Create new column ´rank´ to store rank for all the Winter\n",
    "# Used Pandas DataFrame method ´rank()´ on ´Snow_sum´ coloumn\n",
    "# ´ascending=False´ used so largest Snow sum get 1st rank\n",
    "# ´method=\"min\"´ used if there’s a tie, give all tied values the smallest rank number they could take\n",
    "# ´astype(int)´ used to covert the float into int since ´rank()´ output is float by default\n",
    "snow_stats[\"rank\"] = snow_stats[\"Snow_sum\"].rank(\n",
    "    ascending=False, method=\"min\")\n",
    "\n",
    "# Reorder columns as the per the requirement\n",
    "snow_stats = snow_stats[[\"Snow_sum\", \"rank\", \"count\", \"max\"]]\n",
    "\n",
    "# # Print sample data for debugging\n",
    "# print(snow_stats.head()) # Found float value for ´Snow_sum´, 'rank' and 'max' columns\n",
    "\n",
    "# Convert the values to int for ´Snow_sum´, 'rank' and 'max' columns\n",
    "# Used Pandas DataFrame method ´astype()´ method to convert floats to integers\n",
    "snow_stats[[\"Snow_sum\", \"rank\", \"max\"]] = snow_stats[[\n",
    "    \"Snow_sum\", \"rank\", \"max\"]].astype(int)\n",
    "\n",
    "# # Print sampe data for debugging\n",
    "# print(snow_stats.head())\n",
    "\n",
    "# print(snow_stats.to_string())  # to print the full data as ouput\n",
    "\n",
    "# Print first and last 3 winters as shown in the Problem 5\n",
    "print(snow_stats.head(3))\n",
    "print(\"...\")\n",
    "print(snow_stats.tail(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf511d-2bff-4d3b-8cd3-2231732df527",
   "metadata": {},
   "source": [
    "## Problem 5. Statfi data wrangle.\n",
    "- Here we're trying to make some sense out of the data that we downloaded from the statfi service in problem 5 of the first exercises. If you did it successfully, please use your own data. Otherwise you may use the data found in `public/exrc_02/data`.\n",
    "- That data used the keyword `kihi` and the final table `statfin_kihi_pxt_13zt.px`. You may want to replace them in the front end url below.\n",
    "- First use [this front end](https://pxdata.stat.fi/PxWeb/pxweb/en/StatFin/StatFin__kihi/statfin_kihi_pxt_13zt.px/) to produce a meaningful table, and then try to produce a similar table with pandas and your data.\n",
    "- The most elegant solution wins!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7756c1b-98d4-4582-bbe3-ca915682e8d0",
   "metadata": {},
   "source": [
    "## Solution 5. Staffi data wrangle\n",
    "\n",
    "### In this I used the file downloaded from statfi service related to Railway statistics (rtie) on Problem 5 of Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ec25bd-fa5f-42d5-a040-30640ac5880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Data obtained from statfi service in Round 1 Problem-5 => Railway statistics (rtie)\n",
    "# https://pxdata.stat.fi/PxWeb/pxweb/en/StatFin/StatFin__rtie/statfin_rtie_pxt_12lz.px/\n",
    "\n",
    "# json_data_location = \"prob05.json\" ## I used the file which I dowloaded in Problem 5 of Exercise 1\n",
    "\n",
    "\n",
    "# Read the data into a python dict\n",
    "with open(json_data_location) as handle:\n",
    "    rtie_data = json.load(handle)\n",
    "\n",
    "# # Get basic information about data\n",
    "# print(type(rtie_data))\n",
    "# print(list(rtie_data.keys()))\n",
    "# print(list(rtie_data.values()))\n",
    "\n",
    "# Convert the data into a pandas dataframe using normalisation\n",
    "# Used Pandas DataFrame method ´json_normalize()´ to flatten JSON data into tabular DataFrame\n",
    "df = pd.json_normalize(rtie_data, record_path='data')\n",
    "\n",
    "# # Sample data\n",
    "# print(df.head())\n",
    "'''\n",
    "           key          values\n",
    "0  [SSS, 2005]  [48227, 67559]\n",
    "1  [SSS, 2006]  [50880, 72020]\n",
    "2  [SSS, 2007]  [52577, 73336]\n",
    "3  [SSS, 2008]  [53259, 74901]\n",
    "4  [SSS, 2009]  [50019, 69244]\n",
    "'''\n",
    "\n",
    "# Explode the 'key' column into two columns\n",
    "# Used Pandas DataFrame method ´apply()´ as apply(pd.Series) to convert list into separate columns\n",
    "df[[\"type_of_locomotive\", \"year\"]] = df[\"key\"].apply(pd.Series)\n",
    "\n",
    "# Explode the 'values' column into two \n",
    "# Used Pandas DataFrame method ´apply()´ as apply(pd.Series) to convert list into separate columns\n",
    "df[[\"trainkilometres\", \"locomotivekilometres\"]] = df[\"values\"].apply(pd.Series)\n",
    "\n",
    "# Drop the cloumns 'key' and 'values' not needed\n",
    "# Used Pandas DataFrame method ´drop()´ to drop the required columns\n",
    "df = df.drop(columns=[\"key\", \"values\"])\n",
    "\n",
    "# # Get basic information about data post processing it\n",
    "# print(df.info()) # display concise summary about dataframe\n",
    "# print(df.head()) # display first five rows by default\n",
    "\n",
    "# Convert columns 'year', 'trainkilometres' & 'locomotivekilometres' data into numeric form to perform calculations\n",
    "# Used Pandas DataFrame method ´to_numeric()´ to convert the values to numeric (int/float)\n",
    "df[\"year\"] = pd.to_numeric(df[\"year\"], errors='coerce')\n",
    "df[\"trainkilometres\"] = pd.to_numeric(df[\"trainkilometres\"], errors='coerce')\n",
    "df[\"locomotivekilometres\"] = pd.to_numeric(\n",
    "    df[\"locomotivekilometres\"], errors='coerce')\n",
    "\n",
    "# # Check and confirm the information post converting the data\n",
    "# print(df.info()) # display concise summary about dataframe\n",
    "# print(df.head()) # display first five rows - default\n",
    "\n",
    "# # Write to a CSV file for debugging and anlysis purpose\n",
    "# df.to_csv(\"prob05_output_report.csv\", index=False)\n",
    "\n",
    "# Group by using 'type_of_locomotive' column and compute average for 'trainkilometres' and 'locomotivekilometres' columns\n",
    "# Used Pandas DataFrame method ´groupby()´ to summarise with respect to type_of_locomotive\n",
    "df_summary = (\n",
    "    df.groupby(\"type_of_locomotive\")[\n",
    "        [\"trainkilometres\", \"locomotivekilometres\"]].mean()\n",
    ")\n",
    "\n",
    "# Rename the 'trainkilometres' and 'locomotivekilometres' columns to meaningfule names\n",
    "# Used Pandas DataFrame method ´rename()´ to rename the column names\n",
    "df_summary.rename(columns={\n",
    "    \"trainkilometres\": \"average_train_km_year\",\n",
    "    \"locomotivekilometres\": \"average_locomotive_km_year\"\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "# Sort the data with 'average_train_km_year' column on descending order\n",
    "# Used Pandas DataFrame method ´sort_values()´ for sorting the data\n",
    "# Used Pandas DataFrame method `copy()` to create a true independent copy as a best practice\n",
    "df_final_result = df_summary.sort_values(\n",
    "    by=[\"average_train_km_year\"], ascending=False).copy()\n",
    "\n",
    "# Print the final outcome which summarise the data\n",
    "# print(df_final_result)\n",
    "\n",
    "'''\n",
    "                    average_train_km_year  average_locomotive_km_year\n",
    "type_of_locomotive                                                   \n",
    "SSS                              49667.00                    68715.85\n",
    "050                              43002.75                    53437.15\n",
    "060                              26892.60                    31354.00\n",
    "070                              16110.15                    22083.15\n",
    "010                               6664.35                    15278.70\n",
    "020                               5172.65                    13420.70\n",
    "040                               1491.70                     1858.00\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "Usage summary (most to least activity)\n",
    "    - SSS locomotives have the highest usage, averaging about ~50k train km and ~69k locomotive km per year\n",
    "    - 050 locomotives are next, with roughly ~43k train km and ~53k locomotive km per year\n",
    "    - 060 locomotives follow, averaging around ~27k train km and ~31k locomotive km per year\n",
    "    - 070 locomotives show moderate activity, with about ~16k train km and ~22k locomotive km year\n",
    "    - 010 locomotives record lower usage, roughly ~7k train km and ~15k locomotive km per year\n",
    "    - 020 locomotives are similar, at about ~5k train km and ~13k locomotive km year\n",
    "    - 040 locomotives show the least activity, with only ~1.5k train km and ~2k locomotive km per year\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc0e70b-1a6e-40f7-aba7-d12ae5968d8a",
   "metadata": {},
   "source": [
    "## How to submit my solutions?\n",
    "\n",
    "Open a Terminal tab (e.g. <tt>File $\\rightarrow$ New $\\rightarrow$ Terminal</tt>, copy-paste the following into the Terminal command prompt, and press enter:\n",
    "<pre>\n",
    "  /home/varpha/dan/menu.py\n",
    "</pre>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
