{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebabbbab-c235-4789-a004-14e69d06ec9d",
   "metadata": {},
   "source": [
    "# Data Analytics Spring 2024 &mdash; Exercises 2\n",
    "\n",
    "### XXXXX XXXXX\n",
    "\n",
    "Note from Harri: These are old, but that shouldn't matter much. The role of the model solutions is to provide peer review guidelines by showing what kind of a solution deserves five points.\n",
    "\n",
    "The problem 1 profiles are in UK format (this year we have US format)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe4d8d-c0fc-43f7-b336-3fc95dd0dd76",
   "metadata": {},
   "source": [
    "## Problem 1. Profiles\n",
    "\n",
    "The file `private/exrc_02/data/XXXXX_prob01_profiles.csv` contains some user profiles.\n",
    "Read the csv into a pandas DataFrame and reorganize it as follows:\n",
    "\n",
    "a) Separate the name and address columns so that there are separate columns for\n",
    "- first name\n",
    "- last name\n",
    "- street address\n",
    "- state\n",
    "- postal code.\n",
    "\n",
    "Keep also the ssn, username, sex, mail and birthdate columns. Drop all the other columns.\n",
    "\n",
    "b) Print all entries where the last name begins with the letter J, sorted by:\n",
    "- sex (ladies first)\n",
    "- state (alphabetically)\n",
    "- age (youngest first).\n",
    "\n",
    "I.e. print the entries three times, in these three different ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b32337-b581-4294-a273-d88276da009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for this and all subsequent problems\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420c3986-0aae-4e6c-a1f2-01cd5d4cdd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "df = pd.read_csv('data/XXXXX_prob01_profiles.csv')\n",
    "print(f\"Dataframe shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1864d2-001b-487f-92df-7e0b95a35c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Part a) separate name and address columns into first name, last name\n",
    "# street address, state, postal code\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "# ---------------- SANITY CHECK ----------------------------\n",
    "# There are titles in front of the names. But not always. These need to be accounted for.\n",
    "# Sanity checks to understand what to do with the data:\n",
    "# - Split the name into parts (separated by space)\n",
    "split_names = df.name.map(lambda x: x.split(' '))\n",
    "\n",
    "# - What is the largest number of splits? Would expect 3 (title, first name, last name)\n",
    "print(f\"Maximum number of splits: {split_names.map(lambda x: len(x)).max()}\")\n",
    "# Result: 3. \n",
    "\n",
    "# So, let's check what values are in the first splits of 3-part names. \n",
    "# Because we might still have cases with FirstName1 FirstName2 Surname\n",
    "# (or like the Spanish, FirstName MomsSurname DadsSurname)\n",
    "\n",
    "unique_first_parts = {}\n",
    "\n",
    "for x in split_names:\n",
    "    if len(x) > 2:\n",
    "        unique_first_parts[x[0]] = ''\n",
    "\n",
    "print(f\"Unique values as first words in three-part names: {', '.join(unique_first_parts.keys())}\")\n",
    "# Result: Mr, Dr, Miss, Mrs, Ms\n",
    "\n",
    "# So: if the name is composed of three parts, the first part is always the title. \n",
    "# This lets us assume that we can just take the last two parts of the name field, and\n",
    "# they will be the First Name and the Last Name. \n",
    "# ----------------- SANITY CHECK ENDS ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6695c4b6-1d3a-4650-9843-918ae46c0587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create first_name and last_name columns in the data frame\n",
    "df['first_name'] = df.name.map(lambda x: x.split(' ')[-2])\n",
    "df['last_name'] = df.name.map(lambda x: x.split(' ')[-1])\n",
    "\n",
    "# Example data\n",
    "print(df[['first_name', 'last_name']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b49f00a-1d58-4c25-80f9-3ce40034ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ MORE SANITY CHECKS ---------------------------\n",
    "# What about the address?\n",
    "split_addresses = df.address.map(lambda x: x.split('\\n'))\n",
    "\n",
    "# What is the maximum number of address splits?\n",
    "print(f\"Maximum number of address splits: {split_addresses.map(lambda x: len(x)).max()}\\n\")\n",
    "# Result: 4\n",
    "\n",
    "# At this point I'm making an assumption that the first part may be a specifier\n",
    "# (like a flat number), second part is street name, third part city and last one postal code.\n",
    "# -------------------SANITY CHECK ENDS ----------------------------\n",
    "\n",
    "df['street_address'] = df.address.map(lambda x: x.split('\\n')[-3])\n",
    "df['state'] = df.address.map(lambda x: x.split('\\n')[-2])\n",
    "df['postal_code'] = df.address.map(lambda x: x.split('\\n')[-1])\n",
    "\n",
    "# Sample data\n",
    "print(df[['street_address', 'state', 'postal_code']].head())\n",
    "\n",
    "# Instruction: Keep also the ssn, username, sex, mail and birthdate columns. Drop all the other columns.\n",
    "columns_to_keep = ['first_name', 'last_name', 'street_address', 'state', 'postal_code', 'ssn', 'username', 'sex', 'mail', 'birthdate']\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Results\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1941d86-4d14-4d78-947c-292c4d0c960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# b) Print all entries where the last name begins with the letter J, sorted by:\n",
    "#\n",
    "# sex (ladies first)\n",
    "# state (alphabetically)\n",
    "# age (youngest first).\n",
    "#-----------------------------------------------------------------------------\n",
    "\n",
    "# Filtering all whose last name starts with J\n",
    "all_Js = df[df['last_name'].str.startswith('J')]\n",
    "\n",
    "# Ladies first\n",
    "all_Js.sort_values(by='sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de18b8c-e86a-437f-8bec-21b859cfa0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State (alphabetically)\n",
    "all_Js.sort_values(by='state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480f32bc-f717-4ac8-bff3-8a92f7887e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age (youngest first)\n",
    "all_Js.sort_values(by='birthdate', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c82629f-43fd-4272-a4a2-97addd6614b6",
   "metadata": {},
   "source": [
    "## Problem 2. Weather (part 1/2)\n",
    "\n",
    "The file `private/exrc_02/data/XXXXX_prob02_weather.csv` contains hourly weather observations from Helsinki during one month, downloaded from [fmi.fi](https://en.ilmatieteenlaitos.fi/open-data-manual-fmi-wfs-services) (not recommended).\n",
    "\n",
    "First, please do some data cleaning and reorganizing as you find suitable. Then, please answer the following questions:\n",
    "\n",
    "a) How many percentages of the `(tmax+tmin)/2` observations are at most one standard deviation away from the total average of `(tmax+tmin)/2`?\n",
    "\n",
    "b) Find the top-5 timestamps for the difference between `tmax` and `tmin`, i.e. for `tmax-tmin`. For the found rows, print out the following information: timestamp, max temperature, min temperature and temperature difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d10a6b3-da30-467e-88cf-41ca6009d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in file\n",
    "df = pd.read_csv('data/XXXXX_prob02_weather.csv')\n",
    "\n",
    "# Get some info out of the DataFrame\n",
    "print(df.shape)\n",
    "print(df.describe())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b53b68-3a05-441a-9d41-2ff7322025b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# \"First, please do some data cleaning and reorganizing as you find suitable.\"\n",
    "# -----------------------------------------------------------------------------\n",
    "# Well. There are at least three major things to tackle outright. \n",
    "# 1) Setting the index to a proper value (the datetime would be a good candidate)\n",
    "# 2) ParameterName and ParameterValue columns contain key-value pairs for specific timestamps.\n",
    "#    These should probably be split into their own columns. One row of data per timestamp.\n",
    "# 3) First column can be dropped (only contains an index number)\n",
    "\n",
    "# Turns out that all of these can be done in a single line with pivot_table:\n",
    "df = df.pivot_table(index='Time', columns='ParameterName', values='ParameterValue')\n",
    "df = df.rename_axis(index='Time')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a24893a-a6ae-49d7-b075-8afb841fa345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the data, one more tweak needs to be made. TG_PT12H_min and the\n",
    "# rest of the data points are from the same day, but from six hours apart. Because\n",
    "# of this, they reside in different rows. For the purpose of this exercise, having\n",
    "# all the values in a single row would be easier. So, let's merge the timestamps.\n",
    "\n",
    "# Drop the hour information from timestamps\n",
    "df.set_index(pd.to_datetime(df.index).date, inplace=True)\n",
    "df = df.rename_axis(index='Time')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2647ab-efe9-4372-9b49-4d1e8eba9c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line groups the timestamps in the dataframe together, and selects the first \n",
    "# non-null value (x.dropna() removes all nulls, iloc[0] selects the first one from\n",
    "# the values that remain. The if section ensures that a \"None\" is returned as the\n",
    "# value of there were no values to begin with (like for TG_PT12H_min, days 16.11.-->)\n",
    "df = df.groupby(df.index).agg(lambda x: x.dropna().iloc[0] if not x.dropna().empty else None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15eed87-6f00-4587-8842-a7ab85049bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info regarding the data at this stage\n",
    "print(df.shape)\n",
    "print(df.describe())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb633d2e-ed4f-409f-abea-e1b8db4850c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The -1.0 values in rrday and snow probably correlate to a \"did not snow/rain\" value.\n",
    "# I did not change these values, as I was not sure what the significance of -1.0 was.\n",
    "# I also didn't change the column headers to a more human-readable format, as I would\n",
    "# need to understand the data better for this. I can only assume what the headers mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e4387a-0e48-4ae1-84eb-8146b8471cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# a) How many percentages of the (tmax+tmin)/2 observations are at most one \n",
    "#    standard deviation away from the total average of (tmax+tmin)/2?\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Let's add a column with value for (tmax+tmin)/2, which would be tavg (average)\n",
    "df ['tavg'] = (df.tmax + df.tmin) / 2\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befda32c-e8cc-473c-ad30-ca7119150fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculations for the result:\n",
    "# Get standard deviation and mean\n",
    "tavg_std = df.tavg.std()\n",
    "tavg_mean = df.tavg.mean()\n",
    "\n",
    "# Create filter mask to check whether tavg-value is within one standard deviation\n",
    "filter_mask = (df.tavg >= (tavg_mean - tavg_std)) & (df.tavg <= (tavg_mean + tavg_std))\n",
    "\n",
    "# Filter dataframe\n",
    "filtered = df[filter_mask]\n",
    "\n",
    "# Calculate percentage that passed the filter\n",
    "print(f\"{round(len(filtered)/len(df) * 100, 2)}% of (tmax+tmin)/2 values were within one standard deviation of the mean.\\n\")\n",
    "\n",
    "# Sanity check: \n",
    "# Get min and max of those within one std:\n",
    "print(\"Sanity:\")\n",
    "print(f\"Minimum that made it: {filtered.tavg.min()}, maximum that made it: {filtered.tavg.max()}\")\n",
    "print(f\"Distance between max and min: {round(filtered.tavg.max() - filtered.tavg.min(), 2)}\")\n",
    "print(f\"Two standard deviations: {round(tavg_std * 2, 2)}\")\n",
    "\n",
    "# Check if values that got left out were much smaller or larger than those included\n",
    "print(f\"Minimum that got left out: {df[~filter_mask].tavg.min()}, maximum that got left out: {df[~filter_mask].tavg.max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c6cd50-7396-4b31-80f8-05ce231bd3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# b) Find the top-5 timestamps for the difference between tmax and tmin, i.e. \n",
    "#    for tmax-tmin. For the found rows, print out the following information: \n",
    "#    timestamp, max temperature, min temperature and temperature difference.\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Again, it's easiest to just create a new column for this. Calling it 'tdiff'.\n",
    "df['tdiff'] = df.tmax - df.tmin\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb047b-78bf-48d3-972d-b1834ba53929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort values by tdiff\n",
    "df = df.sort_values(by='tdiff', ascending=False)\n",
    "\n",
    "# Print out tmax, tmin and tdiff for top five temperature differences\n",
    "df.iloc[:5][['tmax', 'tmin', 'tdiff']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fc3925-ea46-4069-a059-1adc344dd929",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem 3. Premier League Table\n",
    "The file `private/exrc_02/data/XXXXX_prob03_epl.csv` has some English Premier League results, downloaded using [this api](https://github.com/miquel-vv/football_data_api).\n",
    "\n",
    "Using the full data, generate a league table which has the team name as the index and columns as follows (a win gives 3 points, a draw gives 1 point, and a loss gives 0 points):\n",
    "* games played\n",
    "* wins\n",
    "* draws\n",
    "* defeats\n",
    "* goals for - goals against\n",
    "* points\n",
    "\n",
    "\n",
    "Sort it with points (most points win). If points are equal, then sorted by\n",
    "* goal difference (goals for - goals against)\n",
    "* goals for\n",
    "\n",
    "\n",
    "The expected result should look something like this (not the same data though):\n",
    "```\n",
    "                games  wins  draws  defeats   goals  points\n",
    "Man City           38    32      4        2  106-27     100\n",
    "Man United         38    25      6        7   68-28      81\n",
    "Tottenham          38    23      8        7   74-36      77\n",
    "Liverpool          38    21     12        5   84-38      75\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c1d7a9-d5a6-4c17-8891-1874d7ec896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data.\n",
    "df = pd.read_csv('data/XXXXX_prob03_epl.csv')\n",
    "\n",
    "# Basic info about data\n",
    "print(df.shape)\n",
    "print(df.describe())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c22bde7-53a4-4cfc-b412-b3b8346949a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First things first, let's get the values in fullTime into their own columns.\n",
    "\n",
    "# Grabbing some tools:\n",
    "import ast\n",
    "\n",
    "# Turn the dictionary string into an actual dictionary\n",
    "goals = df.fullTime.apply(ast.literal_eval)\n",
    "\n",
    "# Create a dataframe from the dictionary and add to existing dataframe\n",
    "df[['homeGoals', 'awayGoals']] = pd.DataFrame(goals.tolist())\n",
    "\n",
    "# Drop the original fullTime column\n",
    "df.drop('fullTime', axis='columns', inplace=True)\n",
    "\n",
    "# Set homeGoals and awayGoals to integers instead of floats\n",
    "df['homeGoals'] = df['homeGoals'].fillna(0).astype(int)\n",
    "df['awayGoals'] = df['awayGoals'].fillna(0).astype(int)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def1832-5d64-448f-b995-c2a342ba1c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next step is to split home and away teams to get their statistics. Each game will\n",
    "# produce two rows of data: one for home games, one for away games, looking at the\n",
    "# statistics from the point of the team. \n",
    "\n",
    "#  Home game teams first:\n",
    "home_games = df[['homeTeam', 'homeGoals', 'awayGoals']].copy()\n",
    "home_games.columns = ['team', 'goals_for', 'goals_against']\n",
    "home_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ecf120-0866-4c5b-84bb-9aff85d246f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for away teams\n",
    "away_games = df[['awayTeam', 'awayGoals', 'homeGoals']].copy()\n",
    "away_games.columns = ['team', 'goals_for', 'goals_against']\n",
    "away_games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f1f4ed-4c0b-48b2-9012-46a8ef94f653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge home_games and away_games\n",
    "league_table = pd.concat([home_games, away_games])\n",
    "league_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa81467-6617-442f-b4a8-e18b679488bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of the data the problem asks for: \n",
    "# games played\n",
    "# wins\n",
    "# draws\n",
    "# defeats\n",
    "# goals for - goals against\n",
    "# points\n",
    "\n",
    "# Creating new columns for game-by-game statistics, which can be\n",
    "# summed up later to get the answer to Problem 3.\n",
    "league_table['games played'] = 1\n",
    "league_table['wins'] = league_table.goals_for > league_table.goals_against\n",
    "league_table['draws'] = league_table.goals_for == league_table.goals_against\n",
    "league_table['defeats'] = league_table.goals_for < league_table.goals_against\n",
    "league_table['points'] = 3 * league_table.wins + league_table.draws\n",
    "\n",
    "league_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd0ba8b-0f3e-47a5-af21-d5ade5aac927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group game data by team and sum up the statistics\n",
    "league_table = league_table.groupby('team').agg('sum')\n",
    "\n",
    "# Add a goals-column\n",
    "league_table['goals'] = league_table.goals_for.astype(str) + '-' + league_table.goals_against.astype(str)\n",
    "\n",
    "league_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e0aef9-7638-4ea8-8ed0-cb29f87144bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and format table to provide the answer to Problem 3:\n",
    "# Helper column for sorting by goal difference\n",
    "league_table['goal_diff'] = league_table.goals_for - league_table.goals_against\n",
    "\n",
    "# Sort table\n",
    "league_table = league_table.sort_values(by=['points', 'goal_diff', 'goals_for'], ascending=False)\n",
    "\n",
    "# Keep only those columns that were requested in the order requested\n",
    "league_table = league_table[['games played', 'wins', 'draws', 'defeats', 'goals', 'points']]\n",
    "league_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0670cdbf-167d-45b5-b2b3-24e1bc8ae4e6",
   "metadata": {},
   "source": [
    "## Problem 4. Weather (part 2/2)\n",
    "\n",
    "The file `private/exrc_02/data/XXXXX_prob04_weather.txt` \n",
    "has some (old) weather data from Jyväskylä 1959-2021, again downloaded from [fmi.fi](https://en.ilmatieteenlaitos.fi/open-data-manual-fmi-wfs-services) (not recommended).\n",
    "\n",
    "Calculate the \"snow sum\" (not an official meteorological term) for each winter by adding the snow depths for each day of that winter. Start from winter 1959-60 and end to 2019-20 since 1958-59 and 2020-21 are only partial.\n",
    "\n",
    "Notes:\n",
    "* You need to define \"winter\" by yourself.\n",
    "* FMI uses -1 as snow depth when \"there is absolutely no snow at all\". We don't want to reduce snow sum in that case, so replace -1 with 0.\n",
    "* For missing data, assume that the snow depth has been the same as during the previous day. (Fill any `NaN`s with the previous valid value.)\n",
    "\n",
    "Then produce a DataFrame that has the winter as the index (in form \"1959-1960\") and columns:\n",
    "* snow sum\n",
    "* snow sum rank among winters so that largest = 1\n",
    "* count of days where snow depth has been positive\n",
    "* max snow depth of the winter.\n",
    "\n",
    "\n",
    "The three first and the three last rows should look something like:\n",
    "```\n",
    "           Snow sum  rank  count  max\n",
    "Winter                               \n",
    "1959-1960      5593    18    169   65\n",
    "1960-1961      5082    28    162   60\n",
    "1961-1962      6644    12    156   78\n",
    "\n",
    "...\n",
    "\n",
    "2017-2018      6882     8    161   81\n",
    "2018-2019      4030    42    150   54\n",
    "2019-2020      1432    59    112   30\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf9b985-febe-4cf3-ab6b-9619814fef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "df = pd.read_csv('data/XXXXX_prob04_weather.csv')\n",
    "\n",
    "# Get info about the data structure\n",
    "print(df.shape)\n",
    "print(df.describe())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07885e73-aa6e-4c61-889b-4d0a33e4db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First things first, handling the notes:\n",
    "\n",
    "# \"You need to define \"winter\" by yourself.\"\n",
    "# Let's define winter. An adequate separator between winters would be\n",
    "# a summer month that has never had snow over the years. Let's find one.\n",
    "\n",
    "# Get maximum values for data when grouped by month (regardless of year or day)\n",
    "df.groupby('Month').agg('max')\n",
    "\n",
    "# June, July and August (6-8) have always been snowless. So, we can assume\n",
    "# that a winter lasts from September to May. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67125b0-bf76-42f3-8b2d-c738f258d0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a winter-identifier for each data row\n",
    "#\n",
    "# If month < 6, data belongs to previous year's winter. If month >= 6,\n",
    "# data belongs to current year's winter. Doesn't matter which winter\n",
    "# the snowless months are included in as they don't affect the sums, \n",
    "# I just don't want them to have NaN values as this will be used as index.\n",
    "\n",
    "df['Winter'] = df.apply(lambda x: (str(x['Year'] - 1) + '-' + str(x['Year'])) \n",
    "                  if x['Month'] < 6 \n",
    "                  else (str(x['Year']) + '-' + str(x['Year'] + 1)), axis='columns')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7e92e7-6a5b-4266-b655-468eb5d74ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"FMI uses -1 as snow depth when \"there is absolutely no snow at all\". \n",
    "#  We don't want to reduce snow sum in that case, so replace -1 with 0.\"\n",
    "\n",
    "# Replace values\n",
    "df['Snow depth (cm)'].replace(-1.0, 0.0, inplace=True)\n",
    "\n",
    "# Sanity check that snow depth in July was 0 cm\n",
    "df[df.Month == 7].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cb21a8-cf56-4da0-a210-f3f78485e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \" For missing data, assume that the snow depth has been the same as \n",
    "#   during the previous day. (Fill any NaNs with the previous valid value.)\"\n",
    "\n",
    "# Fill NaNs with previous value\n",
    "df['Snow depth (cm)'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Check that no NaN values were left\n",
    "print(df.shape)\n",
    "df['Snow depth (cm)'].describe()\n",
    "\n",
    "# Shape and count of snow depths are the same, so all data is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babaa3fc-cf61-41ce-89d4-45651d6073f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more data cleanup: drop data for winters that cannot be considered full.\n",
    "# This includes winters of 1958-1959 and 2020-2021. \n",
    "df = df[~(df['Winter'] == '1958-1959') & ~(df['Winter'] == '2020-2021')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697c45de-390d-40f8-99ff-103bd249bc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce snow_sum DataFrame with snow sum, snow_sum rank (largest = 1),\n",
    "# number of days with snow, max snow depth of winter\n",
    "\n",
    "# Helper column: count, used to calculate number of days that had snow\n",
    "df['count'] = df['Snow depth (cm)'] > 0.0\n",
    "\n",
    "# Dropping unneeded columns\n",
    "df = df[['Winter', 'Snow depth (cm)', 'count']]\n",
    "# Also adding max column that can be aggregated when grouped\n",
    "df['max'] = df['Snow depth (cm)']\n",
    "# Renaming Snow depth column to Snow sum for the aggregation\n",
    "df.rename(columns={\"Snow depth (cm)\": \"Snow sum\"}, inplace=True) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460517ac-4516-480b-9be5-2f360d06ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Winter to get aggregated numbers\n",
    "df = df.groupby('Winter').agg({'Snow sum': 'sum', 'count': 'sum', 'max': 'max'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0a418c-8412-4c2e-89ca-7d1d2c497359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minor tweaking: change max and snow sum columns into integers\n",
    "df['max'] = df['max'].astype(int)\n",
    "df['Snow sum'] = df['Snow sum'].astype(int)\n",
    "\n",
    "# Get rankings for different winters: \n",
    "# This creates a list of numbers from 1 upwards, and a matching list of Year values sorted by amount of snow from most to least\n",
    "# .T transposes the dataframe, as otherwise we'd have one column per year and only two rows of data.\n",
    "winter_ranks = pd.DataFrame([range(1, len(df) + 1), df.sort_values(by='Snow sum', ascending=False).index.tolist()]).T\n",
    "winter_ranks.columns = ['rank', 'Year']\n",
    "winter_ranks.set_index('Year', inplace=True)\n",
    "winter_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc00e1a-b071-409c-bea2-b6816cc4d2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add rank information to the original data frame\n",
    "df = df.join(winter_ranks)\n",
    "\n",
    "# Reorganize columns to match requested data\n",
    "df = df[['Snow sum', 'rank', 'count', 'max']]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf511d-2bff-4d3b-8cd3-2231732df527",
   "metadata": {},
   "source": [
    "## Problem 5. Statfi data wrangle.\n",
    "- Here we're trying to make some sense out of the data that we downloaded from the statfi service in problem 5 of the first exercises. If you did it successfully, please use your own data. Otherwise you may use the data (from the first exercises model solution) at `public/data_statfin_kihi_pxt_13zt.px.json`.\n",
    "- The model solution had the keyword `kihi`, and the final table `statfin_kihi_pxt_13zt.px`. You may want to replace them in the url below.\n",
    "- First use [this front end](https://pxdata.stat.fi/PxWeb/pxweb/en/StatFin/StatFin__kihi/statfin_kihi_pxt_13zt.px/) to produce a meaningful table, and then try to produce a similar table with pandas and your data.\n",
    "- The most elegant solution wins!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba9774-5cb1-41b5-9fa9-7c93ecdc069a",
   "metadata": {},
   "source": [
    "#### Table produced via StatFi UI\n",
    "\n",
    "(used the kihi table above)\n",
    "\n",
    "Information:\n",
    "* Index (1985=100)\n",
    "* Real Price Index (1985=100)\n",
    "\n",
    "Region:\n",
    "* Whole country\n",
    "* Greater Helsinki\n",
    "* Whole country excluding Greater Helsinki\n",
    "\n",
    "Quarter:\n",
    "* All available quarters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d769b-e9b3-4ad0-9a9b-4e90bf8bab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduction of table based on data\n",
    "\n",
    "import json\n",
    "\n",
    "# Read in data\n",
    "# The json file is not something that can be just thrown in a df\n",
    "# so reading it into a dict with json module itself\n",
    "with open('data_statfin_kihi_pxt_13zt.px.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc33edb-73f9-46db-b8b5-e089c0e40117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order of columns in data fields\n",
    "columns = [column['text'] for column in data['columns']]\n",
    "columns\n",
    "\n",
    "# Creation of a similar table requires columns:\n",
    "# 'Region', 'Quarter', 'Index (1985=100)' and 'Real Price Index (1985=100)'\n",
    "# These correspond to indexes 0, 1, 2, 3.\n",
    "\n",
    "# Inspecting the data shows that each row of data contains a dictionary object\n",
    "# with 'key' containing column indexes 0 + 1, and values containing the rest.\n",
    "data['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1aa1bb-6343-46d0-8a6c-21fff26a0d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The needed data can be grabbed off the data section in the following way:\n",
    "data['data'][0]['key'] + data['data'][0]['values'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b49f188-42e2-4f30-900e-811388dc2e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a DataFrame from the data within the JSON file\n",
    "df = pd.DataFrame([row['key'] + row['values'][0:2] for row in data['data']])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd02014-feca-46ee-be53-00890e8d4d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some basic info out of the data frame\n",
    "print(df.shape)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d9c7d2-c591-4ef6-8f58-3427973641ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, the missing values at the end of the dataframe caused questions.\n",
    "# The user interface for StatFi contained a \"API query for this table\" section,\n",
    "# which contained an additional filter that wasn't accounted for previously: \n",
    "'''\n",
    "{\n",
    "      \"code\": \"Alue\",\n",
    "      \"selection\": {\n",
    "        \"filter\": \"item\",\n",
    "        \"values\": [\n",
    "          \"01\",\n",
    "          \"02\",\n",
    "          \"03\"\n",
    "        ]\n",
    "      }\n",
    "'''\n",
    "# So, setting some column names for readability and dropping irrelevant data\n",
    "df.columns = ['Region', 'Quarter', 'IDX', 'RPI']\n",
    "df = df[df.Region.isin(['01', '02', '03'])]\n",
    "df.set_index('Quarter', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a86e15b-b8a1-4d73-bee7-5229d371ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up correct regions:\n",
    "# There was no manual as to which region is which. But, this can be deduced\n",
    "# based on the numbers. For 1985Q1, RPI for whole country was 98.2, greater\n",
    "# Helsinki 93.7 and whole country excl. greater Helsinki 98.8.\n",
    "\n",
    "# Finding 1985Q1 values in df\n",
    "df.loc['1985Q1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358e0035-c10f-4306-a6ea-71d06a798d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, Region info can be replaced as follows:\n",
    "df = df.replace({'Region': {'01': 'Whole country', '02': 'Greater Helsinki', '03': 'Whole country excluding Greater Helsinki'}})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e45d81-0773-4b35-81fc-2523a53e6a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before pivots (though not strictly necessary for just the table),\n",
    "# changing data types of IDX and RPI to floats\n",
    "df['RPI'] = df.RPI.astype(float)\n",
    "df['IDX'] = df.IDX.astype(float)\n",
    "\n",
    "# Pivot 'Region' data into a column\n",
    "df = df.pivot(columns='Region', values=['IDX', 'RPI'])\n",
    "\n",
    "# Rename shortened IDX and RPI into their proper names\n",
    "df = df.rename(columns={'IDX': 'Index (1985=100)', 'RPI': 'Real Price Index (1985=100)'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b58468-b7f0-4fb8-bb89-c16ec2655035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One minor difference in the case above is that Greater Helsinki and \n",
    "# Whole country are in a different order in table above and in the UI.\n",
    "\n",
    "# But, if there's something to do better than the UI, we can plot this \n",
    "# data as a graph. The webpage couldn't, as apparently there was too\n",
    "# much data to plot. :) \n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e15f52-33e5-4e1a-9f21-991c5295a4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot index data\n",
    "df['Index (1985=100)'].plot(figsize=(10, 6))\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Prices of old single family houses, index (1985=100)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2136b6-435a-4e9d-8530-2f0b8c0ed72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That 1990s dip in house prices gives a pretty good\n",
    "# indication of how bad the recession was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c50db0-e81d-402d-b032-b1029087b859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot real price indexes\n",
    "df['Real Price Index (1985=100)'].plot(figsize=(10, 6))\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Prices of old single family houses, Real Price Index (1985=100)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c786661-789c-4420-9503-c51f33798af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This graph shows it even better, as it factors everything into\n",
    "# prices where effects of inflation have been ruled out. No wonder\n",
    "# it was strictly forbidden to write in the exercise books back in \n",
    "# elementary school, and everything was reused. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
