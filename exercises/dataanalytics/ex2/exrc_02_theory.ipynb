{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af9a04cf-3463-4704-86f4-aaa7eca15865",
   "metadata": {},
   "source": [
    "# Theory for Exercises 02 (Fall 2025)\n",
    "\n",
    "Last modified: Tue Sep 23 before session. The order of things may be quite random, but that's minor.\n",
    "\n",
    "Theme: data wrangling with pandas (+ some basic statistics stuff for future exercises)\n",
    "\n",
    "During our Teams sessions, we'll take a look at the following:\n",
    "\n",
    "- The [Wes McKinney book](https://wesmckinney.com/book/) is excellent. Please start with chapter five.\n",
    "- [Mika's material](https://varpha.pages.labranet.jamk.fi/dan-spring2024) (see *Materials*) is good as well. Please report any AI translation weirdness to our Teams channel.\n",
    "- [Official pandas user guide](https://pandas.pydata.org/docs/user_guide/index.html)\n",
    "- A nice [cleaning guide](https://medium.com/data-science/how-to-clean-your-data-in-python-8f178638b98d). There are no recipies really, you always need to make the decisions by yourself (what to do with the \"dirty\" parts of the data).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d010d-28f1-4bb9-a613-4820bf37e7fb",
   "metadata": {},
   "source": [
    "\n",
    "## On pandas\n",
    "\n",
    "A pandas dataframe is a \"table\" that consists of Series objects (\"columns\") aligned by Index (\"row\"). The Index can be almost anything immutable: integers, strings, dates, intervals, etc.\n",
    "\n",
    "A Series is a pandas version of a list. It has builtin Index and allows mixed datatypes in its values.\n",
    "\n",
    "Method chaining is the recommended way of coding in pandas; see [pandas cheat sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf).\n",
    "\n",
    "Harri's phone browser bookmarks on pandas are in `public/exrc_02`. You may put the urls into freedium.cfd (or better still, subscribe to medium; $5/mo. is not too much maybe).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc8cd7c-012d-4602-8b73-f23514e38f38",
   "metadata": {},
   "source": [
    "## A helper cell\n",
    "probably not needed even"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a26312-1246-49de-84ea-feea9a31411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# notebook analogue of sys.exit() is now raise StopExecution\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "\n",
    "# let's make a function of it\n",
    "def myexit():\n",
    "    raise StopExecution\n",
    "\n",
    "# an input function in a similar manner (we may not need it but anyway)\n",
    "def myinput(msg='press enter to continue'):\n",
    "    input(f'{msg} ')\n",
    "    clear_output()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca29babd-9674-4224-9592-340137282373",
   "metadata": {},
   "source": [
    "## On problem 1 (and general philosophy)\n",
    "\n",
    "Here's an excerpt form the 2023 model solutions. (Just a few lines; the code won't run.) Then the problem 1 profiles were in UK format, which were a bit different from our US format profiles. Anyway.\n",
    "\n",
    "An important theme on this course is *being human in the age of AI* or something like that. We will talk about this during the Tue Sep 23 session. This model solution excerpt is a good example.\n",
    "\n",
    "**Note: This is by no means the only way, or *the* recommended way. This is just a kind of nice way that this particular student came up with.**\n",
    "\n",
    "```python\n",
    "# ------------------ MORE SANITY CHECKS ---------------------------\n",
    "# What about the address?\n",
    "split_addresses = df.address.map(lambda x: x.split('\\n'))\n",
    "\n",
    "# What is the maximum number of address splits?\n",
    "print(f\"Maximum number of address splits: {split_addresses.map(lambda x: len(x)).max()}\\n\")\n",
    "# Result: 4\n",
    "\n",
    "# At this point I'm making an assumption that the first part may be a specifier\n",
    "# (like a flat number), second part is street name, third part city and last one postal code.\n",
    "# -------------------SANITY CHECK ENDS ----------------------------\n",
    "\n",
    "df['street_address'] = df.address.map(lambda x: x.split('\\n')[-3])\n",
    "df['state'] = df.address.map(lambda x: x.split('\\n')[-2])\n",
    "df['postal_code'] = df.address.map(lambda x: x.split('\\n')[-1])\n",
    "\n",
    "# Sample data\n",
    "print(df[['street_address', 'state', 'postal_code']].head())\n",
    "\n",
    "# Instruction: Keep also the ssn, username, sex, mail and birthdate columns. Drop all the other columns.\n",
    "columns_to_keep = ['first_name', 'last_name', 'street_address', 'state', 'postal_code', 'ssn', 'username', 'sex', 'mail', 'birthdate']\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Results so far\n",
    "df.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d868d8f-dbbb-4e57-9e5a-cc3129ea7185",
   "metadata": {},
   "source": [
    "## Problem 3 hints\n",
    "\n",
    "This is also from the 2023 model solutions. Just a few lines, but the idea is: wrangle in whatever ways suit you best, until you are happy! **Again, the below is by no means optimal, and there is always more than one way to do things.**\n",
    "\n",
    "```python\n",
    "\n",
    "# harri's addition: running 'eval' on anything is questionable, but literal_eval is okay! it takes a string and converts it into the corresponding python datatype.\n",
    "from ast import literal_eval\n",
    "\n",
    "df=pd.read_csv('data/XXXXX_prob03_epl.csv')\n",
    "\n",
    "# read the object data from the dataframe\n",
    "df['fullTime']=df['fullTime'].apply(lambda x: literal_eval(x))\n",
    "df['homeScore'] = df['fullTime'].apply(lambda x: x['homeTeam'])\n",
    "\n",
    "# Build Home metrics - True = 1 and false =0\n",
    "df['homeWins']=df['homeScore']>df['awayScore']\n",
    "\n",
    "# ... (many lines snipped)\n",
    "\n",
    "# apply game dummy to get count of games - both home and away can use the same column\n",
    "df['Games']=1\n",
    "\n",
    "# build home results dataset with home team and home metrics - rename all columns to same schema\n",
    "home_results=df[['Games','homeTeam','homeWins','homedraws','homeLoss','homePoints','homeScore','awayScore']]\\\n",
    ".rename(columns={\"homeTeam\":\"Team\",\"homeWins\":\"Wins\",\"homedraws\":\"Draws\",\"homeLoss\":\"Defeats\",\"homePoints\":\"Points\", \"homeScore\":\"TeamGoals\", \"awayScore\":\"OppGoals\"})\n",
    "\n",
    "# ... (many lines snipped)\n",
    "\n",
    "# group results and sort on three metrics points, goal differential and overall goals\n",
    "standings_grouped = standings.groupby(standings['Team']).sum().sort_values(by=(['Points','GoalDiff', 'TeamGoals']), ascending=False)\n",
    "\n",
    "# and so on!\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d929d8b-708d-4598-b750-af2f232aa3f1",
   "metadata": {},
   "source": [
    "## About Problem 5 \n",
    "\n",
    "Problem 5 is meant to be hard!\n",
    "\n",
    "Let's say we want to produce a thing like [this](https://student.labranet.jamk.fi/~varpha/data_analytics/statfi_gui_result.png) from our data obtained in Round 1 Problem 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904cebf8-2c5c-4468-b920-eb697423afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "statfi_data_path = '/home/varpha/dan/public/exrc_02/data/data_statfin_kihi_pxt_13zt.px.json'\n",
    "\n",
    "# let's read the data into a plain python dict first\n",
    "with open(statfi_data_path) as handle:\n",
    "    statfi_data = json.load(handle)\n",
    "\n",
    "# some plain python things one can try to get an initial grip about a json data\n",
    "'''\n",
    "print(type(statfi_data))\n",
    "print(list(statfi_data.keys()))\n",
    "print(list(statfi_data.values()))\n",
    "'''\n",
    "\n",
    "# then convert the data into a pandas dataframe, see\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.json_normalize.html\n",
    "df = pd.json_normalize(statfi_data, record_path = 'data')\n",
    "\n",
    "# some pandas things one can try to get an initial grip about the data\n",
    "\n",
    "print(df.head(5))\n",
    "'''\n",
    "print(df.shape)\n",
    "print(df.index)\n",
    "print(df.columns)\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "'''\n",
    "\n",
    "'''\n",
    "Of the above, df.head(5) was the most important for me in this case.\n",
    "It revealed that we had two columns in the data: key and values,\n",
    "and that they both contained lists. We want to get rid of the lists.\n",
    "\n",
    "Below is one way to do it.\n",
    "\n",
    "'''\n",
    "\n",
    "key_exploded = df['key'].apply(pd.Series)\n",
    "# print(key_exploded.head())\n",
    "\n",
    "values_exploded = df['values'].apply(pd.Series)\n",
    "# print(values_exploded.head())\n",
    "\n",
    "df = pd.concat([key_exploded, values_exploded], axis=1)\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb16f3b-ada2-43c1-a1fb-46271f8922b8",
   "metadata": {},
   "source": [
    "How to advance from here: see e.g.\n",
    "\n",
    "- [Reshaping and pivot tables User Guide](https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html)\n",
    "\n",
    "There are many good medium blog texts as well. (And substack blog texts, and ...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4689e9c4-5365-4e77-a98f-be5937525723",
   "metadata": {},
   "source": [
    "## On Problem 2\n",
    "\n",
    "Easier than Problem 5!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b574c8-a647-4bb5-801c-553ddb14cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/home/varpha/dan/private/x1234/exrc_02/data/x1234_prob02_weather.csv')\n",
    "\n",
    "df.index = df.Time\n",
    "\n",
    "df = df.drop('Time', axis=1)\n",
    "\n",
    "df = df.pivot(columns='ParameterName', values='ParameterValue')\n",
    "\n",
    "df = df.drop(['TG_PT12H_min', 'rrday', 'tday', 'snow'], axis=1)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "# print(df)\n",
    "\n",
    "# without the _= part, the below multiline comment\n",
    "# would be spammed as output (jupyter notebook feature)\n",
    "_ = '''\n",
    "\n",
    "my_mean = ((df['tmin'] + df['tmax'])/2).mean()\n",
    "\n",
    "my_std = ((df['tmin'] + df['tmax'])/2).std()\n",
    "\n",
    "print(my_mean)\n",
    "print(my_std)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8c40ab-e490-4fc5-90ee-264a6da33145",
   "metadata": {},
   "source": [
    "Your task in problem 2a now is to investigate, how far away `(tmin + tmax)/2` (on each row) is from its `avg` (over all rows). Here \"far away\" is measured in terms of the `std` of `(tmin + tmax)/2`. (For example: *this row is 1.2 stds away from the avg*.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dd078e-1bdf-4055-b067-381ecadb9b25",
   "metadata": {},
   "source": [
    "## About big data\n",
    "\n",
    "Let's consider a student question: *How to deal with data bigger than the computer memory?*\n",
    "\n",
    "First, here's a quick python code that generates some fake data into a csv file:\n",
    "\n",
    "```python\n",
    "# %pip install --user faker\n",
    "import random\n",
    "import faker\n",
    "from datetime import datetime, timedelta\n",
    "from csv import DictWriter\n",
    "\n",
    "FILENAME = 'my_fake_data.csv'\n",
    "ROWS = 20000\n",
    "\n",
    "fake = faker.Faker()\n",
    "\n",
    "def random_timestamp():\n",
    "    dt = datetime.now() - timedelta(days=random.randint(0, 60), seconds=random.randint(0, 86400))\n",
    "    return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def fake_computer_log_row():\n",
    "    row = {\n",
    "        'timestamp': random_timestamp(),\n",
    "        'user': fake.user_name(),\n",
    "        'commit_hash': fake.sha1(),\n",
    "        'cpu_usage': random.uniform(0, 100),\n",
    "        'memory_usage': random.uniform(0, 100),\n",
    "        'error_code': random.choice([None, 200, 404, 500])\n",
    "    }\n",
    "    return row\n",
    "\n",
    "\n",
    "#### main ####\n",
    "\n",
    "with open(FILENAME, 'w') as handle:\n",
    "    \n",
    "    fieldnames = ['timestamp', 'user', 'commit_hash', 'cpu_usage', 'memory_usage', 'error_code']\n",
    "    writer = DictWriter(handle, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    rows_written = 0\n",
    "    while (rows_written < ROWS):\n",
    "        writer.writerow(fake_computer_log_row())\n",
    "        rows_written += 1\n",
    "```\n",
    "\n",
    "Then imagine that this data is so big that it doesn't fit into your computer memory. People may start installing heavy frameworks right away, but most often that's unnecessary. (One of the heavy frameworks is Dash and I don't have much positive to say about it. If you really need to take a framework route, consider Polars or Vaex instead.) Often a *filter* is enough, because you don't really need to read all the data into your computer memory at once. Here's an example filter in python:\n",
    "\n",
    "```python\n",
    "\n",
    "# open two filehandles: one for reading and one for writing\n",
    "with open('my_fake_data.csv') as read_handle, \\\n",
    "    open('my_filtered_data.csv', 'w') as write_handle:\n",
    "\n",
    "    # read one line at the time    \n",
    "    while myline := read_handle.readline():\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        - take only the data that you need (cols 0, 3, 4, say)\n",
    "        \n",
    "        - entry refers to a single csv entry (a \"cell\")\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # this is now a list\n",
    "        essential_data = [entry for i, entry in enumerate(myline.split(',')) if i in [0,3,4]]\n",
    "        \n",
    "        # write your essential data list to the disk as a csv row  \n",
    "        write_handle.write(','.join(essential_data) + '\\n')\n",
    "```\n",
    "\n",
    "This filter only reads one line at a time into your computer memory. Then you can start working with the filtered data file.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3bda2c-ae03-44ff-8f93-2cdf01209d69",
   "metadata": {},
   "source": [
    "## Some basic statistics\n",
    "\n",
    "In general, we should have some intuition about the following:\n",
    "\n",
    "- frequencies etc\n",
    "    - coin toss\n",
    "    - probability mass (discrete) vs density (continuous)\n",
    "    - normal distribution\n",
    "- types of variables\n",
    "    - categorical\n",
    "    - ordinal\n",
    "    - numerical\n",
    "- mean, std, median, percentiles\n",
    "- normalized data (*standard coordinates*):\n",
    "    - subtract the mean, divide by std\n",
    "    - --> mean = 0, std = 1\n",
    "\n",
    "We'll get back to these later during our course, but let's do something already here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccbf27a-bb83-4759-9979-2d1a4b246706",
   "metadata": {},
   "source": [
    "## Coin tosses, part 1/2 (theoretical probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233ff575-ea35-4b9d-8d6f-562df34c5d19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy import stats as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 40 # number of coin tosses\n",
    "p = 0.5 # probability of heads (say)\n",
    "\n",
    "# theoretical probabilities for obtaining i heads for 0 <= i <= n\n",
    "heads_probabilities = np.array([st.binom.pmf(i, n, p) for i in range(n+1)])\n",
    "\n",
    "# the sum must be one (because we're talking about probabilities here)\n",
    "# print(sum(heads_probabilities))\n",
    "\n",
    "# initialize plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# bar plot of the probabilities\n",
    "ax.bar(range(n+1), heads_probabilities)\n",
    "\n",
    "# next, let's compare with the normal distribution\n",
    "# with the same mean and std as our binomial distribution\n",
    "\n",
    "# calculate the mean and std of our distribution\n",
    "mean = st.binom.mean(n,p)\n",
    "std = st.binom.std(n,p)\n",
    "\n",
    "# create an \"x-axis\" for the \"continuous\" plot\n",
    "x = np.linspace(0,n,1000) # 1000 is the mesh size\n",
    "\n",
    "# plot the normal distribution curve\n",
    "ax.plot(x, st.norm.pdf(x, mean, std))\n",
    "\n",
    "# one should always make clear legends in plots\n",
    "# (that wasn't done here)...\n",
    "# but that will be part of exercises 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb499389-5efe-4c00-882a-8cbfdc07e074",
   "metadata": {},
   "source": [
    "## Coin tosses, part 2/2 (simulation probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e920a-ee95-4973-837b-1186efa64026",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# clear and initialize plot again\n",
    "fig.clear()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# this is a coin toss simulation\n",
    "# imports, n and p: see previous cell (assume executed)\n",
    "toss_experiments = 10000\n",
    "heads_simulation = st.binom.rvs(n, p, size=toss_experiments)\n",
    "\n",
    "# wrangle the simulation outcomes into a probability distribution\n",
    "# there could be a more elegant way (one line) but who cares\n",
    "myseries = pd.Series(heads_simulation)\n",
    "mydict = myseries.value_counts().to_dict()\n",
    "myvalues = np.array([mydict.get(i, 0) for i in range(n+1)])\n",
    "heads_probabilities = myvalues/sum(myvalues)\n",
    "\n",
    "# plot as before\n",
    "ax.bar(range(n+1), heads_probabilities)\n",
    "\n",
    "# the mean and std should now be calculated from our simulation\n",
    "mean = np.mean(heads_simulation)\n",
    "std = np.std(heads_simulation)\n",
    "x = np.linspace(0,n,1000)\n",
    "\n",
    "# plot the normal distribution with mean & std from above\n",
    "ax.plot(x, st.norm.pdf(x, mean, std))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f943df5-6169-43cc-ada5-0bf0350bbde5",
   "metadata": {},
   "source": [
    "Below is some gpt4-1106-preview output to fix the \"who cares\" part above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec867d6-c8ca-409c-8195-94a6c0af73e5",
   "metadata": {},
   "source": [
    "\n",
    "The code snippet simulates the outcomes of coin tosses and then converts the results into a probability distribution. To make this process more elegant, you can use the `numpy` library's `bincount` function, which counts the number of occurrences of each value in an array of non-negative integers. The `normalize` parameter in `value_counts` can also be used to directly obtain probabilities. Here's a refined version of the code:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import binom\n",
    "\n",
    "# Parameters for the binomial distribution\n",
    "n = 1  # number of trials (1 for a coin toss)\n",
    "p = 0.5  # probability of success (heads)\n",
    "\n",
    "# Simulate coin toss experiments\n",
    "toss_experiments = 10000\n",
    "heads_simulation = binom.rvs(n, p, size=toss_experiments)\n",
    "\n",
    "# Use numpy's bincount to get the count of each outcome\n",
    "outcome_counts = np.bincount(heads_simulation)\n",
    "\n",
    "# Convert counts to probabilities\n",
    "heads_probabilities = outcome_counts / toss_experiments\n",
    "```\n",
    "\n",
    "This code does away with the need for a `pandas` Series and dictionary conversion, resulting in a more efficient and elegant solution. The `heads_probabilities` array now contains the probabilities of each outcome (0 for tails and 1 for heads) directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae9381c-8624-419d-9336-4941048ebbd8",
   "metadata": {},
   "source": [
    "### Central Limit Theorem\n",
    "\n",
    "me: Please give a python simulation example of the central limit theorem.\n",
    "\n",
    "gpt-4-1106-preview: Sure, here's a Python simulation that demonstrates the central limit theorem.\n",
    "\n",
    "In this simulation, we generate random numbers from an exponential distribution, which is not a normal distribution. We then take 1000 samples of size 50 from this data and calculate the mean of each sample. The histogram of the sample means should approximate a normal distribution, as demonstrated by the central limit theorem. The green curve in the plot represents a normal distribution with the same mean and standard deviation as the histogram of the sample means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398aedff-4b36-4c86-a612-3f20dc888e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate random numbers from a non-normal distribution\n",
    "np.random.seed(0)\n",
    "data = np.random.exponential(scale=5, size=1000)\n",
    "\n",
    "# Plot histogram of original data\n",
    "plt.hist(data, bins=50, density=True, color='blue', alpha=0.5, label='Original Data')\n",
    "\n",
    "# Calculate mean and standard deviation of original data\n",
    "mean = np.mean(data)\n",
    "std = np.std(data)\n",
    "\n",
    "# Generate 1000 samples of size 50 from the original data\n",
    "samples = [np.random.choice(data, size=50, replace=True) for i in range(1000)]\n",
    "sample_means = [np.mean(sample) for sample in samples]\n",
    "\n",
    "# Plot histogram of sample means\n",
    "plt.hist(sample_means, bins=50, density=True, color='red', alpha=0.5, label='Sample Means')\n",
    "\n",
    "# Calculate mean and standard deviation of sample means\n",
    "mean_sample_means = np.mean(sample_means)\n",
    "std_sample_means = np.std(sample_means)\n",
    "\n",
    "# Plot normal distribution with mean and standard deviation of sample means\n",
    "x = np.linspace(mean_sample_means - 3 * std_sample_means, mean_sample_means + 3 * std_sample_means, 1000)\n",
    "\n",
    "# here the ai uses the formula but one could use st.norm.pdf(x,mean,std) // Harri\n",
    "y = 1 / (std_sample_means * np.sqrt(2 * np.pi)) * np.exp(-0.5 * ((x - mean_sample_means) / std_sample_means)**2)\n",
    "\n",
    "plt.plot(x, y, color='green', label='Normal Distribution')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c1a1d8-60fd-48fc-bdec-22ca797bb885",
   "metadata": {},
   "source": [
    "## On AI\n",
    "\n",
    "(to be continued at some point)\n",
    "\n",
    "The AI companies always have a separate developer platform, where you pay by usage only, and if you are smart, you can get away with very little cost, and still have access to all the latest and best models. For example, [platform.openai.com](platform.openai.com) is different from chatgpt.com (different user account). Similarly, [console.anthropic.com](console.anthropic.com) is different from claude.com. Etc!\n",
    "\n",
    "**If you experiment with these, remember to use billing guards!**\n",
    "\n",
    "Harri uses [https://github.com/sigoden/aichat](https://github.com/sigoden/aichat), but a corresponding gui tool is [https://github.com/Mintplex-Labs/anything-llm](https://github.com/Mintplex-Labs/anything-llm)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
